{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview This tutorial provides a step-by-step guide to performing basic polygenic risk score (PRS) analyses and accompanies our PRS Guide paper . The aim of this tutorial is to provide a simple introduction to PRS analyses to those new to PRS, while equipping existing users with a better understanding of the processes and implementation \"underneath the hood\" of popular PRS software. The tutorial is separated into four main sections and reflects the structure of our guide paper : the first two sections on QC correspond to Section 2 of the paper and constitute a 'QC checklist' for PRS analyses, the third section on calculating PRS (here with examples using PLINK , PRSice-2 , LDpred and lassosum ) corresponds to Section 3 of the paper, while the fourth section, which provides some examples of visualising PRS results, accompanies Section 4 of the paper. Quality Control (QC) of Base Data Quality Control (QC) of Target Data Calculating and analysing PRS Visualising PRS Results We will be referring to our guide paper in each section and so you may find it helpful to have the paper open as you go through the tutorial. If you are only interested in how to perform PRS on previously QC'ed data then you can skip to Step 3 . Links to download the required data are provided under each section. Note This tutorial is written for Linux and OS X operating systems. Windows users will need to change some commands accordingly. Note Throughout the tutorial you will see tabs above some of the code: A echo Tab A B echo Tab B You can click on the tab to change to an alternative code (eg. to a different operation system) Datasets Base data : Modified summary statistics file from the GIANT consortium study on height Target data : Simulated data based on the 1000 Genomes Project European samples Requirements To follow the tutorial, you will need the following programs installed: R ( version 3.2.3+ ) PLINK 1.9 Citation If you find this tutorial helpful for a publication, then please consider citing: Citation Choi SW, Mak TSH, O'Reilly PF. A guide to performing Polygenic Risk Score analyses. bioRxiv 416545 (2018). https://doi.org/10.1101/416545","title":"Overview"},{"location":"#overview","text":"This tutorial provides a step-by-step guide to performing basic polygenic risk score (PRS) analyses and accompanies our PRS Guide paper . The aim of this tutorial is to provide a simple introduction to PRS analyses to those new to PRS, while equipping existing users with a better understanding of the processes and implementation \"underneath the hood\" of popular PRS software. The tutorial is separated into four main sections and reflects the structure of our guide paper : the first two sections on QC correspond to Section 2 of the paper and constitute a 'QC checklist' for PRS analyses, the third section on calculating PRS (here with examples using PLINK , PRSice-2 , LDpred and lassosum ) corresponds to Section 3 of the paper, while the fourth section, which provides some examples of visualising PRS results, accompanies Section 4 of the paper. Quality Control (QC) of Base Data Quality Control (QC) of Target Data Calculating and analysing PRS Visualising PRS Results We will be referring to our guide paper in each section and so you may find it helpful to have the paper open as you go through the tutorial. If you are only interested in how to perform PRS on previously QC'ed data then you can skip to Step 3 . Links to download the required data are provided under each section. Note This tutorial is written for Linux and OS X operating systems. Windows users will need to change some commands accordingly. Note Throughout the tutorial you will see tabs above some of the code: A echo Tab A B echo Tab B You can click on the tab to change to an alternative code (eg. to a different operation system)","title":"Overview"},{"location":"#datasets","text":"Base data : Modified summary statistics file from the GIANT consortium study on height Target data : Simulated data based on the 1000 Genomes Project European samples","title":"Datasets"},{"location":"#requirements","text":"To follow the tutorial, you will need the following programs installed: R ( version 3.2.3+ ) PLINK 1.9","title":"Requirements"},{"location":"#citation","text":"If you find this tutorial helpful for a publication, then please consider citing: Citation Choi SW, Mak TSH, O'Reilly PF. A guide to performing Polygenic Risk Score analyses. bioRxiv 416545 (2018). https://doi.org/10.1101/416545","title":"Citation"},{"location":"base/","text":"Obtaining the base data file The first step in Polygenic Risk Score (PRS) analyses is to generate or obtain the base data (GWAS summary statistics). Ideally these will correspond to the most powerful GWAS results available on the phenotype under study. In this example, we will use a modified version of the Height GWAS summary statistics generated by the GIANT consortium . You can download the summary statistic file here or you can use the following bash command: curl https://github.com/choishingwan/PRS-Tutorial/raw/master/resources/GIANT.height.gz -L -O which will create a file called GIANT.height.gz in your working directory. Warning If you download the summary statistics without using the bash command, and are using a MAC machine, the gz file will be decompressed automatically, resulting in a GIANT.height file instead. To maintain consistency, we suggest compressing the GIANT.height file with gzip GIANT.height before starting the tutorial Reading the base data file GIANT.height.gz is compressed. To read its content, you can type: gunzip -c GIANT.height.gz | head which will display the first 10 lines of the file Note Working with compressed files reduces storage space requirements The GIANT.height.gz file contains the following columns: SNP CHR BP A1 A2 MAF SE P N INFO OR rs2073813 1 753541 A G 0.125 0.0083 0.68 69852 0.866425782879888 0.996605773454898 rs12562034 1 768448 A G 0.092 0.0088 0.55 88015 0.917520990188678 0.994714020220009 rs2980319 1 777122 A T 0.125 0.006 0.65 148975 0.847126999058955 0.997303641721713 The column headers correspond to the following: SNP : SNP ID, usually in the form of rs-ID CHR : The chromosome in which the SNP resides BP : Chromosomal co-ordinate of the SNP A1 : The effect allele of the SNP A2 : The non-effect allele of the SNP MAF : The minor allele frequency (MAF) of the SNP SE : The standard error (SE) of the effect size esimate P : The P-value of association between the SNP genotypes and the base phenotype N : Number of samples used to obtain the effect size estimate INFO : The imputation information score OR : The effect size estimate of the SNP, if the outcome is binary/case-control. If the outcome is continuous or treated as continuous then this will be the BETA QC checklist: Base data Below we perform QC on these base data according to the 'QC checklist' in the guide paper, which we recommend that users follow each time they perform a PRS analysis: # Heritability check We recommend that PRS analyses are performed on base data with a chip-heritability estimate \\(h_{snp}^{2} 0.05\\) . The chip-heritability of a GWAS can be estimated using e.g. LD Score Regression (LDSC). Our GIANT height GWAS data are known to have a chip-heritability much greater than 0.05 and so we can move on to the next QC step. # Effect allele The GIANT consortium report which is the effect allele and which is the non-effect allele in their results, critical for PRS association results to be in the correction direction. Important Some GWAS results files do not make clear which allele is the effect allele and which the non-effect allele. If the incorrect assumption is made in computing the PRS, then the effect of the PRS in the target data will be in the wrong direction. To avoid misleading conclusions the effect allele from the base (GWAS) data must be known. # File transfer A common problem is that the downloaded base data file becomes corrupted during download, which can cause PRS software to crash or to produce errors in results. However, a md5sum hash is generally included in files so that file integrity can be checked. The following command performs this md5sum check: Linux md5sum GIANT.height.gz OS X md5 GIANT.height.gz if the file is intact, then md5sum generates a string of characters, which in this case should be: 80e48168416a2fdbe88d68cdfebd4ca2 . If a different string is generated, then the file is corrupted. # Genome build These base data are on the same genome build as the target data that we will be using. You must check that your base and target data are on the same genome build, and if they are not then use a tool such as LiftOver to make the builds consistent across the data sets. # Standard GWAS QC As described in the paper, both the base and target data should be subjected to the standard stringent QC steps performed in GWAS. If the base data have been obtained as summary statistics from a public source, then the typical QC steps that you will be able to perform on them are to filter the SNPs according to INFO score and MAF. SNPs with low minor allele frequency (MAF) or imputation information score (INFO) are more likely to generate false positive results due to their lower statistical power (and higher probability of genotyping errors in the case of low MAF). Therefore, SNPs with low MAF and INFO are typically removed before performing downstream analyses. We recommend removing SNPs with MAF 1% and INFO 0.8 (with very large base sample sizes these thresholds could be reduced if sensitivity checks indicate reliable results). These SNP filters can be acheived using the following code: gunzip -c GIANT.height.gz | \\ awk NR==1 || ($6 0.01) ($10 0.8) {print} | \\ gzip Height.gz The bash code above does the following: 1. Decompresses and reads the GIANT.height.gz file 2. Prints the header line ( NR==1 ) 3. Prints any line with MAF above 0.01 ( $6 because the sixth column of the file contains the MAF information) 4. Prints any line with INFO above 0.8 ( $10 because the tenth column of the file contains the INFO information) 5. Compresses and writes the results to Height.gz # Ambiguous SNPs If the base and target data were generated using different genotyping chips and the chromosome strand (+/-) for either is unknown, then it is not possible to match ambiguous SNPs (i.e. those with complementary alleles, either C/G or A/T) across the data sets, because it will be unknown whether the base and target data are referring to the same allele or not. Ambiguous SNPs can be removed in the base data and then there will be no such SNPs in the subsequent analyses, since analyses are performed only on SNPs that overlap between the base and target data. Nonambiguous SNPs can be retained using the following: gunzip -c Height.gz | \\ awk !( ($4== A $5== T ) || \\ ($4== T $5== A ) || \\ ($4== G $5== C ) || \\ ($4== C $5== G )) {print} | \\ gzip Height.noambig.gz How many non-ambiguous SNPs were there? There are 609,041 ambiguous SNPs # Mismatching genotypes If there is a non-ambiguous mismatch in allele coding between the base and target data sets, such as A/C in the base and G/T in the target data, then this can be resolved by \u2018flipping\u2019 the alleles in either data set to their complementary alleles. However, since we need the target data to know which SNPs have mismatching genotypes across the data sets, then we will perform this 'allele flipping' in the target data. # Duplicate SNPs If an error has occurred in the generation of the base data then there may be duplicated SNPs in the base data file. Most PRS software do not allow duplicated SNPs in the base data input and thus they should be removed, using a command such as the one below: gunzip -c Height.noambig.gz | \\ awk { print $1} | \\ sort | \\ uniq -d duplicated.snp The above command does the following: Decompresses and reads the Height.noambig.gz file Prints out the first column of the file (which contains the SNP ID; change $1 to another number if the SNP ID is located in another column, e.g. $3 if the SNP ID is located on the third column) Sort the SNP IDs. This will put duplicated SNP IDs next to each other Print out any duplicated SNP IDs using the uniq command and print them to the duplicated.snp file How many duplicated SNPs are there? There are a total of 10 duplicated SNPs Duplicated SNPs can then be removed using the grep command: gunzip -c Height.noambig.gz | \\ grep -vf duplicated.snp | \\ gzip - Height.QC.gz The above script does the following: Decompresses and reads the Height.noambig.gz file Establishes whether any row contains entries observed in duplicated.snp and removes them if so Compresses and writes the results to Height.QC.gz # Sex chromosomes Previously performed QC on these data removed individuals with mismatching (inferred) biological and reported sex, while the sex chromosomes are not included. Please refer to the corresponding section in the paper for details relating QC performed in relation to the sex chromosomes. # Sample overlap In this tutorial the target data are simulated and thus there must be no sample overlap. However, users should ensure that the possibility of sample overlap between the base and target data is minimised. # Relatedness In this tutorial the target data are simulated and thus there must be no closely related individuals across the base and target data. However, users should ensure that the possibility of closely related individuals between the base and target data is minimised. The Height.QC.gz base data are now ready for using in downstream analyses.","title":"1. QC of Base Data"},{"location":"base/#obtaining-the-base-data-file","text":"The first step in Polygenic Risk Score (PRS) analyses is to generate or obtain the base data (GWAS summary statistics). Ideally these will correspond to the most powerful GWAS results available on the phenotype under study. In this example, we will use a modified version of the Height GWAS summary statistics generated by the GIANT consortium . You can download the summary statistic file here or you can use the following bash command: curl https://github.com/choishingwan/PRS-Tutorial/raw/master/resources/GIANT.height.gz -L -O which will create a file called GIANT.height.gz in your working directory. Warning If you download the summary statistics without using the bash command, and are using a MAC machine, the gz file will be decompressed automatically, resulting in a GIANT.height file instead. To maintain consistency, we suggest compressing the GIANT.height file with gzip GIANT.height before starting the tutorial","title":"Obtaining the base data file"},{"location":"base/#reading-the-base-data-file","text":"GIANT.height.gz is compressed. To read its content, you can type: gunzip -c GIANT.height.gz | head which will display the first 10 lines of the file Note Working with compressed files reduces storage space requirements The GIANT.height.gz file contains the following columns: SNP CHR BP A1 A2 MAF SE P N INFO OR rs2073813 1 753541 A G 0.125 0.0083 0.68 69852 0.866425782879888 0.996605773454898 rs12562034 1 768448 A G 0.092 0.0088 0.55 88015 0.917520990188678 0.994714020220009 rs2980319 1 777122 A T 0.125 0.006 0.65 148975 0.847126999058955 0.997303641721713 The column headers correspond to the following: SNP : SNP ID, usually in the form of rs-ID CHR : The chromosome in which the SNP resides BP : Chromosomal co-ordinate of the SNP A1 : The effect allele of the SNP A2 : The non-effect allele of the SNP MAF : The minor allele frequency (MAF) of the SNP SE : The standard error (SE) of the effect size esimate P : The P-value of association between the SNP genotypes and the base phenotype N : Number of samples used to obtain the effect size estimate INFO : The imputation information score OR : The effect size estimate of the SNP, if the outcome is binary/case-control. If the outcome is continuous or treated as continuous then this will be the BETA","title":"Reading the base data file"},{"location":"base/#qc-checklist-base-data","text":"Below we perform QC on these base data according to the 'QC checklist' in the guide paper, which we recommend that users follow each time they perform a PRS analysis:","title":"QC checklist: Base data"},{"location":"base/#35-heritability-check","text":"We recommend that PRS analyses are performed on base data with a chip-heritability estimate \\(h_{snp}^{2} 0.05\\) . The chip-heritability of a GWAS can be estimated using e.g. LD Score Regression (LDSC). Our GIANT height GWAS data are known to have a chip-heritability much greater than 0.05 and so we can move on to the next QC step.","title":"# Heritability check"},{"location":"base/#35-effect-allele","text":"The GIANT consortium report which is the effect allele and which is the non-effect allele in their results, critical for PRS association results to be in the correction direction. Important Some GWAS results files do not make clear which allele is the effect allele and which the non-effect allele. If the incorrect assumption is made in computing the PRS, then the effect of the PRS in the target data will be in the wrong direction. To avoid misleading conclusions the effect allele from the base (GWAS) data must be known.","title":"# Effect allele"},{"location":"base/#35-file-transfer","text":"A common problem is that the downloaded base data file becomes corrupted during download, which can cause PRS software to crash or to produce errors in results. However, a md5sum hash is generally included in files so that file integrity can be checked. The following command performs this md5sum check: Linux md5sum GIANT.height.gz OS X md5 GIANT.height.gz if the file is intact, then md5sum generates a string of characters, which in this case should be: 80e48168416a2fdbe88d68cdfebd4ca2 . If a different string is generated, then the file is corrupted.","title":"# File transfer"},{"location":"base/#35-genome-build","text":"These base data are on the same genome build as the target data that we will be using. You must check that your base and target data are on the same genome build, and if they are not then use a tool such as LiftOver to make the builds consistent across the data sets.","title":"# Genome build"},{"location":"base/#35-standard-gwas-qc","text":"As described in the paper, both the base and target data should be subjected to the standard stringent QC steps performed in GWAS. If the base data have been obtained as summary statistics from a public source, then the typical QC steps that you will be able to perform on them are to filter the SNPs according to INFO score and MAF. SNPs with low minor allele frequency (MAF) or imputation information score (INFO) are more likely to generate false positive results due to their lower statistical power (and higher probability of genotyping errors in the case of low MAF). Therefore, SNPs with low MAF and INFO are typically removed before performing downstream analyses. We recommend removing SNPs with MAF 1% and INFO 0.8 (with very large base sample sizes these thresholds could be reduced if sensitivity checks indicate reliable results). These SNP filters can be acheived using the following code: gunzip -c GIANT.height.gz | \\ awk NR==1 || ($6 0.01) ($10 0.8) {print} | \\ gzip Height.gz The bash code above does the following: 1. Decompresses and reads the GIANT.height.gz file 2. Prints the header line ( NR==1 ) 3. Prints any line with MAF above 0.01 ( $6 because the sixth column of the file contains the MAF information) 4. Prints any line with INFO above 0.8 ( $10 because the tenth column of the file contains the INFO information) 5. Compresses and writes the results to Height.gz","title":"# Standard GWAS QC"},{"location":"base/#35-ambiguous-snps","text":"If the base and target data were generated using different genotyping chips and the chromosome strand (+/-) for either is unknown, then it is not possible to match ambiguous SNPs (i.e. those with complementary alleles, either C/G or A/T) across the data sets, because it will be unknown whether the base and target data are referring to the same allele or not. Ambiguous SNPs can be removed in the base data and then there will be no such SNPs in the subsequent analyses, since analyses are performed only on SNPs that overlap between the base and target data. Nonambiguous SNPs can be retained using the following: gunzip -c Height.gz | \\ awk !( ($4== A $5== T ) || \\ ($4== T $5== A ) || \\ ($4== G $5== C ) || \\ ($4== C $5== G )) {print} | \\ gzip Height.noambig.gz How many non-ambiguous SNPs were there? There are 609,041 ambiguous SNPs","title":"# Ambiguous SNPs"},{"location":"base/#35-mismatching-genotypes","text":"If there is a non-ambiguous mismatch in allele coding between the base and target data sets, such as A/C in the base and G/T in the target data, then this can be resolved by \u2018flipping\u2019 the alleles in either data set to their complementary alleles. However, since we need the target data to know which SNPs have mismatching genotypes across the data sets, then we will perform this 'allele flipping' in the target data.","title":"# Mismatching genotypes"},{"location":"base/#35-duplicate-snps","text":"If an error has occurred in the generation of the base data then there may be duplicated SNPs in the base data file. Most PRS software do not allow duplicated SNPs in the base data input and thus they should be removed, using a command such as the one below: gunzip -c Height.noambig.gz | \\ awk { print $1} | \\ sort | \\ uniq -d duplicated.snp The above command does the following: Decompresses and reads the Height.noambig.gz file Prints out the first column of the file (which contains the SNP ID; change $1 to another number if the SNP ID is located in another column, e.g. $3 if the SNP ID is located on the third column) Sort the SNP IDs. This will put duplicated SNP IDs next to each other Print out any duplicated SNP IDs using the uniq command and print them to the duplicated.snp file How many duplicated SNPs are there? There are a total of 10 duplicated SNPs Duplicated SNPs can then be removed using the grep command: gunzip -c Height.noambig.gz | \\ grep -vf duplicated.snp | \\ gzip - Height.QC.gz The above script does the following: Decompresses and reads the Height.noambig.gz file Establishes whether any row contains entries observed in duplicated.snp and removes them if so Compresses and writes the results to Height.QC.gz","title":"# Duplicate SNPs"},{"location":"base/#35-sex-chromosomes","text":"Previously performed QC on these data removed individuals with mismatching (inferred) biological and reported sex, while the sex chromosomes are not included. Please refer to the corresponding section in the paper for details relating QC performed in relation to the sex chromosomes.","title":"# Sex chromosomes"},{"location":"base/#35-sample-overlap","text":"In this tutorial the target data are simulated and thus there must be no sample overlap. However, users should ensure that the possibility of sample overlap between the base and target data is minimised.","title":"# Sample overlap"},{"location":"base/#35-relatedness","text":"In this tutorial the target data are simulated and thus there must be no closely related individuals across the base and target data. However, users should ensure that the possibility of closely related individuals between the base and target data is minimised. The Height.QC.gz base data are now ready for using in downstream analyses.","title":"# Relatedness"},{"location":"lassosum/","text":"Here we use another PRS program, lassosum , which is an R package that uses penalised regression (LASSO) in its approach to PRS calculation. You can install lassosum and its dependencies in R with the following command: install.packages ( c ( devtools , RcppArmadillo , data.table , Matrix ), dependencies = TRUE ) library ( devtools ) install_github ( tshmak/lassosum ) Again, we assume that we have the following files: File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.valid.sample This file contains the samples that passed all the QC EUR.height This file contains the phenotype of the samples EUR.covariate This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples Running PRS analysis We can run lassosum as follows: library ( lassosum ) # Prefer to work with data.table as it speeds up file reading library ( data.table ) library ( methods ) # We like to use dplyr for it makes codes much more readable library ( dplyr ) sum.stat - Height.QC.gz bfile - EUR.QC # Read in and process the covariates covariate - fread ( EUR.covariate ) pcs - fread ( EUR.eigenvec ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) # Need as.data.frame here as lassosum doesn t handle data.table # covariates very well cov - as.data.frame ( merge ( covariate , pcs , by = c ( FID , IID ))) # We will need the EUR.hg19 file provided by lassosum # which are LD regions defined in Berisa and Pickrell (2015) for the European population and the hg19 genome. ld.file - system.file ( data , Berisa.EUR.hg19.bed , package = lassosum ) # output prefix prefix - EUR # Read in the target phenotype file target.pheno - as.data.frame ( fread ( EUR.height )[, c ( FID , IID , Height )]) # Read in samples to include in the analysis target.keep - fread ( EUR.valid.sample )[, c ( FID , IID )] # Read in the summary statistics ss - fread ( sum.stat ) # Number of sample in base size - 253288 # Remove P-value = 0, which causes problem in the transformation ss - ss [ ! P == 0 ] # Read in the LD blocks ld - fread ( ld.file ) # Transform the P-values into correlation cor - p2cor ( p = ss $ P , n = size , sign = log ( ss $ OR ) ) # Because FID of our samples are all 0, we might encounter problem with lassosum # we need to provide a T/F vector instead of the target.keep file target.keep [, ID := do.call ( paste , c ( . SD , sep = : )), . SDcols = c ( 1 : 2 )] fam - fread ( paste0 ( bfile , .fam )) fam [, ID := do.call ( paste , c ( . SD , sep = : )), . SDcols = c ( 1 : 2 )] keep - fam $ ID %in% target.keep $ ID # Run the lassosum pipeline out - lassosum.pipeline ( cor = cor , chr = ss $ CHR , pos = ss $ BP , A1 = ss $ A1 , A2 = ss $ A2 , ref.bfile = bfile , keep.ref = keep , test.bfile = bfile , keep.test = keep , LDblocks = ld , trace = 2 ) # Store the R2 results target.res - validate ( out , pheno = target.pheno , covar = cov ) # Get the maximum R2 r2 - max ( target.res $ validation.table $ value ) ^ 2","title":"lassosum"},{"location":"lassosum/#running-prs-analysis","text":"We can run lassosum as follows: library ( lassosum ) # Prefer to work with data.table as it speeds up file reading library ( data.table ) library ( methods ) # We like to use dplyr for it makes codes much more readable library ( dplyr ) sum.stat - Height.QC.gz bfile - EUR.QC # Read in and process the covariates covariate - fread ( EUR.covariate ) pcs - fread ( EUR.eigenvec ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) # Need as.data.frame here as lassosum doesn t handle data.table # covariates very well cov - as.data.frame ( merge ( covariate , pcs , by = c ( FID , IID ))) # We will need the EUR.hg19 file provided by lassosum # which are LD regions defined in Berisa and Pickrell (2015) for the European population and the hg19 genome. ld.file - system.file ( data , Berisa.EUR.hg19.bed , package = lassosum ) # output prefix prefix - EUR # Read in the target phenotype file target.pheno - as.data.frame ( fread ( EUR.height )[, c ( FID , IID , Height )]) # Read in samples to include in the analysis target.keep - fread ( EUR.valid.sample )[, c ( FID , IID )] # Read in the summary statistics ss - fread ( sum.stat ) # Number of sample in base size - 253288 # Remove P-value = 0, which causes problem in the transformation ss - ss [ ! P == 0 ] # Read in the LD blocks ld - fread ( ld.file ) # Transform the P-values into correlation cor - p2cor ( p = ss $ P , n = size , sign = log ( ss $ OR ) ) # Because FID of our samples are all 0, we might encounter problem with lassosum # we need to provide a T/F vector instead of the target.keep file target.keep [, ID := do.call ( paste , c ( . SD , sep = : )), . SDcols = c ( 1 : 2 )] fam - fread ( paste0 ( bfile , .fam )) fam [, ID := do.call ( paste , c ( . SD , sep = : )), . SDcols = c ( 1 : 2 )] keep - fam $ ID %in% target.keep $ ID # Run the lassosum pipeline out - lassosum.pipeline ( cor = cor , chr = ss $ CHR , pos = ss $ BP , A1 = ss $ A1 , A2 = ss $ A2 , ref.bfile = bfile , keep.ref = keep , test.bfile = bfile , keep.test = keep , LDblocks = ld , trace = 2 ) # Store the R2 results target.res - validate ( out , pheno = target.pheno , covar = cov ) # Get the maximum R2 r2 - max ( target.res $ validation.table $ value ) ^ 2","title":"Running PRS analysis"},{"location":"ldpred/","text":"Here we use another PRS program, LDpred , that uses a Bayesian approach to polygenic risk scoring. Note Python 3 and other packages need to be installed before running LDpred. Please refer to their website for installation instructions. If you have Python installed, you should be able to install LDpred with the following command: pip install ldpred Note The script used here is based on LDpred version 1.0.6 We assume that you have the following files: File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.valid.sample This file contains the samples that passed all the QC EUR.height This file contains the phenotype of the samples EUR.covariate This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples Running PRS analysis LDpred does not support filtering of samples and SNPs, so therefore we must generate a new QC'ed genotype file using plink : # We also add the height phenotype for convenience plink \\ --bfile EUR.QC \\ --pheno EUR.height \\ --keep EUR.valid.sample \\ --make-bed \\ --out EUR.ldpred LDpred can then perform PRS analysis in three steps: Preprocessing the base data file: # There are 253,288 samples in the Height GWAS python LDpred.py coord \\ --rs SNP \\ --A1 A1 \\ --A2 A2 \\ --pos BP \\ --chr CHR \\ --pval P \\ --eff OR \\ --ssf-format CUSTOM \\ --N 253288 \\ --ssf Height.QC.gz \\ --out EUR.coord \\ --gf EUR.ldpred Adjust the effect size estimates: # LDpred recommend radius to be Total number of SNPs in target / 3000 python LDpred.py gibbs \\ --cf EUR.coord \\ --ldr 183 \\ --ldf EUR.ld \\ --out EUR.weight \\ --N 253288 Calculate the PRS: python LDpred.py score \\ --gf EUR.ldpred \\ --rf EUR.weight \\ --out EUR.score \\ --pf EUR.height \\ --pf-format LSTANDARD Note To obtain the PRS \\(R^2\\) according to the use of different parameters, and / or to adjust for covariates, you may need to use R (see here ).","title":"LDpred"},{"location":"ldpred/#running-prs-analysis","text":"LDpred does not support filtering of samples and SNPs, so therefore we must generate a new QC'ed genotype file using plink : # We also add the height phenotype for convenience plink \\ --bfile EUR.QC \\ --pheno EUR.height \\ --keep EUR.valid.sample \\ --make-bed \\ --out EUR.ldpred LDpred can then perform PRS analysis in three steps: Preprocessing the base data file: # There are 253,288 samples in the Height GWAS python LDpred.py coord \\ --rs SNP \\ --A1 A1 \\ --A2 A2 \\ --pos BP \\ --chr CHR \\ --pval P \\ --eff OR \\ --ssf-format CUSTOM \\ --N 253288 \\ --ssf Height.QC.gz \\ --out EUR.coord \\ --gf EUR.ldpred Adjust the effect size estimates: # LDpred recommend radius to be Total number of SNPs in target / 3000 python LDpred.py gibbs \\ --cf EUR.coord \\ --ldr 183 \\ --ldf EUR.ld \\ --out EUR.weight \\ --N 253288 Calculate the PRS: python LDpred.py score \\ --gf EUR.ldpred \\ --rf EUR.weight \\ --out EUR.score \\ --pf EUR.height \\ --pf-format LSTANDARD Note To obtain the PRS \\(R^2\\) according to the use of different parameters, and / or to adjust for covariates, you may need to use R (see here ).","title":"Running PRS analysis"},{"location":"plink/","text":"Background In this section of the tutorial you will use four different software programs to compute PRS from the base and target data that you QC'ed in the previous two sections. On this page, you will compute PRS using the popular genetic analyses tool plink - while plink is not a dedicated PRS software, each of the steps required to compute PRS using the C+T standard approach can be performed in plink and carrying out this multi-step process can be a good way to learn the processes involved in computing PRS (which are typically performed automatically by PRS software). Required Data In the previous sections, we have generated the following files: File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.QC.valid This file contains the samples that passed all the QC EUR.height This file contains the phenotype of the samples EUR.covariate This file contains the covariates of the samples Update Effect Size When the effect size relates to disease risk and is thus given as an odds ratio (OR), rather than BETA (for continuous traits), then the PRS is computed as a product of ORs. To simplify this calculation, we usually take the natural logarithm of the OR so that the PRS can be computed using a simple summation instead (which can be back-transformed afterwards). We can obtain the transformed summary statistics with R : Without data.table dat - read.table ( gzfile ( Height.QC.gz ), header = T ) dat $ OR - log ( dat $ OR ) write.table ( dat , Height.QC.Transformed , quote = F , row.names = F ) q () # exit R With data.table library ( data.table ) dat - fread ( Height.QC.gz ) fwrite ( dat [, OR := log ( OR )], Height.QC.Transformed , sep = \\t ) q () # exit R Warning It may be tempting to perform the log transofrmation using awk . However, due to rounding of values performed in awk , less accurate results may be obtained. Therefore, we recommend performing the transformation in R or allow the PRS software to perform the transformation directly. Clumping Linkage disequilibrium, which corresponds to the correlation between the genotypes of genetic variants across the genome, makes identifying the contribution from causal independent genetic variants extremely challenging. One way of approximately capturing the right level of causal signal is to perform clumping, which removes SNPs in such a way that only weakly correlated SNPs are retained but preferentially retaining the SNPs most associated with the phenotype under study. Clumping can be performed using the following command in plink : plink \\ --bfile EUR.QC \\ --clump-p1 1 \\ --clump-r2 0 .1 \\ --clump-kb 250 \\ --clump Height.QC.Transformed \\ --clump-snp-field SNP \\ --clump-field P \\ --out EUR Each of the new parameters corresponds to the following Paramter Value Description clump-p1 1 P-value threshold for a SNP to be included as an index SNP. 1 is selected such that all SNPs are include for clumping clump-r2 0.1 SNPs having \\(r^2\\) higher than 0.1 with the index SNPs will be removed clump-kb 250 SNPs within 250k of the index SNP are considered for clumping clump Height.QC.Transformed Base data (summary statistic) file containing the P-value information clump-snp-field SNP Specifies that the column SNP contains the SNP IDs clump-field P Specifies that the column P contains the P-value information A more detailed description of the clumping process can be found here Note The \\(r^2\\) values computed by --clump are based on maximum likelihood haplotype frequency estimates This will generate EUR.clumped , containing the index SNPs after clumping is performed. We can extract the index SNP ID by performing the following command: awk NR!=1{print $3} EUR.clumped EUR.valid.snp $3 because the third column contains the SNP ID Note If your target data are small (e.g. N 500) then you can use the 1000 Genomes Project samples for the LD calculation. Make sure to use the population that most closely reflects represents the base sample. Generate PRS plink provides a convenient function --score and --q-score-range for calculating polygenic scores. We will need three files: The base data file: Height.QC.Transformed A file containing SNP IDs and their corresponding P-values ( $1 because SNP ID is located in the first column; $8 because the P-value is located in the eighth column) awk {print $1,$8} Height.QC.Transformed SNP.pvalue A file containing the different P-value thresholds for inclusion of SNPs in the PRS. Here calculate PRS corresponding to a few thresholds for illustration purposes: echo 0.001 0 0.001 range_list echo 0.05 0 0.05 range_list echo 0.1 0 0.1 range_list echo 0.2 0 0.2 range_list echo 0.3 0 0.3 range_list echo 0.4 0 0.4 range_list echo 0.5 0 0.5 range_list The format of the range_list file should be as follows: Name of Threshold Lower bound Upper Bound Note The threshold boundaries are inclusive. For example, for the 0.05 threshold, we include all SNPs with P-value from 0 to 0.05 , including any SNPs with P-value equal to 0.05 . We can then calculate the PRS with the following plink command: plink \\ --bfile EUR.QC \\ --score Height.QC.Transformed 1 4 11 header \\ --q-score-range range_list SNP.pvalue \\ --extract EUR.valid.snp \\ --out EUR The meaning of the new parameters are as follows: Paramter Value Description score Height.QC.Transformed 1 4 11 header We read from the Height.QC.Transformed file, assuming that the 1 st column is the SNP ID; 4 th column is the effective allele information; the 11 th column is the effect size estimate; and that the file contains a header q-score-range range_test SNP.pvalue We want to calculate PRS based on the thresholds defined in range_test , where the threshold values (P-values) were stored in SNP.pvalue The above command and range_list will generate 7 files: EUR.0.5.profile EUR.0.4.profile EUR.0.3.profile EUR.0.2.profile EUR.0.1.profile EUR.0.05.profile EUR.0.001.profile Note The default formula for PRS calculation in PLINK is: \\[ PRS_j =\\frac{ \\sum_i^NS_i*G_{ij}}{P*M_j} \\] where the effect size of SNP \\(i\\) is \\(S_i\\) ; the number of effect alleles observed in sample \\(j\\) is \\(G_{ij}\\) ; the ploidy of the sample is \\(P\\) (is generally 2 for humans); the number of samples included in the PRS is \\(N\\) ; and the number of non-missing SNPs observed in sample \\(j\\) is \\(M_j\\) . If the sample has a missing genotype for SNP \\(i\\) , then the population minor allele frequency multiplied by the ploidy ( \\(MAF_i*P\\) ) is used instead of \\(G_{ij}\\) . Accounting for Population Stratification Population structure is the principal source of confounding in GWAS and is usually accounted for by incorporating principal components (PCs) as covariates. We can incorporate PCs into our PRS analysis to account for population stratification. Again, we can calculate the PCs using plink : # First, we need to perform prunning plink \\ --bfile EUR.QC \\ --indep-pairwise 200 50 0 .25 \\ --out EUR # Then we calculate the first 6 PCs plink \\ --bfile EUR.QC \\ --extract EUR.prune.in \\ --pca 6 \\ --out EUR Note One way to select the appropriate number of PCs is to perform GWAS on the phenotype under study with different numbers of PCs. LDSC analysis can then be performed on the set of GWAS summary statistics and the GWAS that used the number of PCs that gave an LDSC intercept closest to 1 should correspond to that for which population structure was most accurately controlled for. Here the PCs have been stored in the EUR.eigenvec file and can be used as covariates in the regression model to account for population stratification. Important If the base and target samples are collected from different worldwide populations then the results from the PRS analysis may be biased (see Section 3.4 of our papper). Finding the \"best-fit\" PRS The P-value threshold that provides the \"best-fit\" PRS under the C+T method is usually unknown. To approximate the \"best-fit\" PRS, we can perform a regression between PRS calculated at a range of P-value thresholds and then select the PRS that explains the highest phenotypic variance (please see Section 4.6 of our paper on overfitting issues). This can be achieved using R as follows: detail p.threshold - c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) # Read in the phenotype file phenotype - read.table ( EUR.height , header = T ) # Read in the PCs pcs - read.table ( EUR.eigenvec , header = F ) # The default output from plink does not include a header # To make things simple, we will add the appropriate headers # (1:6 because there are 6 PCs) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) # Read in the covariates (here, it is sex) covariate - read.table ( EUR.covariate , header = T ) # Now merge the files pheno - merge ( merge ( phenotype , covariate , by = c ( FID , IID )), pcs , by = c ( FID , IID )) # We can then calculate the null model (model with PRS) using a linear regression # (as height is quantitative) null.model - lm ( Height ~ . , data = pheno [, ! colnames ( pheno ) %in% c ( FID , IID )]) # And the R2 of the null model is null.r2 - summary ( null.model ) $ r.squared prs.result - NULL for ( i in p.threshold ){ # Go through each p-value threshold prs - read.table ( paste0 ( EUR. , i , .profile ), header = T ) # Merge the prs with the phenotype matrix # We only want the FID, IID and PRS from the PRS file, therefore we only select the # relevant columns pheno.prs - merge ( pheno , prs [, c ( FID , IID , SCORE )], by = c ( FID , IID )) # Now perform a linear regression on Height with PRS and the covariates # ignoring the FID and IID from our model model - lm ( Height ~ . , data = pheno.prs [, ! colnames ( pheno.prs ) %in% c ( FID , IID )]) # model R2 is obtained as model.r2 - summary ( model ) $ r.squared # R2 of PRS is simply calculated as the model R2 minus the null R2 prs.r2 - model.r2 - null.r2 # We can also obtain the coeffcient and p-value of association of PRS as follow prs.coef - summary ( model ) $ coeff [ SCORE ,] prs.beta - as.numeric ( prs.coef [ 1 ]) prs.se - as.numeric ( prs.coef [ 2 ]) prs.p - as.numeric ( prs.coef [ 4 ]) # We can then store the results prs.result - rbind ( prs.result , data.frame ( Threshold = i , R2 = prs.r2 , P = prs.p , BETA = prs.beta , SE = prs.se )) } # Best result is: prs.result [ which.max ( prs.result $ R2 ),] q () # exit R quick p.threshold - c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) phenotype - read.table ( EUR.height , header = T ) pcs - read.table ( EUR.eigenvec , header = F ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) covariate - read.table ( EUR.covariate , header = T ) pheno - merge ( merge ( phenotype , covariate , by = c ( FID , IID )), pcs , by = c ( FID , IID )) null.r2 - summary ( lm ( Height ~ . , data = pheno [, ! colnames ( pheno ) %in% c ( FID , IID )])) $ r.squared prs.result - NULL for ( i in p.threshold ){ pheno.prs - merge ( pheno , read.table ( paste0 ( EUR. , i , .profile ), header = T )[, c ( FID , IID , SCORE )], by = c ( FID , IID )) model - summary ( lm ( Height ~ . , data = pheno.prs [, ! colnames ( pheno.prs ) %in% c ( FID , IID )])) model.r2 - model $ r.squared prs.r2 - model.r2 - null.r2 prs.coef - model $ coeff [ SCORE ,] prs.result - rbind ( prs.result , data.frame ( Threshold = i , R2 = prs.r2 , P = as.numeric ( prs.coef [ 4 ]), BETA = as.numeric ( prs.coef [ 1 ]), SE = as.numeric ( prs.coef [ 2 ]))) } print ( prs.result [ which.max ( prs.result $ R2 ),]) q () # exit R Which P-value threshold generates the \"best-fit\" PRS? 0.2 How much phenotypic variation does the \"best-fit\" PRS explain? 0.04003232","title":"PLINK"},{"location":"plink/#background","text":"In this section of the tutorial you will use four different software programs to compute PRS from the base and target data that you QC'ed in the previous two sections. On this page, you will compute PRS using the popular genetic analyses tool plink - while plink is not a dedicated PRS software, each of the steps required to compute PRS using the C+T standard approach can be performed in plink and carrying out this multi-step process can be a good way to learn the processes involved in computing PRS (which are typically performed automatically by PRS software).","title":"Background"},{"location":"plink/#required-data","text":"In the previous sections, we have generated the following files: File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.QC.valid This file contains the samples that passed all the QC EUR.height This file contains the phenotype of the samples EUR.covariate This file contains the covariates of the samples","title":"Required Data"},{"location":"plink/#update-effect-size","text":"When the effect size relates to disease risk and is thus given as an odds ratio (OR), rather than BETA (for continuous traits), then the PRS is computed as a product of ORs. To simplify this calculation, we usually take the natural logarithm of the OR so that the PRS can be computed using a simple summation instead (which can be back-transformed afterwards). We can obtain the transformed summary statistics with R : Without data.table dat - read.table ( gzfile ( Height.QC.gz ), header = T ) dat $ OR - log ( dat $ OR ) write.table ( dat , Height.QC.Transformed , quote = F , row.names = F ) q () # exit R With data.table library ( data.table ) dat - fread ( Height.QC.gz ) fwrite ( dat [, OR := log ( OR )], Height.QC.Transformed , sep = \\t ) q () # exit R Warning It may be tempting to perform the log transofrmation using awk . However, due to rounding of values performed in awk , less accurate results may be obtained. Therefore, we recommend performing the transformation in R or allow the PRS software to perform the transformation directly.","title":"Update Effect Size"},{"location":"plink/#clumping","text":"Linkage disequilibrium, which corresponds to the correlation between the genotypes of genetic variants across the genome, makes identifying the contribution from causal independent genetic variants extremely challenging. One way of approximately capturing the right level of causal signal is to perform clumping, which removes SNPs in such a way that only weakly correlated SNPs are retained but preferentially retaining the SNPs most associated with the phenotype under study. Clumping can be performed using the following command in plink : plink \\ --bfile EUR.QC \\ --clump-p1 1 \\ --clump-r2 0 .1 \\ --clump-kb 250 \\ --clump Height.QC.Transformed \\ --clump-snp-field SNP \\ --clump-field P \\ --out EUR Each of the new parameters corresponds to the following Paramter Value Description clump-p1 1 P-value threshold for a SNP to be included as an index SNP. 1 is selected such that all SNPs are include for clumping clump-r2 0.1 SNPs having \\(r^2\\) higher than 0.1 with the index SNPs will be removed clump-kb 250 SNPs within 250k of the index SNP are considered for clumping clump Height.QC.Transformed Base data (summary statistic) file containing the P-value information clump-snp-field SNP Specifies that the column SNP contains the SNP IDs clump-field P Specifies that the column P contains the P-value information A more detailed description of the clumping process can be found here Note The \\(r^2\\) values computed by --clump are based on maximum likelihood haplotype frequency estimates This will generate EUR.clumped , containing the index SNPs after clumping is performed. We can extract the index SNP ID by performing the following command: awk NR!=1{print $3} EUR.clumped EUR.valid.snp $3 because the third column contains the SNP ID Note If your target data are small (e.g. N 500) then you can use the 1000 Genomes Project samples for the LD calculation. Make sure to use the population that most closely reflects represents the base sample.","title":"Clumping"},{"location":"plink/#generate-prs","text":"plink provides a convenient function --score and --q-score-range for calculating polygenic scores. We will need three files: The base data file: Height.QC.Transformed A file containing SNP IDs and their corresponding P-values ( $1 because SNP ID is located in the first column; $8 because the P-value is located in the eighth column) awk {print $1,$8} Height.QC.Transformed SNP.pvalue A file containing the different P-value thresholds for inclusion of SNPs in the PRS. Here calculate PRS corresponding to a few thresholds for illustration purposes: echo 0.001 0 0.001 range_list echo 0.05 0 0.05 range_list echo 0.1 0 0.1 range_list echo 0.2 0 0.2 range_list echo 0.3 0 0.3 range_list echo 0.4 0 0.4 range_list echo 0.5 0 0.5 range_list The format of the range_list file should be as follows: Name of Threshold Lower bound Upper Bound Note The threshold boundaries are inclusive. For example, for the 0.05 threshold, we include all SNPs with P-value from 0 to 0.05 , including any SNPs with P-value equal to 0.05 . We can then calculate the PRS with the following plink command: plink \\ --bfile EUR.QC \\ --score Height.QC.Transformed 1 4 11 header \\ --q-score-range range_list SNP.pvalue \\ --extract EUR.valid.snp \\ --out EUR The meaning of the new parameters are as follows: Paramter Value Description score Height.QC.Transformed 1 4 11 header We read from the Height.QC.Transformed file, assuming that the 1 st column is the SNP ID; 4 th column is the effective allele information; the 11 th column is the effect size estimate; and that the file contains a header q-score-range range_test SNP.pvalue We want to calculate PRS based on the thresholds defined in range_test , where the threshold values (P-values) were stored in SNP.pvalue The above command and range_list will generate 7 files: EUR.0.5.profile EUR.0.4.profile EUR.0.3.profile EUR.0.2.profile EUR.0.1.profile EUR.0.05.profile EUR.0.001.profile Note The default formula for PRS calculation in PLINK is: \\[ PRS_j =\\frac{ \\sum_i^NS_i*G_{ij}}{P*M_j} \\] where the effect size of SNP \\(i\\) is \\(S_i\\) ; the number of effect alleles observed in sample \\(j\\) is \\(G_{ij}\\) ; the ploidy of the sample is \\(P\\) (is generally 2 for humans); the number of samples included in the PRS is \\(N\\) ; and the number of non-missing SNPs observed in sample \\(j\\) is \\(M_j\\) . If the sample has a missing genotype for SNP \\(i\\) , then the population minor allele frequency multiplied by the ploidy ( \\(MAF_i*P\\) ) is used instead of \\(G_{ij}\\) .","title":"Generate PRS"},{"location":"plink/#accounting-for-population-stratification","text":"Population structure is the principal source of confounding in GWAS and is usually accounted for by incorporating principal components (PCs) as covariates. We can incorporate PCs into our PRS analysis to account for population stratification. Again, we can calculate the PCs using plink : # First, we need to perform prunning plink \\ --bfile EUR.QC \\ --indep-pairwise 200 50 0 .25 \\ --out EUR # Then we calculate the first 6 PCs plink \\ --bfile EUR.QC \\ --extract EUR.prune.in \\ --pca 6 \\ --out EUR Note One way to select the appropriate number of PCs is to perform GWAS on the phenotype under study with different numbers of PCs. LDSC analysis can then be performed on the set of GWAS summary statistics and the GWAS that used the number of PCs that gave an LDSC intercept closest to 1 should correspond to that for which population structure was most accurately controlled for. Here the PCs have been stored in the EUR.eigenvec file and can be used as covariates in the regression model to account for population stratification. Important If the base and target samples are collected from different worldwide populations then the results from the PRS analysis may be biased (see Section 3.4 of our papper).","title":"Accounting for Population Stratification"},{"location":"plink/#finding-the-best-fit-prs","text":"The P-value threshold that provides the \"best-fit\" PRS under the C+T method is usually unknown. To approximate the \"best-fit\" PRS, we can perform a regression between PRS calculated at a range of P-value thresholds and then select the PRS that explains the highest phenotypic variance (please see Section 4.6 of our paper on overfitting issues). This can be achieved using R as follows: detail p.threshold - c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) # Read in the phenotype file phenotype - read.table ( EUR.height , header = T ) # Read in the PCs pcs - read.table ( EUR.eigenvec , header = F ) # The default output from plink does not include a header # To make things simple, we will add the appropriate headers # (1:6 because there are 6 PCs) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) # Read in the covariates (here, it is sex) covariate - read.table ( EUR.covariate , header = T ) # Now merge the files pheno - merge ( merge ( phenotype , covariate , by = c ( FID , IID )), pcs , by = c ( FID , IID )) # We can then calculate the null model (model with PRS) using a linear regression # (as height is quantitative) null.model - lm ( Height ~ . , data = pheno [, ! colnames ( pheno ) %in% c ( FID , IID )]) # And the R2 of the null model is null.r2 - summary ( null.model ) $ r.squared prs.result - NULL for ( i in p.threshold ){ # Go through each p-value threshold prs - read.table ( paste0 ( EUR. , i , .profile ), header = T ) # Merge the prs with the phenotype matrix # We only want the FID, IID and PRS from the PRS file, therefore we only select the # relevant columns pheno.prs - merge ( pheno , prs [, c ( FID , IID , SCORE )], by = c ( FID , IID )) # Now perform a linear regression on Height with PRS and the covariates # ignoring the FID and IID from our model model - lm ( Height ~ . , data = pheno.prs [, ! colnames ( pheno.prs ) %in% c ( FID , IID )]) # model R2 is obtained as model.r2 - summary ( model ) $ r.squared # R2 of PRS is simply calculated as the model R2 minus the null R2 prs.r2 - model.r2 - null.r2 # We can also obtain the coeffcient and p-value of association of PRS as follow prs.coef - summary ( model ) $ coeff [ SCORE ,] prs.beta - as.numeric ( prs.coef [ 1 ]) prs.se - as.numeric ( prs.coef [ 2 ]) prs.p - as.numeric ( prs.coef [ 4 ]) # We can then store the results prs.result - rbind ( prs.result , data.frame ( Threshold = i , R2 = prs.r2 , P = prs.p , BETA = prs.beta , SE = prs.se )) } # Best result is: prs.result [ which.max ( prs.result $ R2 ),] q () # exit R quick p.threshold - c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) phenotype - read.table ( EUR.height , header = T ) pcs - read.table ( EUR.eigenvec , header = F ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) covariate - read.table ( EUR.covariate , header = T ) pheno - merge ( merge ( phenotype , covariate , by = c ( FID , IID )), pcs , by = c ( FID , IID )) null.r2 - summary ( lm ( Height ~ . , data = pheno [, ! colnames ( pheno ) %in% c ( FID , IID )])) $ r.squared prs.result - NULL for ( i in p.threshold ){ pheno.prs - merge ( pheno , read.table ( paste0 ( EUR. , i , .profile ), header = T )[, c ( FID , IID , SCORE )], by = c ( FID , IID )) model - summary ( lm ( Height ~ . , data = pheno.prs [, ! colnames ( pheno.prs ) %in% c ( FID , IID )])) model.r2 - model $ r.squared prs.r2 - model.r2 - null.r2 prs.coef - model $ coeff [ SCORE ,] prs.result - rbind ( prs.result , data.frame ( Threshold = i , R2 = prs.r2 , P = as.numeric ( prs.coef [ 4 ]), BETA = as.numeric ( prs.coef [ 1 ]), SE = as.numeric ( prs.coef [ 2 ]))) } print ( prs.result [ which.max ( prs.result $ R2 ),]) q () # exit R Which P-value threshold generates the \"best-fit\" PRS? 0.2 How much phenotypic variation does the \"best-fit\" PRS explain? 0.04003232","title":"Finding the \"best-fit\" PRS"},{"location":"plink_visual/","text":"Plotting the Results The PRS results corresponding to a range of P-value thresholds obtained by application of the C+T PRS method (eg. using PLINK or PRSice-2) can be visualised using R as follows: Note We will be using prs.result , which was generated in the previous section Without ggplot2 # We strongly recommend the use of ggplot2. Only follow this code if you # are desperate. # Specify that we want to generate plot in EUR.height.bar.png png ( EUR.height.bar.png , height = 10 , width = 10 , res = 300 , unit = in ) # First, obtain the colorings based on the p-value col - suppressWarnings ( colorRampPalette ( c ( dodgerblue , firebrick ))) # We want the color gradient to match the ranking of p-values prs.result - prs.result [ order ( - log10 ( prs.result $ P )),] prs.result $ color - col ( nrow ( prs.result )) prs.result - prs.result [ order ( prs.result $ Threshold ),] # generate a pretty format for p-value output prs.result $ print.p - round ( prs.result $ P , digits = 3 ) prs.result $ print.p [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ] - format ( prs.result $ P [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ], digits = 2 ) prs.result $ print.p - sub ( e , *x*10^ , prs.result $ print.p ) # Generate the axis labels xlab - expression ( italic ( P ) - value ~ threshold ~ ( italic ( P )[ T ])) ylab - expression ( paste ( PRS model fit: , R ^ 2 )) # Setup the drawing area layout ( t ( 1 : 2 ), widths = c ( 8.8 , 1.2 )) par ( cex.lab = 1.5 , cex.axis = 1.25 , font.lab = 2 , oma = c ( 0 , 0.5 , 0 , 0 ), mar = c ( 4 , 6 , 0.5 , 0.5 )) # Plotting the bars b - barplot ( height = prs.result $ R2 , col = prs.result $ color , border = NA , ylim = c ( 0 , max ( prs.result $ R2 ) * 1.25 ), axes = F , ann = F ) # Plot the axis labels and axis ticks odd - seq ( 0 , nrow ( prs.result ) +1 , 2 ) even - seq ( 1 , nrow ( prs.result ), 2 ) axis ( side = 1 , at = b [ odd ], labels = prs.result $ Threshold [ odd ], lwd = 2 ) axis ( side = 1 , at = b [ even ], labels = prs.result $ Threshold [ even ], lwd = 2 ) axis ( side = 1 , at = c ( 0 , b [ 1 ], 2 * b [ length ( b )] - b [ length ( b ) -1 ]), labels = c ( , , ), lwd = 2 , lwd.tick = 0 ) # Write the p-value on top of each bar text ( parse ( text = paste ( prs.result $ print.p )), x = b +0.1 , y = prs.result $ R2 + ( max ( prs.result $ R2 ) * 1.05 - max ( prs.result $ R2 )), srt = 45 ) # Now plot the axis lines box ( bty = L , lwd = 2 ) axis ( 2 , las = 2 , lwd = 2 ) # Plot the axis titles title ( ylab = ylab , line = 4 , cex.lab = 1.5 , font = 2 ) title ( xlab = xlab , line = 2.5 , cex.lab = 1.5 , font = 2 ) # Generate plot area for the legend par ( cex.lab = 1.5 , cex.axis = 1.25 , font.lab = 2 , mar = c ( 20 , 0 , 20 , 4 )) prs.result - prs.result [ order ( - log10 ( prs.result $ P )),] image ( 1 , - log10 ( prs.result $ P ), t ( seq_along ( - log10 ( prs.result $ P ))), col = prs.result $ color , axes = F , ann = F ) axis ( 4 , las = 2 , xaxs = r , yaxs = r , tck = 0.2 , col = white ) # plot legend title title ( bquote ( atop ( - log [ 10 ] ~ model , italic ( P ) - value ), ), line = 2 , cex = 1.5 , font = 2 , adj = 0 ) # write the plot to file dev.off () q () # exit R ggplot2 # ggplot2 is a handy package for plotting library ( ggplot2 ) # generate a pretty format for p-value output prs.result $ print.p - round ( prs.result $ P , digits = 3 ) prs.result $ print.p [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ] - format ( prs.result $ P [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ], digits = 2 ) prs.result $ print.p - sub ( e , *x*10^ , prs.result $ print.p ) # Initialize ggplot, requiring the threshold as the x-axis (use factor so that it is uniformly distributed) ggplot ( data = prs.result , aes ( x = factor ( Threshold ), y = R2 )) + # Specify that we want to print p-value on top of the bars geom_text ( aes ( label = paste ( print.p )), vjust = -1.5 , hjust = 0 , angle = 45 , cex = 4 , parse = T ) + # Specify the range of the plot, *1.25 to provide enough space for the p-values scale_y_continuous ( limits = c ( 0 , max ( prs.result $ R2 ) * 1.25 )) + # Specify the axis labels xlab ( expression ( italic ( P ) - value ~ threshold ~ ( italic ( P )[ T ]))) + ylab ( expression ( paste ( PRS model fit: , R ^ 2 ))) + # Draw a bar plot geom_bar ( aes ( fill = - log10 ( P )), stat = identity ) + # Specify the colors scale_fill_gradient2 ( low = dodgerblue , high = firebrick , mid = dodgerblue , midpoint = 1e-4 , name = bquote ( atop ( - log [ 10 ] ~ model , italic ( P ) - value ),) ) + # Some beautification of the plot theme_classic () + theme ( axis.title = element_text ( face = bold , size = 18 ), axis.text = element_text ( size = 14 ), legend.title = element_text ( face = bold , size = 18 ), legend.text = element_text ( size = 14 ), axis.text.x = element_text ( angle = 45 , hjust = 1 ) ) # save the plot ggsave ( EUR.height.bar.png , height = 7 , width = 7 ) q () # exit R An example bar plot generated using ggplot2 In addition, we can visualise the relationship between the \"best-fit\" PRS (which may have been obtained from any of the PRS programs) and the phenotype of interest, coloured according to sex: Without ggplot2 # Read in the files prs - read.table ( EUR.0.2.profile , header = T ) height - read.table ( EUR.height , header = T ) sex - read.table ( EUR.covariate , header = T ) # Rename the sex sex $ Sex - as.factor ( sex $ Sex ) levels ( sex $ Sex ) - c ( Male , Female ) # Merge the files dat - merge ( merge ( prs , height ), sex ) # Start plotting plot ( x = dat $ SCORE , y = dat $ Height , col = white , xlab = Polygenic Score , ylab = Height ) with ( subset ( dat , Sex == Male ), points ( x = SCORE , y = Height , col = red )) with ( subset ( dat , Sex == Female ), points ( x = SCORE , y = Height , col = blue )) q () # exit R ggplot2 library ( ggplot2 ) # Read in the files prs - read.table ( EUR.0.2.profile , header = T ) height - read.table ( EUR.height , header = T ) sex - read.table ( EUR.covariate , header = T ) # Rename the sex sex $ Sex - as.factor ( sex $ Sex ) levels ( sex $ Sex ) - c ( Male , Female ) # Merge the files dat - merge ( merge ( prs , height ), sex ) # Start plotting ggplot ( dat , aes ( x = SCORE , y = Height , color = Sex )) + geom_point () + theme_classic () + labs ( x = Polygenic Score , y = Height ) q () # exit R An example scatter plot generated using ggplot2 Programs such as PRSice-2 and bigsnpr include numerous options for plotting PRS results.","title":"4. Visualizing PRS Results"},{"location":"plink_visual/#plotting-the-results","text":"The PRS results corresponding to a range of P-value thresholds obtained by application of the C+T PRS method (eg. using PLINK or PRSice-2) can be visualised using R as follows: Note We will be using prs.result , which was generated in the previous section Without ggplot2 # We strongly recommend the use of ggplot2. Only follow this code if you # are desperate. # Specify that we want to generate plot in EUR.height.bar.png png ( EUR.height.bar.png , height = 10 , width = 10 , res = 300 , unit = in ) # First, obtain the colorings based on the p-value col - suppressWarnings ( colorRampPalette ( c ( dodgerblue , firebrick ))) # We want the color gradient to match the ranking of p-values prs.result - prs.result [ order ( - log10 ( prs.result $ P )),] prs.result $ color - col ( nrow ( prs.result )) prs.result - prs.result [ order ( prs.result $ Threshold ),] # generate a pretty format for p-value output prs.result $ print.p - round ( prs.result $ P , digits = 3 ) prs.result $ print.p [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ] - format ( prs.result $ P [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ], digits = 2 ) prs.result $ print.p - sub ( e , *x*10^ , prs.result $ print.p ) # Generate the axis labels xlab - expression ( italic ( P ) - value ~ threshold ~ ( italic ( P )[ T ])) ylab - expression ( paste ( PRS model fit: , R ^ 2 )) # Setup the drawing area layout ( t ( 1 : 2 ), widths = c ( 8.8 , 1.2 )) par ( cex.lab = 1.5 , cex.axis = 1.25 , font.lab = 2 , oma = c ( 0 , 0.5 , 0 , 0 ), mar = c ( 4 , 6 , 0.5 , 0.5 )) # Plotting the bars b - barplot ( height = prs.result $ R2 , col = prs.result $ color , border = NA , ylim = c ( 0 , max ( prs.result $ R2 ) * 1.25 ), axes = F , ann = F ) # Plot the axis labels and axis ticks odd - seq ( 0 , nrow ( prs.result ) +1 , 2 ) even - seq ( 1 , nrow ( prs.result ), 2 ) axis ( side = 1 , at = b [ odd ], labels = prs.result $ Threshold [ odd ], lwd = 2 ) axis ( side = 1 , at = b [ even ], labels = prs.result $ Threshold [ even ], lwd = 2 ) axis ( side = 1 , at = c ( 0 , b [ 1 ], 2 * b [ length ( b )] - b [ length ( b ) -1 ]), labels = c ( , , ), lwd = 2 , lwd.tick = 0 ) # Write the p-value on top of each bar text ( parse ( text = paste ( prs.result $ print.p )), x = b +0.1 , y = prs.result $ R2 + ( max ( prs.result $ R2 ) * 1.05 - max ( prs.result $ R2 )), srt = 45 ) # Now plot the axis lines box ( bty = L , lwd = 2 ) axis ( 2 , las = 2 , lwd = 2 ) # Plot the axis titles title ( ylab = ylab , line = 4 , cex.lab = 1.5 , font = 2 ) title ( xlab = xlab , line = 2.5 , cex.lab = 1.5 , font = 2 ) # Generate plot area for the legend par ( cex.lab = 1.5 , cex.axis = 1.25 , font.lab = 2 , mar = c ( 20 , 0 , 20 , 4 )) prs.result - prs.result [ order ( - log10 ( prs.result $ P )),] image ( 1 , - log10 ( prs.result $ P ), t ( seq_along ( - log10 ( prs.result $ P ))), col = prs.result $ color , axes = F , ann = F ) axis ( 4 , las = 2 , xaxs = r , yaxs = r , tck = 0.2 , col = white ) # plot legend title title ( bquote ( atop ( - log [ 10 ] ~ model , italic ( P ) - value ), ), line = 2 , cex = 1.5 , font = 2 , adj = 0 ) # write the plot to file dev.off () q () # exit R ggplot2 # ggplot2 is a handy package for plotting library ( ggplot2 ) # generate a pretty format for p-value output prs.result $ print.p - round ( prs.result $ P , digits = 3 ) prs.result $ print.p [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ] - format ( prs.result $ P [ ! is.na ( prs.result $ print.p ) prs.result $ print.p == 0 ], digits = 2 ) prs.result $ print.p - sub ( e , *x*10^ , prs.result $ print.p ) # Initialize ggplot, requiring the threshold as the x-axis (use factor so that it is uniformly distributed) ggplot ( data = prs.result , aes ( x = factor ( Threshold ), y = R2 )) + # Specify that we want to print p-value on top of the bars geom_text ( aes ( label = paste ( print.p )), vjust = -1.5 , hjust = 0 , angle = 45 , cex = 4 , parse = T ) + # Specify the range of the plot, *1.25 to provide enough space for the p-values scale_y_continuous ( limits = c ( 0 , max ( prs.result $ R2 ) * 1.25 )) + # Specify the axis labels xlab ( expression ( italic ( P ) - value ~ threshold ~ ( italic ( P )[ T ]))) + ylab ( expression ( paste ( PRS model fit: , R ^ 2 ))) + # Draw a bar plot geom_bar ( aes ( fill = - log10 ( P )), stat = identity ) + # Specify the colors scale_fill_gradient2 ( low = dodgerblue , high = firebrick , mid = dodgerblue , midpoint = 1e-4 , name = bquote ( atop ( - log [ 10 ] ~ model , italic ( P ) - value ),) ) + # Some beautification of the plot theme_classic () + theme ( axis.title = element_text ( face = bold , size = 18 ), axis.text = element_text ( size = 14 ), legend.title = element_text ( face = bold , size = 18 ), legend.text = element_text ( size = 14 ), axis.text.x = element_text ( angle = 45 , hjust = 1 ) ) # save the plot ggsave ( EUR.height.bar.png , height = 7 , width = 7 ) q () # exit R An example bar plot generated using ggplot2 In addition, we can visualise the relationship between the \"best-fit\" PRS (which may have been obtained from any of the PRS programs) and the phenotype of interest, coloured according to sex: Without ggplot2 # Read in the files prs - read.table ( EUR.0.2.profile , header = T ) height - read.table ( EUR.height , header = T ) sex - read.table ( EUR.covariate , header = T ) # Rename the sex sex $ Sex - as.factor ( sex $ Sex ) levels ( sex $ Sex ) - c ( Male , Female ) # Merge the files dat - merge ( merge ( prs , height ), sex ) # Start plotting plot ( x = dat $ SCORE , y = dat $ Height , col = white , xlab = Polygenic Score , ylab = Height ) with ( subset ( dat , Sex == Male ), points ( x = SCORE , y = Height , col = red )) with ( subset ( dat , Sex == Female ), points ( x = SCORE , y = Height , col = blue )) q () # exit R ggplot2 library ( ggplot2 ) # Read in the files prs - read.table ( EUR.0.2.profile , header = T ) height - read.table ( EUR.height , header = T ) sex - read.table ( EUR.covariate , header = T ) # Rename the sex sex $ Sex - as.factor ( sex $ Sex ) levels ( sex $ Sex ) - c ( Male , Female ) # Merge the files dat - merge ( merge ( prs , height ), sex ) # Start plotting ggplot ( dat , aes ( x = SCORE , y = Height , color = Sex )) + geom_point () + theme_classic () + labs ( x = Polygenic Score , y = Height ) q () # exit R An example scatter plot generated using ggplot2 Programs such as PRSice-2 and bigsnpr include numerous options for plotting PRS results.","title":"Plotting the Results"},{"location":"prsice/","text":"Over the following three pages you can run three dedicated PRS programs, which automate many of the steps from the previous page that used a sequence of PLINK functions (plus some QC steps from earlier pages). On this page you will run a PRS analysis using PRSice-2, which implements the standard C+T method. This analysis assumes that you have the following files: File Name Description GIANT.height.gz The original base data file. PRSice-2 can apply INFO and MAF filtering to these base summary statistic data directly EUR.QC.bed This file contains the genotype data that passed the QC steps EUR.QC.bim This file contains the list of SNPs that passed the QC steps EUR.QC.fam This file contains the samples that passed the QC steps EUR.valid.sample This file contains the samples that passed the QC steps EUR.height This file contains the phenotype data of the samples EUR.covariate This file contains the covariates of the samples EUR.eigenvec This file contains the principal components (PCs) of the samples And PRSice-2 , which can be downloaded from: Operating System Link Linux 64-bit v2.2.5 OS X 64-bit v2.2.5 Windows 32-bit v2.2.5 Windows 64-bit v2.2.5 In this tutorial, you will only need PRSice.R and PRSice_XXX where XXX is the operation system Running PRS analysis To run PRSice-2 we need a single covariate file, and therefore our covariate file and PCs file should be combined. This can be done with R as follows: covariate - read.table ( EUR.covariate , header = T ) pcs - read.table ( EUR.eigenvec , header = F ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) cov - merge ( covariate , pcs , by = c ( FID , IID )) write.table ( cov , EUR.cov , quote = F , row.names = F ) which generates EUR.cov . PRSice-2 can then be run to obtain the PRS results as follows: Linux Rscript PRSice.R \\ --prsice PRSice_linux \\ --base Height.QC.gz \\ --target EUR \\ --keep EUR.QC.rel.id \\ --extract EUR.QC.snplist \\ --binary-target F \\ --pheno EUR.height \\ --cov EUR.cov \\ --base-maf MAF,0.01 \\ --base-info INFO,0.8 \\ --out EUR OS X Rscript PRSice.R \\ --prsice PRSice_mac \\ --base Height.QC.gz \\ --target EUR \\ --keep EUR.QC.rel.id \\ --extract EUR.QC.snplist \\ --binary-target F \\ --pheno EUR.height \\ --cov EUR.cov \\ --base-maf MAF,0.01 \\ --base-info INFO,0.8 \\ --out EUR Windows Rscript PRSice.R ^ --prsice PRSice_win64.exe ^ --base Height.QC.gz ^ --target EUR ^ --keep EUR.QC.rel.id ^ --extract EUR.QC.snplist ^ --binary-target F ^ --pheno EUR.height ^ --cov EUR.cov ^ --base-maf MAF,0.05 ^ --base-info INFO,0.8 ^ --out EUR This will automatically perform \"high-resolution scoring\" and generate the \"best-fit\" PRS (in EUR.best ), with associated plots of the results. Users should read Section 4.6 of our paper to learn more about issues relating to overfitting in PRS analyses.","title":"PRSice-2"},{"location":"prsice/#running-prs-analysis","text":"To run PRSice-2 we need a single covariate file, and therefore our covariate file and PCs file should be combined. This can be done with R as follows: covariate - read.table ( EUR.covariate , header = T ) pcs - read.table ( EUR.eigenvec , header = F ) colnames ( pcs ) - c ( FID , IID , paste0 ( PC , 1 : 6 )) cov - merge ( covariate , pcs , by = c ( FID , IID )) write.table ( cov , EUR.cov , quote = F , row.names = F ) which generates EUR.cov . PRSice-2 can then be run to obtain the PRS results as follows: Linux Rscript PRSice.R \\ --prsice PRSice_linux \\ --base Height.QC.gz \\ --target EUR \\ --keep EUR.QC.rel.id \\ --extract EUR.QC.snplist \\ --binary-target F \\ --pheno EUR.height \\ --cov EUR.cov \\ --base-maf MAF,0.01 \\ --base-info INFO,0.8 \\ --out EUR OS X Rscript PRSice.R \\ --prsice PRSice_mac \\ --base Height.QC.gz \\ --target EUR \\ --keep EUR.QC.rel.id \\ --extract EUR.QC.snplist \\ --binary-target F \\ --pheno EUR.height \\ --cov EUR.cov \\ --base-maf MAF,0.01 \\ --base-info INFO,0.8 \\ --out EUR Windows Rscript PRSice.R ^ --prsice PRSice_win64.exe ^ --base Height.QC.gz ^ --target EUR ^ --keep EUR.QC.rel.id ^ --extract EUR.QC.snplist ^ --binary-target F ^ --pheno EUR.height ^ --cov EUR.cov ^ --base-maf MAF,0.05 ^ --base-info INFO,0.8 ^ --out EUR This will automatically perform \"high-resolution scoring\" and generate the \"best-fit\" PRS (in EUR.best ), with associated plots of the results. Users should read Section 4.6 of our paper to learn more about issues relating to overfitting in PRS analyses.","title":"Running PRS analysis"},{"location":"target/","text":"Obtaining the target data Target data consist of individual-level genotype-phenotype data, usually generated within your lab/department/collaboration. For this tutorial, we have simulated some genotype-phenotype data using the 1000 Genomes Project European samples. You can download the data here or you can download the data using the following command: curl https://github.com/choishingwan/PRS-Tutorial/raw/master/resources/EUR.zip -L -O Unzip the data as follow: unzip EUR.zip Note Install the program PLINK and include its location in your PATH directory, which allows us to use plink instead of ./plink in the commands below. If PLINK is not in your PATH directory and is instead in your working directory, then replace all instances of plink in the tutorial with ./plink . QC checklist: Target data Below are the QC steps that comprise the QC checklist for the target data. # Sample size We recommend that users only perform PRS analyses on target data of at least 100 individuals. The sample size of our target data here is 503 individuals. # File transfer Usually we do not need to download and transfer the target data file because it is typically generated locally. However, the file should contain an md5sum code in case we send the data file to collaborators who may want to confirm that the file has not changed during the transfer. What is the md5sum code for each of the target files? File md5sum EUR.bed 940f5a760b41270662eba6264b262a2d EUR.bim a528020cc2448aa04a7499f13bf9f16a EUR.covariate afff13f8f9e15815f2237a62b8bec00b EUR.fam 17e8184fb03c690db6980bb7499d4982 EUR.height 052beb4cae32ac7673f1d6b9e854c85b # Genome build As stated in the base data section, the genome build for our base and target data is the same, as it should be. # Standard GWAS QC The target data must be quality controlled to at least the standards implemented in GWAS studies, e.g. removing SNPs with low genotyping rate, low minor allele frequency, out of Hardy-Weinberg Equilibrium, removing individuals with low genotyping rate (see Marees et al ). The following plink command applies some of these QC metrics to the target data: plink \\ --bfile EUR \\ --maf 0 .05 \\ --hwe 1e-6 \\ --geno 0 .01 \\ --mind 0 .01 \\ --write-snplist \\ --make-just-fam \\ --out EUR.QC Each of the parameters corresponds to the following Paramter Value Description bfile EUR Informs plink that the input genotype files should have a prefix of EUR maf 0.05 Removes all SNPs with minor allele frequency less than 0.05. Genotyping errors typically have a larger influence on SNPs with low MAF. Studies with large sample sizes could apply a lower MAF threshold hwe 1e-6 Removes SNPs with low P-value from the Hardy-Weinberg Equilibrium Fisher's exact or chi-squared test. SNPs with significant P-values from the HWE test are more likely affected by genotyping error or the effects of natural selection. Filtering should be performed on the control samples to avoid filtering SNPs that are causal (under selection in cases) geno 0.01 Excludes SNPs that are missing in a high fraction of subjects. A two-stage filtering process is usually performed (see Marees et al ). mind 0.01 Excludes individuals who have a high rate of genotype missingness, since this may indicate problems in the DNA sample or processing. (see Marees et al for more details). make-just-fam - Informs plink to only generate the QC'ed sample name to avoid generating the .bed file. write-snplist - Informs plink to only generate the QC'ed SNP list to avoid generating the .bed file. out EUR Informs plink that all output should have a prefix of EUR How many SNPs and samples were filtered? 5 samples were removed due to a high rate of genotype missingness 1 SNP were removed due to missing genotype data 872 SNPs were removed due to being out of Hardy-Weinberg Equilibrium 242,459 SNPs were removed due to low minor allele frequency Note Normally, we can generate a new genotype file using the new sample list. However, this will use up a lot of storage space. Using plink 's --extract , --exclude , --keep , --remove , --make-just-fam and --write-snplist functions, we can work solely on the list of samples and SNPs without duplicating the genotype file, reducing the storage space usage. Very high or low heterozygosity rates in individuals could be due to DNA contamination or to high levels of inbreeding. Therefore, samples with extreme heterozygosity are typically removed prior to downstream analyses. First, we perform prunning to remove highly correlated SNPs: plink \\ --bfile EUR \\ --keep EUR.QC.fam \\ --extract EUR.QC.snplist \\ --indep-pairwise 200 50 0 .25 \\ --out EUR.QC This will generate two files 1) EUR.QC.prune.in and 2) EUR.QC.prune.out .All SNPs within EUR.QC.prune.in have a pairwise \\(r^2 0.25\\) . Heterozygosity rates can then be computed using plink : plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.QC.fam \\ --het \\ --out EUR.QC This will generate the EUR.QC.het file, which contains F coefficient estimates for assessing heterozygosity. We will remove individuals with F coefficients that are more than 3 standard deviation (SD) units from the mean, which can be performed using the following R command (assuming that you have R downloaded, then you can open an R session by typing R in your terminal): Without library dat - read.table ( EUR.QC.het , header = T ) # Read in the EUR.het file, specify it has header m - mean ( dat $ F ) # Calculate the mean s - sd ( dat $ F ) # Calculate the SD valid - subset ( dat , F = m +3 * s F = m -3 * s ) # Get any samples with F coefficient within 3 SD of the population mean write.table ( valid [, c ( 1 , 2 )], EUR.valid.sample , quote = F , row.names = F ) # print FID and IID for valid samples q () # exit R With data.table library ( data.table ) # Read in file dat - fread ( EUR.QC.het ) # Get samples with F coefficient within 3 SD of the population mean valid - dat [ F = mean ( F ) +3 * sd ( F ) F = mean ( F ) -3 * sd ( F )] # print FID and IID for valid samples fwrite ( valid [, c ( FID , IID )], EUR.valid.sample , sep = \\t ) q () # exit R How many samples were excluded due to high heterozygosity rate? 7 samples were excluded # Ambiguous SNPs There were removed during the base data QC # Sex chromosomes Sometimes sample mislabelling can occur, which may lead to invalid results. A good indication of a mislabelled sample is a mismatch between biological sex and reported sex. If the biological sex does not match up with the reported sex, then the sample may have been mislabelled. Before performing a sex check, pruning should be performed (see here ). A sex check can then easily be conducted using plink plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.valid.sample \\ --check-sex \\ --out EUR.QC This will generate a file called EUR.QC.sexcheck containing the F-statistics for each individual. Individuals are typically called as being biologically male if the F-statistic is 0.8 and biologically female if F 0.2. Without library # Read in file valid - read.table ( EUR.valid.sample , header = T ) dat - read.table ( EUR.QC.sexcheck , header = T ) valid - subset ( dat , STATUS == OK FID %in% valid $ FID ) write.table ( valid [, c ( FID , IID )], EUR.QC.valid , row.names = F , col.names = F , sep = \\t , quote = F ) q () # exit R With data.table library ( data.table ) # Read in file valid - fread ( EUR.valid.sample ) dat - fread ( EUR.QC.sexcheck )[ FID %in% valid $ FID ] fwrite ( dat [ STATUS == OK , c ( FID , IID )], EUR.QC.valid , sep = \\t ) q () # exit R How many samples were excluded due mismatched Sex information? 2 samples were excluded # Mismatching genotypes In addition, when there are non-ambiguous mismatches in allele coding between the data sets, such as A/C in the base and G/T in the target data, then this can be resolved by \u2018flipping\u2019 the alleles in the target data to their complementary alleles. This can be achieved with the following steps: 1. Load the bim file, the GIANT summary statistic and the QC SNP list into R Without data.table # Read in bim file bim - read.table ( EUR.bim ) colnames ( bim ) - c ( CHR , SNP , CM , BP , B.A1 , B.A2 ) # Read in QCed SNPs qc - read.table ( EUR.QC.snplist , header = F , stringsAsFactors = F ) # Read in GIANT data height - read.table ( gzfile ( Height.QC.gz ), header = T , stringsAsFactors = F , sep = \\t ) # Change all alleles to upper case for easy comparison height $ A1 - toupper ( height $ A1 ) height $ A2 - toupper ( height $ A2 ) bim $ B.A1 - toupper ( bim $ B.A1 ) bim $ B.A2 - toupper ( bim $ B.A2 ) With data.table library ( data.table ) # Read in bim file bim - fread ( EUR.bim ) setnames ( bim , colnames ( bim ), c ( CHR , SNP , CM , BP , B.A1 , B.A2 )) # Read in GIANT data (require data.table v1.12.0+) height - fread ( Height.QC.gz ) # Change all alleles to upper case for easy comparison height [, c ( A1 , A2 ) := list ( toupper ( A1 ), toupper ( A2 ))] bim [, c ( B.A1 , B.A2 ) := list ( toupper ( B.A1 ), toupper ( B.A2 ))] # Read in QCed SNPs qc - fread ( EUR.QC.snplist , header = F ) 2. Identify SNPs that require strand flipping Without data.table # Merge GIANT with target info - merge ( bim , height , by = c ( SNP , CHR , BP )) # Filter QCed SNPs info - info [ info $ SNP %in% qc $ V1 ,] # Function for finding the complementary allele complement - function ( x ) { switch ( x , A = T , C = G , T = A , G = C , return ( NA ) ) } # Get SNPs that have the same alleles across base and target info.match - subset ( info , A1 == B.A1 A2 == B.A2 ) # Identify SNPs that are complementary between base and target info $ C.A1 - sapply ( info $ B.A1 , complement ) info $ C.A2 - sapply ( info $ B.A2 , complement ) info.complement - subset ( info , A1 == C.A1 A2 == C.A2 ) # Update these allele coding in the bim file bim [ bim $ SNP %in% info.complement $ SNP ,] $ B.A1 - sapply ( bim [ bim $ SNP %in% info.complement $ SNP ,] $ B.A1 , complement ) bim [ bim $ SNP %in% info.complement $ SNP ,] $ B.A2 - sapply ( bim [ bim $ SNP %in% info.complement $ SNP ,] $ B.A2 , complement ) With data.table # Merge GIANT with target info - merge ( bim , height , by = c ( SNP , CHR , BP )) info - info [ SNP %in% qc $ V1 ] # Function for calculating the complementary allele complement - function ( x ){ switch ( x , A = T , C = G , T = A , G = C , return ( NA ) ) } # Identify SNPs that are complementary between base and target com.snps - info [ sapply ( B.A1 , complement ) == A1 sapply ( B.A2 , complement ) == A2 , SNP ] # Now update the bim file bim [ SNP %in% com.snps , c ( B.A1 , B.A2 ) := list ( sapply ( B.A1 , complement ), sapply ( B.A2 , complement ))] # And update the info structure info [ SNP %in% com.snps , c ( B.A1 , B.A2 ) := list ( sapply ( B.A1 , complement ), sapply ( B.A2 , complement ))] 3. Identify SNPs that require recoding in the target (to ensure the coding allele in the target data is the effective allele in the base summary statistic) Without data.table # identify SNPs that need recoding info.recode - subset ( info , A1 == B.A2 A2 == B.A1 ) # identify SNPs that need recoding complement info.crecode - subset ( info , A1 == C.A2 A2 == C.A1 ) # Update these allele coding in the bim file com.snps - bim $ SNP %in% info.crecode $ SNP tmp - bim [ com.snps ,] $ B.A1 bim [ com.snps ,] $ B.A1 - as.character ( sapply ( bim [ com.snps ,] $ B.A2 , complement )) bim [ com.snps ,] $ B.A2 - as.character ( sapply ( tmp , complement )) # Output updated bim file write.table ( bim , EUR.QC.adj.bim , quote = F , row.names = F , col.names = F ) With data.table # identify SNPs that need recoding complement com.recode - info [ sapply ( B.A1 , complement ) == A2 sapply ( B.A2 , complement ) == A1 , SNP ] # Now update the bim file bim [ SNP %in% com.recode , c ( B.A1 , B.A2 ) := list ( sapply ( B.A2 , complement ), sapply ( B.A1 , complement ))] # And update the info structure info [ SNP %in% com.recode , c ( B.A1 , B.A2 ) := list ( sapply ( B.A2 , complement ), sapply ( B.A1 , complement ))] # Write the updated bim file fwrite ( bim , EUR.QC.adj.bim , col.names = F , sep = \\t ) 4. Identify SNPs that have different allele in base and target (usually due to difference in genome build or Indel) Without data.table mismatch - bim $ SNP [ ! ( bim $ SNP %in% info.match $ SNP | bim $ SNP %in% info.complement $ SNP | bim $ SNP %in% info.recode $ SNP | bim $ SNP %in% info.crecode $ SNP )] write.table ( mismatch , EUR.mismatch , quote = F , row.names = F , col.names = F ) q () # exit R With data.table matched - info [( A1 == B.A1 A2 == B.A2 ) | ( A1 == B.A2 A2 == B.A1 )] mismatch - bim [ ! SNP %in% matched $ SNP , SNP ] write.table ( mismatch , EUR.mismatch , quote = F , row.names = F , col.names = F ) q () # exit R 5. Replace EUR.bim with EUR.QC.adj.bim : # Make a back up mv EUR.bim EUR.bim.bk ln -s EUR.QC.adj.bim EUR.bim Note Most PRS software will perform flipping automatically, thus this step is usually not required. # Duplicate SNPs Make sure to remove any duplicate SNPs in your target data (these target data were simulated and so include no duplicated SNPs) # Sample overlap Since the target data were simulated there are no overlapping samples between the base and target data here (see the relevant section of the paper for discussion of the importance of avoiding sample overlap). # Relatedness Closely related individuals in the target data may lead to overfitted results, limiting the generalisability of the results. Before calculating the relatedness, pruning should be performed (see here ). Individuals that have a first or second degree relative in the sample ( \\(\\hat{\\pi} 0.125\\) ) can be removed with the following command: plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.QC.valid \\ --rel-cutoff 0 .125 \\ --out EUR.QC How many related samples were excluded? 2 samples were excluded Note A greedy algorithm is used to remove closely related individuals in a way that optimises the size of the sample retained. However, the algorithm is dependent on the random seed used, which can generate different results. Therefore, to reproduce the same result, you will need to specify the same random seed. PLINK's algorithm for removing related individuals does not account for the phenotype under study. To minimize the removal of cases of a disease, the following algorithm can be used instead: GreedyRelated . Generate final QC'ed target data file After performing the full analysis, you can generate a QC'ed data set with the following command: plink \\ --bfile EUR \\ --make-bed \\ --keep EUR.QC.rel.id \\ --out EUR.QC \\ --extract EUR.QC.snplist","title":"2. QC of Target Data"},{"location":"target/#obtaining-the-target-data","text":"Target data consist of individual-level genotype-phenotype data, usually generated within your lab/department/collaboration. For this tutorial, we have simulated some genotype-phenotype data using the 1000 Genomes Project European samples. You can download the data here or you can download the data using the following command: curl https://github.com/choishingwan/PRS-Tutorial/raw/master/resources/EUR.zip -L -O Unzip the data as follow: unzip EUR.zip Note Install the program PLINK and include its location in your PATH directory, which allows us to use plink instead of ./plink in the commands below. If PLINK is not in your PATH directory and is instead in your working directory, then replace all instances of plink in the tutorial with ./plink .","title":"Obtaining the target data"},{"location":"target/#qc-checklist-target-data","text":"Below are the QC steps that comprise the QC checklist for the target data.","title":"QC checklist: Target data"},{"location":"target/#35-sample-size","text":"We recommend that users only perform PRS analyses on target data of at least 100 individuals. The sample size of our target data here is 503 individuals.","title":"# Sample size"},{"location":"target/#35-file-transfer","text":"Usually we do not need to download and transfer the target data file because it is typically generated locally. However, the file should contain an md5sum code in case we send the data file to collaborators who may want to confirm that the file has not changed during the transfer. What is the md5sum code for each of the target files? File md5sum EUR.bed 940f5a760b41270662eba6264b262a2d EUR.bim a528020cc2448aa04a7499f13bf9f16a EUR.covariate afff13f8f9e15815f2237a62b8bec00b EUR.fam 17e8184fb03c690db6980bb7499d4982 EUR.height 052beb4cae32ac7673f1d6b9e854c85b","title":"# File transfer"},{"location":"target/#35-genome-build","text":"As stated in the base data section, the genome build for our base and target data is the same, as it should be.","title":"# Genome build"},{"location":"target/#35-standard-gwas-qc","text":"The target data must be quality controlled to at least the standards implemented in GWAS studies, e.g. removing SNPs with low genotyping rate, low minor allele frequency, out of Hardy-Weinberg Equilibrium, removing individuals with low genotyping rate (see Marees et al ). The following plink command applies some of these QC metrics to the target data: plink \\ --bfile EUR \\ --maf 0 .05 \\ --hwe 1e-6 \\ --geno 0 .01 \\ --mind 0 .01 \\ --write-snplist \\ --make-just-fam \\ --out EUR.QC Each of the parameters corresponds to the following Paramter Value Description bfile EUR Informs plink that the input genotype files should have a prefix of EUR maf 0.05 Removes all SNPs with minor allele frequency less than 0.05. Genotyping errors typically have a larger influence on SNPs with low MAF. Studies with large sample sizes could apply a lower MAF threshold hwe 1e-6 Removes SNPs with low P-value from the Hardy-Weinberg Equilibrium Fisher's exact or chi-squared test. SNPs with significant P-values from the HWE test are more likely affected by genotyping error or the effects of natural selection. Filtering should be performed on the control samples to avoid filtering SNPs that are causal (under selection in cases) geno 0.01 Excludes SNPs that are missing in a high fraction of subjects. A two-stage filtering process is usually performed (see Marees et al ). mind 0.01 Excludes individuals who have a high rate of genotype missingness, since this may indicate problems in the DNA sample or processing. (see Marees et al for more details). make-just-fam - Informs plink to only generate the QC'ed sample name to avoid generating the .bed file. write-snplist - Informs plink to only generate the QC'ed SNP list to avoid generating the .bed file. out EUR Informs plink that all output should have a prefix of EUR How many SNPs and samples were filtered? 5 samples were removed due to a high rate of genotype missingness 1 SNP were removed due to missing genotype data 872 SNPs were removed due to being out of Hardy-Weinberg Equilibrium 242,459 SNPs were removed due to low minor allele frequency Note Normally, we can generate a new genotype file using the new sample list. However, this will use up a lot of storage space. Using plink 's --extract , --exclude , --keep , --remove , --make-just-fam and --write-snplist functions, we can work solely on the list of samples and SNPs without duplicating the genotype file, reducing the storage space usage. Very high or low heterozygosity rates in individuals could be due to DNA contamination or to high levels of inbreeding. Therefore, samples with extreme heterozygosity are typically removed prior to downstream analyses. First, we perform prunning to remove highly correlated SNPs: plink \\ --bfile EUR \\ --keep EUR.QC.fam \\ --extract EUR.QC.snplist \\ --indep-pairwise 200 50 0 .25 \\ --out EUR.QC This will generate two files 1) EUR.QC.prune.in and 2) EUR.QC.prune.out .All SNPs within EUR.QC.prune.in have a pairwise \\(r^2 0.25\\) . Heterozygosity rates can then be computed using plink : plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.QC.fam \\ --het \\ --out EUR.QC This will generate the EUR.QC.het file, which contains F coefficient estimates for assessing heterozygosity. We will remove individuals with F coefficients that are more than 3 standard deviation (SD) units from the mean, which can be performed using the following R command (assuming that you have R downloaded, then you can open an R session by typing R in your terminal): Without library dat - read.table ( EUR.QC.het , header = T ) # Read in the EUR.het file, specify it has header m - mean ( dat $ F ) # Calculate the mean s - sd ( dat $ F ) # Calculate the SD valid - subset ( dat , F = m +3 * s F = m -3 * s ) # Get any samples with F coefficient within 3 SD of the population mean write.table ( valid [, c ( 1 , 2 )], EUR.valid.sample , quote = F , row.names = F ) # print FID and IID for valid samples q () # exit R With data.table library ( data.table ) # Read in file dat - fread ( EUR.QC.het ) # Get samples with F coefficient within 3 SD of the population mean valid - dat [ F = mean ( F ) +3 * sd ( F ) F = mean ( F ) -3 * sd ( F )] # print FID and IID for valid samples fwrite ( valid [, c ( FID , IID )], EUR.valid.sample , sep = \\t ) q () # exit R How many samples were excluded due to high heterozygosity rate? 7 samples were excluded","title":"# Standard GWAS QC"},{"location":"target/#35-ambiguous-snps","text":"There were removed during the base data QC","title":"# Ambiguous SNPs"},{"location":"target/#35-sex-chromosomes","text":"Sometimes sample mislabelling can occur, which may lead to invalid results. A good indication of a mislabelled sample is a mismatch between biological sex and reported sex. If the biological sex does not match up with the reported sex, then the sample may have been mislabelled. Before performing a sex check, pruning should be performed (see here ). A sex check can then easily be conducted using plink plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.valid.sample \\ --check-sex \\ --out EUR.QC This will generate a file called EUR.QC.sexcheck containing the F-statistics for each individual. Individuals are typically called as being biologically male if the F-statistic is 0.8 and biologically female if F 0.2. Without library # Read in file valid - read.table ( EUR.valid.sample , header = T ) dat - read.table ( EUR.QC.sexcheck , header = T ) valid - subset ( dat , STATUS == OK FID %in% valid $ FID ) write.table ( valid [, c ( FID , IID )], EUR.QC.valid , row.names = F , col.names = F , sep = \\t , quote = F ) q () # exit R With data.table library ( data.table ) # Read in file valid - fread ( EUR.valid.sample ) dat - fread ( EUR.QC.sexcheck )[ FID %in% valid $ FID ] fwrite ( dat [ STATUS == OK , c ( FID , IID )], EUR.QC.valid , sep = \\t ) q () # exit R How many samples were excluded due mismatched Sex information? 2 samples were excluded","title":"# Sex chromosomes"},{"location":"target/#35-mismatching-genotypes","text":"In addition, when there are non-ambiguous mismatches in allele coding between the data sets, such as A/C in the base and G/T in the target data, then this can be resolved by \u2018flipping\u2019 the alleles in the target data to their complementary alleles. This can be achieved with the following steps: 1. Load the bim file, the GIANT summary statistic and the QC SNP list into R Without data.table # Read in bim file bim - read.table ( EUR.bim ) colnames ( bim ) - c ( CHR , SNP , CM , BP , B.A1 , B.A2 ) # Read in QCed SNPs qc - read.table ( EUR.QC.snplist , header = F , stringsAsFactors = F ) # Read in GIANT data height - read.table ( gzfile ( Height.QC.gz ), header = T , stringsAsFactors = F , sep = \\t ) # Change all alleles to upper case for easy comparison height $ A1 - toupper ( height $ A1 ) height $ A2 - toupper ( height $ A2 ) bim $ B.A1 - toupper ( bim $ B.A1 ) bim $ B.A2 - toupper ( bim $ B.A2 ) With data.table library ( data.table ) # Read in bim file bim - fread ( EUR.bim ) setnames ( bim , colnames ( bim ), c ( CHR , SNP , CM , BP , B.A1 , B.A2 )) # Read in GIANT data (require data.table v1.12.0+) height - fread ( Height.QC.gz ) # Change all alleles to upper case for easy comparison height [, c ( A1 , A2 ) := list ( toupper ( A1 ), toupper ( A2 ))] bim [, c ( B.A1 , B.A2 ) := list ( toupper ( B.A1 ), toupper ( B.A2 ))] # Read in QCed SNPs qc - fread ( EUR.QC.snplist , header = F ) 2. Identify SNPs that require strand flipping Without data.table # Merge GIANT with target info - merge ( bim , height , by = c ( SNP , CHR , BP )) # Filter QCed SNPs info - info [ info $ SNP %in% qc $ V1 ,] # Function for finding the complementary allele complement - function ( x ) { switch ( x , A = T , C = G , T = A , G = C , return ( NA ) ) } # Get SNPs that have the same alleles across base and target info.match - subset ( info , A1 == B.A1 A2 == B.A2 ) # Identify SNPs that are complementary between base and target info $ C.A1 - sapply ( info $ B.A1 , complement ) info $ C.A2 - sapply ( info $ B.A2 , complement ) info.complement - subset ( info , A1 == C.A1 A2 == C.A2 ) # Update these allele coding in the bim file bim [ bim $ SNP %in% info.complement $ SNP ,] $ B.A1 - sapply ( bim [ bim $ SNP %in% info.complement $ SNP ,] $ B.A1 , complement ) bim [ bim $ SNP %in% info.complement $ SNP ,] $ B.A2 - sapply ( bim [ bim $ SNP %in% info.complement $ SNP ,] $ B.A2 , complement ) With data.table # Merge GIANT with target info - merge ( bim , height , by = c ( SNP , CHR , BP )) info - info [ SNP %in% qc $ V1 ] # Function for calculating the complementary allele complement - function ( x ){ switch ( x , A = T , C = G , T = A , G = C , return ( NA ) ) } # Identify SNPs that are complementary between base and target com.snps - info [ sapply ( B.A1 , complement ) == A1 sapply ( B.A2 , complement ) == A2 , SNP ] # Now update the bim file bim [ SNP %in% com.snps , c ( B.A1 , B.A2 ) := list ( sapply ( B.A1 , complement ), sapply ( B.A2 , complement ))] # And update the info structure info [ SNP %in% com.snps , c ( B.A1 , B.A2 ) := list ( sapply ( B.A1 , complement ), sapply ( B.A2 , complement ))] 3. Identify SNPs that require recoding in the target (to ensure the coding allele in the target data is the effective allele in the base summary statistic) Without data.table # identify SNPs that need recoding info.recode - subset ( info , A1 == B.A2 A2 == B.A1 ) # identify SNPs that need recoding complement info.crecode - subset ( info , A1 == C.A2 A2 == C.A1 ) # Update these allele coding in the bim file com.snps - bim $ SNP %in% info.crecode $ SNP tmp - bim [ com.snps ,] $ B.A1 bim [ com.snps ,] $ B.A1 - as.character ( sapply ( bim [ com.snps ,] $ B.A2 , complement )) bim [ com.snps ,] $ B.A2 - as.character ( sapply ( tmp , complement )) # Output updated bim file write.table ( bim , EUR.QC.adj.bim , quote = F , row.names = F , col.names = F ) With data.table # identify SNPs that need recoding complement com.recode - info [ sapply ( B.A1 , complement ) == A2 sapply ( B.A2 , complement ) == A1 , SNP ] # Now update the bim file bim [ SNP %in% com.recode , c ( B.A1 , B.A2 ) := list ( sapply ( B.A2 , complement ), sapply ( B.A1 , complement ))] # And update the info structure info [ SNP %in% com.recode , c ( B.A1 , B.A2 ) := list ( sapply ( B.A2 , complement ), sapply ( B.A1 , complement ))] # Write the updated bim file fwrite ( bim , EUR.QC.adj.bim , col.names = F , sep = \\t ) 4. Identify SNPs that have different allele in base and target (usually due to difference in genome build or Indel) Without data.table mismatch - bim $ SNP [ ! ( bim $ SNP %in% info.match $ SNP | bim $ SNP %in% info.complement $ SNP | bim $ SNP %in% info.recode $ SNP | bim $ SNP %in% info.crecode $ SNP )] write.table ( mismatch , EUR.mismatch , quote = F , row.names = F , col.names = F ) q () # exit R With data.table matched - info [( A1 == B.A1 A2 == B.A2 ) | ( A1 == B.A2 A2 == B.A1 )] mismatch - bim [ ! SNP %in% matched $ SNP , SNP ] write.table ( mismatch , EUR.mismatch , quote = F , row.names = F , col.names = F ) q () # exit R 5. Replace EUR.bim with EUR.QC.adj.bim : # Make a back up mv EUR.bim EUR.bim.bk ln -s EUR.QC.adj.bim EUR.bim Note Most PRS software will perform flipping automatically, thus this step is usually not required.","title":"# Mismatching genotypes"},{"location":"target/#35-duplicate-snps","text":"Make sure to remove any duplicate SNPs in your target data (these target data were simulated and so include no duplicated SNPs)","title":"# Duplicate SNPs"},{"location":"target/#35-sample-overlap","text":"Since the target data were simulated there are no overlapping samples between the base and target data here (see the relevant section of the paper for discussion of the importance of avoiding sample overlap).","title":"# Sample overlap"},{"location":"target/#35-relatedness","text":"Closely related individuals in the target data may lead to overfitted results, limiting the generalisability of the results. Before calculating the relatedness, pruning should be performed (see here ). Individuals that have a first or second degree relative in the sample ( \\(\\hat{\\pi} 0.125\\) ) can be removed with the following command: plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.QC.valid \\ --rel-cutoff 0 .125 \\ --out EUR.QC How many related samples were excluded? 2 samples were excluded Note A greedy algorithm is used to remove closely related individuals in a way that optimises the size of the sample retained. However, the algorithm is dependent on the random seed used, which can generate different results. Therefore, to reproduce the same result, you will need to specify the same random seed. PLINK's algorithm for removing related individuals does not account for the phenotype under study. To minimize the removal of cases of a disease, the following algorithm can be used instead: GreedyRelated .","title":"# Relatedness"},{"location":"target/#generate-final-qced-target-data-file","text":"After performing the full analysis, you can generate a QC'ed data set with the following command: plink \\ --bfile EUR \\ --make-bed \\ --keep EUR.QC.rel.id \\ --out EUR.QC \\ --extract EUR.QC.snplist","title":"Generate final QC'ed target data file"}]}