{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 Overview \u00b6 This tutorial provides a step-by-step guide to performing basic polygenic risk score (PRS) analyses and accompanies our PRS Guide paper . The aim of this tutorial is to provide a simple introduction of PRS analyses to those new to PRS, while equipping existing users with a better understanding of the processes and implementation \"underneath the hood\" of popular PRS software. The tutorial is separated into four main sections and reflects the structure of our guide paper : the first two sections on QC correspond to Section 2 of the paper and constitute a 'QC checklist' for PRS analyses, the third section on calculating PRS (here with examples using PLINK , PRSice-2 , LDpred-2 and lassosum ) corresponds to Section 3 of the paper, while the fourth section, which provides some examples of visualising PRS results, accompanies Section 4 of the paper. Quality Control (QC) of Base Data Quality Control (QC) of Target Data Calculating and analysing PRS Visualising PRS Results We will be referring to our guide paper in each section and so you may find it helpful to have the paper open as you go through the tutorial. Warning Data used in this tutorial are simulated and intended for demonstration purposes only. The results from this tutorial will not reflect the true performance of different software. Note We assume you have basic knownledges on how to use the terminal, plink and R . If you are unfamiliar with any of those, you can refer to the following online resources: Software link terminal (OS X / Linux) 1 , 2 terminal (Windows) 1 , 2 plink v1.90 , v1.07 R 1 Note This tutorial is written for Linux and OS X operating systems. Windows users will need to change some commands accordingly. Note Throughout the tutorial you will see tabs above some of the code: A echo \"Tab A\" B echo \"Tab B\" You can click on the tab to change to an alternative code (eg. to a different operation system) Datasets \u00b6 Base data Target data : Simulated data based on the 1000 Genomes Project European samples Requirements \u00b6 To follow the tutorial, you will need the following programs installed: R ( version 3.2.3+ ) PLINK 1.9 Citation \u00b6 If you find this tutorial helpful for a publication, then please consider citing: Citation Choi, S.W., Mak, T.S. & O\u2019Reilly, P.F. Tutorial: a guide to performing polygenic risk score analyses. Nat Protoc (2020). https://doi.org/10.1038/s41596-020-0353-1","title":"Home"},{"location":"#home","text":"","title":"Home"},{"location":"#overview","text":"This tutorial provides a step-by-step guide to performing basic polygenic risk score (PRS) analyses and accompanies our PRS Guide paper . The aim of this tutorial is to provide a simple introduction of PRS analyses to those new to PRS, while equipping existing users with a better understanding of the processes and implementation \"underneath the hood\" of popular PRS software. The tutorial is separated into four main sections and reflects the structure of our guide paper : the first two sections on QC correspond to Section 2 of the paper and constitute a 'QC checklist' for PRS analyses, the third section on calculating PRS (here with examples using PLINK , PRSice-2 , LDpred-2 and lassosum ) corresponds to Section 3 of the paper, while the fourth section, which provides some examples of visualising PRS results, accompanies Section 4 of the paper. Quality Control (QC) of Base Data Quality Control (QC) of Target Data Calculating and analysing PRS Visualising PRS Results We will be referring to our guide paper in each section and so you may find it helpful to have the paper open as you go through the tutorial. Warning Data used in this tutorial are simulated and intended for demonstration purposes only. The results from this tutorial will not reflect the true performance of different software. Note We assume you have basic knownledges on how to use the terminal, plink and R . If you are unfamiliar with any of those, you can refer to the following online resources: Software link terminal (OS X / Linux) 1 , 2 terminal (Windows) 1 , 2 plink v1.90 , v1.07 R 1 Note This tutorial is written for Linux and OS X operating systems. Windows users will need to change some commands accordingly. Note Throughout the tutorial you will see tabs above some of the code: A echo \"Tab A\" B echo \"Tab B\" You can click on the tab to change to an alternative code (eg. to a different operation system)","title":"Overview"},{"location":"#datasets","text":"Base data Target data : Simulated data based on the 1000 Genomes Project European samples","title":"Datasets"},{"location":"#requirements","text":"To follow the tutorial, you will need the following programs installed: R ( version 3.2.3+ ) PLINK 1.9","title":"Requirements"},{"location":"#citation","text":"If you find this tutorial helpful for a publication, then please consider citing: Citation Choi, S.W., Mak, T.S. & O\u2019Reilly, P.F. Tutorial: a guide to performing polygenic risk score analyses. Nat Protoc (2020). https://doi.org/10.1038/s41596-020-0353-1","title":"Citation"},{"location":"base/","text":"QC of Base Data \u00b6 Obtaining the base data file \u00b6 The first step in Polygenic Risk Score (PRS) analyses is to generate or obtain the base data (GWAS summary statistics). Ideally these will correspond to the most powerful GWAS results available on the phenotype under study. In this example, we will use GWAS on simulated height. You can download the summary statistic file here Note Due to limitation to bandwidth, we are currently using google drive to host the files, which doesn't allow the use of wget or curl to download the file. Please download the files manually. Warning If you download the summary statistics on a MAC machine, the gz file may decompress automatically, which might result in a corrupted Height.gwas.txt file (including only the header). If this happens then you'll need to disable automatic decompression when downloading the file (eg. follow instructions here ) Reading the base data file \u00b6 Height.gwas.txt.gz is compressed. To read its content, you can type: gunzip -c Height.gwas.txt.gz | head which will display the first 10 lines of the file Note Working with compressed files reduces storage space requirements The Height.gwas.txt.gz file contains the following columns: CHR BP SNP A1 A2 N SE P OR INFO MAF 1 756604 rs3131962 A G 388028 0.00301666 0.483171 0.997886915712657 0.890557941364774 0.369389592764921 1 768448 rs12562034 A G 388028 0.00329472 0.834808 1.00068731609353 0.895893511351165 0.336845754096289 1 779322 rs4040617 G A 388028 0.00303344 0.42897 0.997603556067569 0.897508290615237 0.377368010940814 The column headers correspond to the following: CHR : The chromosome in which the SNP resides BP : Chromosomal co-ordinate of the SNP SNP : SNP ID, usually in the form of rs-ID A1 : The effect allele of the SNP A2 : The non-effect allele of the SNP N : Number of samples used to obtain the effect size estimate SE : The standard error (SE) of the effect size esimate P : The P-value of association between the SNP genotypes and the base phenotype OR : The effect size estimate of the SNP, if the outcome is binary/case-control. If the outcome is continuous or treated as continuous then this will usually be BETA INFO : The imputation information score MAF : The minor allele frequency (MAF) of the SNP QC checklist: Base data \u00b6 Below we perform QC on these base data according to the 'QC checklist' in our guide paper , which we recommend that users follow while going through this tutorial and when performing PRS analyses: # Heritability check \u00b6 We recommend that PRS analyses are performed on base data with a chip-heritability estimate \\(h_{snp}^{2} > 0.05\\) . The chip-heritability of a GWAS can be estimated using e.g. LD Score Regression (LDSC). Our height GWAS data are simulated to have a chip-heritability much greater than 0.05 and so we can move on to the next QC step. # Effect allele \u00b6 It is important to know which allele is the effect allele and which is the non-effect allele for PRS association results to be in the correct direction. Important Some GWAS results files do not make clear which allele is the effect allele and which is the non-effect allele. If the incorrect assumption is made in computing the PRS, then the effect of the PRS in the target data will be in the wrong direction. To avoid misleading conclusions the effect allele from the base (GWAS) data must be known. # File transfer \u00b6 A common problem is that the downloaded base data file can be corrupted during download, which can cause PRS software to crash or to produce errors in results. However, a md5sum hash is generally included in files so that file integrity can be checked. The following command performs this md5sum check: Linux md5sum Height.gwas.txt.gz OS X md5 Height.gwas.txt.gz if the file is intact, then md5sum generates a string of characters, which in this case should be: a2b15fb6a2bbbe7ef49f67959b43b160 . If a different string is generated, then the file is corrupted. # Genome build \u00b6 The height summary statistic are on the same genome build as the target data that we will be using. You must check that your base and target data are on the same genome build, and if they are not then use a tool such as LiftOver to make the builds consistent across the data sets. # Standard GWAS QC \u00b6 As described in the paper, both the base and target data should be subjected to the standard stringent QC steps performed in GWAS. If the base data have been obtained as summary statistics from a public source, then the typical QC steps that you will be able to perform on them are to filter the SNPs according to INFO score and MAF. SNPs with low minor allele frequency (MAF) or imputation information score (INFO) are more likely to generate false positive results due to their lower statistical power (and higher probability of genotyping errors in the case of low MAF). Therefore, SNPs with low MAF and INFO are typically removed before performing downstream analyses. We recommend removing SNPs with MAF < 1% and INFO < 0.8 (with very large base sample sizes these thresholds could be reduced if sensitivity checks indicate reliable results). These SNP filters can be achieved using the following code: Using bash gunzip -c Height.gwas.txt.gz | \\ awk 'NR==1 || ($11 > 0.01) && ($10 > 0.8) {print}' | \\ gzip > Height.gz Using R with data.table # Alternatively, you can use R, with data.table v1.11.8+ library (data.table) # Read in file dat <- fread ( \"Height.gwas.txt.gz\" ) # Filter out SNPs result <- dat[INFO > 0.8 & MAF > 0.01 ] # Output the gz file fwrite (result, \"Height.gz\" , sep = \"\\t\" ) The bash code above does the following: Decompresses and reads the Height.gwas.txt.gz file Prints the header line ( NR==1 ) Prints any line with MAF above 0.01 ( $11 because the eleventh column of the file contains the MAF information) Prints any line with INFO above 0.8 ( $10 because the tenth column of the file contains the INFO information) Compresses and writes the results to Height.gz # Mismatching SNPs \u00b6 SNPs that have mismatching alleles reported in the base and target data are either resolvable by \"strand-flipping\" the alleles to their complementary alleles in e.g. the target data, such as for a SNP with A/C in the base data and G/T in the target, or non-resolvable, such as for a SNP with C/G in the base and C/T in the target. Most polygenic score software perform strand-flipping automatically for SNPs that are resolvable, and remove non-resolvable mismatching SNPs. Since we need the target data to know which SNPs have mismatching alleles, we will perform this strand-flipping in the target data. # Duplicate SNPs \u00b6 If an error has occurred in the generation of the base data then there may be duplicated SNPs in the base data file. Most PRS software do not allow duplicated SNPs in the base data input and thus they should be removed, using a command such as the one below: gunzip -c Height.gz | \\ awk '{seen[$3]++; if(seen[$3]==1){ print}}' | \\ gzip - > Height.nodup.gz The above command does the following: Decompresses and reads the Height.gz file Count number of time SNP ID was observed, assuming the third column contian the SNP ID ( seen[$3]++ ). If this it the first time seeing this SNP ID, print it. 3. Compresses and writes the results to Height.nodup.gz How many duplicated SNPs are there? There are a total of 2 duplicated SNPs # Ambiguous SNPs \u00b6 If the base and target data were generated using different genotyping chips and the chromosome strand (+/-) that was used for either is unknown, then it is not possible to pair-up the alleles of ambiguous SNPs (i.e. those with complementary alleles, either C/G or A/T SNPs) across the data sets, because it will be unknown whether the base and target data are referring to the same allele or not. While allele frequencies could be used to infer which alleles are on the same strand, the accuracy of this could be low for SNPs with MAF close to 50% or when the base and target data are from different populations. Therefore, we recommend removing all ambiguous SNPs to avoid introducing this potential source of systematic error. Ambiguous SNPs can be removed in the base data and then there will be no such SNPs in the subsequent analyses, since analyses are performed only on SNPs that overlap between the base and target data. Nonambiguous SNPs can be retained using the following: gunzip -c Height.nodup.gz | \\ awk '!( ($4==\"A\" && $5==\"T\") || \\ ($4==\"T\" && $5==\"A\") || \\ ($4==\"G\" && $5==\"C\") || \\ ($4==\"C\" && $5==\"G\")) {print}' | \\ gzip > Height.QC.gz How many non-ambiguous SNPs were there? There are 499,617 non-ambiguous SNPs # Sex chromosomes \u00b6 Sometimes sample mislabelling can occur, which may lead to invalid results. One indication of a mislabelled sample is a difference between reported sex and that indicated by the sex chromosomes. While this may be due to a difference in sex and gender identity, it could also reflect mislabeling of samples or misreporting and, thus, individuals in which there is a mismatch between biological and reported sex are typically removed. See the Target Data section in which a sex-check is performed. # Sample overlap \u00b6 Since the target data were simulated there are no overlapping samples between the base and target data here (see the relevant section of the paper for discussion of the importance of avoiding sample overlap). # Relatedness \u00b6 Closely related individuals within and between the base and the target data may lead to overfitted results, limiting the generalizability of the results (see the relevant sections of the paper ). Relatedness within the target data is tested in the Target Data section. The Height.QC.gz base data are now ready for using in downstream analyses.","title":"1. QC of Base Data"},{"location":"base/#qc-of-base-data","text":"","title":"QC of Base Data"},{"location":"base/#obtaining-the-base-data-file","text":"The first step in Polygenic Risk Score (PRS) analyses is to generate or obtain the base data (GWAS summary statistics). Ideally these will correspond to the most powerful GWAS results available on the phenotype under study. In this example, we will use GWAS on simulated height. You can download the summary statistic file here Note Due to limitation to bandwidth, we are currently using google drive to host the files, which doesn't allow the use of wget or curl to download the file. Please download the files manually. Warning If you download the summary statistics on a MAC machine, the gz file may decompress automatically, which might result in a corrupted Height.gwas.txt file (including only the header). If this happens then you'll need to disable automatic decompression when downloading the file (eg. follow instructions here )","title":"Obtaining the base data file"},{"location":"base/#reading-the-base-data-file","text":"Height.gwas.txt.gz is compressed. To read its content, you can type: gunzip -c Height.gwas.txt.gz | head which will display the first 10 lines of the file Note Working with compressed files reduces storage space requirements The Height.gwas.txt.gz file contains the following columns: CHR BP SNP A1 A2 N SE P OR INFO MAF 1 756604 rs3131962 A G 388028 0.00301666 0.483171 0.997886915712657 0.890557941364774 0.369389592764921 1 768448 rs12562034 A G 388028 0.00329472 0.834808 1.00068731609353 0.895893511351165 0.336845754096289 1 779322 rs4040617 G A 388028 0.00303344 0.42897 0.997603556067569 0.897508290615237 0.377368010940814 The column headers correspond to the following: CHR : The chromosome in which the SNP resides BP : Chromosomal co-ordinate of the SNP SNP : SNP ID, usually in the form of rs-ID A1 : The effect allele of the SNP A2 : The non-effect allele of the SNP N : Number of samples used to obtain the effect size estimate SE : The standard error (SE) of the effect size esimate P : The P-value of association between the SNP genotypes and the base phenotype OR : The effect size estimate of the SNP, if the outcome is binary/case-control. If the outcome is continuous or treated as continuous then this will usually be BETA INFO : The imputation information score MAF : The minor allele frequency (MAF) of the SNP","title":"Reading the base data file"},{"location":"base/#qc-checklist-base-data","text":"Below we perform QC on these base data according to the 'QC checklist' in our guide paper , which we recommend that users follow while going through this tutorial and when performing PRS analyses:","title":"QC checklist: Base data"},{"location":"base/#heritability-check","text":"We recommend that PRS analyses are performed on base data with a chip-heritability estimate \\(h_{snp}^{2} > 0.05\\) . The chip-heritability of a GWAS can be estimated using e.g. LD Score Regression (LDSC). Our height GWAS data are simulated to have a chip-heritability much greater than 0.05 and so we can move on to the next QC step.","title":"# Heritability check"},{"location":"base/#effect-allele","text":"It is important to know which allele is the effect allele and which is the non-effect allele for PRS association results to be in the correct direction. Important Some GWAS results files do not make clear which allele is the effect allele and which is the non-effect allele. If the incorrect assumption is made in computing the PRS, then the effect of the PRS in the target data will be in the wrong direction. To avoid misleading conclusions the effect allele from the base (GWAS) data must be known.","title":"# Effect allele"},{"location":"base/#file-transfer","text":"A common problem is that the downloaded base data file can be corrupted during download, which can cause PRS software to crash or to produce errors in results. However, a md5sum hash is generally included in files so that file integrity can be checked. The following command performs this md5sum check: Linux md5sum Height.gwas.txt.gz OS X md5 Height.gwas.txt.gz if the file is intact, then md5sum generates a string of characters, which in this case should be: a2b15fb6a2bbbe7ef49f67959b43b160 . If a different string is generated, then the file is corrupted.","title":"# File transfer"},{"location":"base/#genome-build","text":"The height summary statistic are on the same genome build as the target data that we will be using. You must check that your base and target data are on the same genome build, and if they are not then use a tool such as LiftOver to make the builds consistent across the data sets.","title":"# Genome build"},{"location":"base/#standard-gwas-qc","text":"As described in the paper, both the base and target data should be subjected to the standard stringent QC steps performed in GWAS. If the base data have been obtained as summary statistics from a public source, then the typical QC steps that you will be able to perform on them are to filter the SNPs according to INFO score and MAF. SNPs with low minor allele frequency (MAF) or imputation information score (INFO) are more likely to generate false positive results due to their lower statistical power (and higher probability of genotyping errors in the case of low MAF). Therefore, SNPs with low MAF and INFO are typically removed before performing downstream analyses. We recommend removing SNPs with MAF < 1% and INFO < 0.8 (with very large base sample sizes these thresholds could be reduced if sensitivity checks indicate reliable results). These SNP filters can be achieved using the following code: Using bash gunzip -c Height.gwas.txt.gz | \\ awk 'NR==1 || ($11 > 0.01) && ($10 > 0.8) {print}' | \\ gzip > Height.gz Using R with data.table # Alternatively, you can use R, with data.table v1.11.8+ library (data.table) # Read in file dat <- fread ( \"Height.gwas.txt.gz\" ) # Filter out SNPs result <- dat[INFO > 0.8 & MAF > 0.01 ] # Output the gz file fwrite (result, \"Height.gz\" , sep = \"\\t\" ) The bash code above does the following: Decompresses and reads the Height.gwas.txt.gz file Prints the header line ( NR==1 ) Prints any line with MAF above 0.01 ( $11 because the eleventh column of the file contains the MAF information) Prints any line with INFO above 0.8 ( $10 because the tenth column of the file contains the INFO information) Compresses and writes the results to Height.gz","title":"# Standard GWAS QC"},{"location":"base/#mismatching-snps","text":"SNPs that have mismatching alleles reported in the base and target data are either resolvable by \"strand-flipping\" the alleles to their complementary alleles in e.g. the target data, such as for a SNP with A/C in the base data and G/T in the target, or non-resolvable, such as for a SNP with C/G in the base and C/T in the target. Most polygenic score software perform strand-flipping automatically for SNPs that are resolvable, and remove non-resolvable mismatching SNPs. Since we need the target data to know which SNPs have mismatching alleles, we will perform this strand-flipping in the target data.","title":"# Mismatching SNPs"},{"location":"base/#duplicate-snps","text":"If an error has occurred in the generation of the base data then there may be duplicated SNPs in the base data file. Most PRS software do not allow duplicated SNPs in the base data input and thus they should be removed, using a command such as the one below: gunzip -c Height.gz | \\ awk '{seen[$3]++; if(seen[$3]==1){ print}}' | \\ gzip - > Height.nodup.gz The above command does the following: Decompresses and reads the Height.gz file Count number of time SNP ID was observed, assuming the third column contian the SNP ID ( seen[$3]++ ). If this it the first time seeing this SNP ID, print it. 3. Compresses and writes the results to Height.nodup.gz How many duplicated SNPs are there? There are a total of 2 duplicated SNPs","title":"# Duplicate SNPs"},{"location":"base/#ambiguous-snps","text":"If the base and target data were generated using different genotyping chips and the chromosome strand (+/-) that was used for either is unknown, then it is not possible to pair-up the alleles of ambiguous SNPs (i.e. those with complementary alleles, either C/G or A/T SNPs) across the data sets, because it will be unknown whether the base and target data are referring to the same allele or not. While allele frequencies could be used to infer which alleles are on the same strand, the accuracy of this could be low for SNPs with MAF close to 50% or when the base and target data are from different populations. Therefore, we recommend removing all ambiguous SNPs to avoid introducing this potential source of systematic error. Ambiguous SNPs can be removed in the base data and then there will be no such SNPs in the subsequent analyses, since analyses are performed only on SNPs that overlap between the base and target data. Nonambiguous SNPs can be retained using the following: gunzip -c Height.nodup.gz | \\ awk '!( ($4==\"A\" && $5==\"T\") || \\ ($4==\"T\" && $5==\"A\") || \\ ($4==\"G\" && $5==\"C\") || \\ ($4==\"C\" && $5==\"G\")) {print}' | \\ gzip > Height.QC.gz How many non-ambiguous SNPs were there? There are 499,617 non-ambiguous SNPs","title":"# Ambiguous SNPs"},{"location":"base/#sex-chromosomes","text":"Sometimes sample mislabelling can occur, which may lead to invalid results. One indication of a mislabelled sample is a difference between reported sex and that indicated by the sex chromosomes. While this may be due to a difference in sex and gender identity, it could also reflect mislabeling of samples or misreporting and, thus, individuals in which there is a mismatch between biological and reported sex are typically removed. See the Target Data section in which a sex-check is performed.","title":"# Sex chromosomes"},{"location":"base/#sample-overlap","text":"Since the target data were simulated there are no overlapping samples between the base and target data here (see the relevant section of the paper for discussion of the importance of avoiding sample overlap).","title":"# Sample overlap"},{"location":"base/#relatedness","text":"Closely related individuals within and between the base and the target data may lead to overfitted results, limiting the generalizability of the results (see the relevant sections of the paper ). Relatedness within the target data is tested in the Target Data section. The Height.QC.gz base data are now ready for using in downstream analyses.","title":"# Relatedness"},{"location":"cal_prs/","text":"Calculating and Analysing PRS \u00b6 Background \u00b6 In this section of the tutorial you will use four different software programs to compute PRS from the base and target data that you QC'ed in the previous two sections. The programs are PLINK PRSice-2 LDPred-2 lassosum","title":"3. Calculating and analysing PRS"},{"location":"cal_prs/#calculating-and-analysing-prs","text":"","title":"Calculating and Analysing PRS"},{"location":"cal_prs/#background","text":"In this section of the tutorial you will use four different software programs to compute PRS from the base and target data that you QC'ed in the previous two sections. The programs are PLINK PRSice-2 LDPred-2 lassosum","title":"Background"},{"location":"lassosum/","text":"Background \u00b6 lassosum is one of the dedicated PRS programs which is an R package that uses penalised regression (LASSO) in its approach to PRS calculation. Installing lassosum \u00b6 Note The script used here is based on lassosum version 0.4.4 Note For more details, please refer to lassosum's homepage You can install lassosum and its dependencies in R with the following command: install.packages ( c ( \"devtools\" , \"RcppArmadillo\" , \"data.table\" , \"Matrix\" ), dependencies = TRUE ) library (devtools) install_github ( \"tshmak/lassosum\" ) Required Data \u00b6 Again, we assume that we have the following files (or you can download it from here ): File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.cov This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples Running PRS analysis \u00b6 We can run lassosum as follows: library (lassosum) # Prefer to work with data.table as it speeds up file reading library (data.table) library (methods) library (magrittr) # For multi-threading, you can use the parallel package and # invoke cl which is then passed to lassosum.pipeline library (parallel) # This will invoke 2 threads. cl <- makeCluster ( 2 ) sum.stat <- \"Height.QC.gz\" bfile <- \"EUR.QC\" # Read in and process the covariates covariate <- fread ( \"EUR.cov\" ) pcs <- fread ( \"EUR.eigenvec\" ) %>% setnames (., colnames (.), c ( \"FID\" , \"IID\" , paste0 ( \"PC\" , 1:6 ))) # Need as.data.frame here as lassosum doesn't handle data.table # covariates very well cov <- merge (covariate, pcs) # We will need the EUR.hg19 file provided by lassosum # which are LD regions defined in Berisa and Pickrell (2015) for the European population and the hg19 genome. ld.file <- \"EUR.hg19\" # output prefix prefix <- \"EUR\" # Read in the target phenotype file target.pheno <- fread ( \"EUR.height\" )[, c ( \"FID\" , \"IID\" , \"Height\" )] # Read in the summary statistics ss <- fread (sum.stat) # Remove P-value = 0, which causes problem in the transformation ss <- ss[ ! P == 0 ] # Transform the P-values into correlation cor <- p2cor (p = ss $ P, n = ss $ N, sign = log (ss $ OR) ) fam <- fread ( paste0 (bfile, \".fam\" )) fam[,ID := do.call (paste, c (.SD, sep = \":\" )),.SDcols = c ( 1:2 )] # Run the lassosum pipeline # The cluster parameter is used for multi-threading # You can ignore that if you do not wish to perform multi-threaded processing out <- lassosum.pipeline ( cor = cor, chr = ss $ CHR, pos = ss $ BP, A1 = ss $ A1, A2 = ss $ A2, ref.bfile = bfile, test.bfile = bfile, LDblocks = ld.file, cluster = cl ) # Store the R2 results target.res <- validate (out, pheno = as.data.frame (target.pheno), covar = as.data.frame (cov)) # Get the maximum R2 r2 <- max (target.res $ validation.table $ value) ^2 How much phenotypic variation does the \"best-fit\" PRS explain? 0.2474679","title":"lassosum"},{"location":"lassosum/#background","text":"lassosum is one of the dedicated PRS programs which is an R package that uses penalised regression (LASSO) in its approach to PRS calculation.","title":"Background"},{"location":"lassosum/#installing-lassosum","text":"Note The script used here is based on lassosum version 0.4.4 Note For more details, please refer to lassosum's homepage You can install lassosum and its dependencies in R with the following command: install.packages ( c ( \"devtools\" , \"RcppArmadillo\" , \"data.table\" , \"Matrix\" ), dependencies = TRUE ) library (devtools) install_github ( \"tshmak/lassosum\" )","title":"Installing lassosum"},{"location":"lassosum/#required-data","text":"Again, we assume that we have the following files (or you can download it from here ): File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.cov This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples","title":"Required Data"},{"location":"lassosum/#running-prs-analysis","text":"We can run lassosum as follows: library (lassosum) # Prefer to work with data.table as it speeds up file reading library (data.table) library (methods) library (magrittr) # For multi-threading, you can use the parallel package and # invoke cl which is then passed to lassosum.pipeline library (parallel) # This will invoke 2 threads. cl <- makeCluster ( 2 ) sum.stat <- \"Height.QC.gz\" bfile <- \"EUR.QC\" # Read in and process the covariates covariate <- fread ( \"EUR.cov\" ) pcs <- fread ( \"EUR.eigenvec\" ) %>% setnames (., colnames (.), c ( \"FID\" , \"IID\" , paste0 ( \"PC\" , 1:6 ))) # Need as.data.frame here as lassosum doesn't handle data.table # covariates very well cov <- merge (covariate, pcs) # We will need the EUR.hg19 file provided by lassosum # which are LD regions defined in Berisa and Pickrell (2015) for the European population and the hg19 genome. ld.file <- \"EUR.hg19\" # output prefix prefix <- \"EUR\" # Read in the target phenotype file target.pheno <- fread ( \"EUR.height\" )[, c ( \"FID\" , \"IID\" , \"Height\" )] # Read in the summary statistics ss <- fread (sum.stat) # Remove P-value = 0, which causes problem in the transformation ss <- ss[ ! P == 0 ] # Transform the P-values into correlation cor <- p2cor (p = ss $ P, n = ss $ N, sign = log (ss $ OR) ) fam <- fread ( paste0 (bfile, \".fam\" )) fam[,ID := do.call (paste, c (.SD, sep = \":\" )),.SDcols = c ( 1:2 )] # Run the lassosum pipeline # The cluster parameter is used for multi-threading # You can ignore that if you do not wish to perform multi-threaded processing out <- lassosum.pipeline ( cor = cor, chr = ss $ CHR, pos = ss $ BP, A1 = ss $ A1, A2 = ss $ A2, ref.bfile = bfile, test.bfile = bfile, LDblocks = ld.file, cluster = cl ) # Store the R2 results target.res <- validate (out, pheno = as.data.frame (target.pheno), covar = as.data.frame (cov)) # Get the maximum R2 r2 <- max (target.res $ validation.table $ value) ^2 How much phenotypic variation does the \"best-fit\" PRS explain? 0.2474679","title":"Running PRS analysis"},{"location":"ldpred/","text":"Background \u00b6 LDpred-2 is one of the dedicated PRS programs which is an R package that uses a Bayesian approach to polygenic risk scoring. Installing LDpred-2 \u00b6 Note The script used here is based on LDpred 2 implemented under bigsnpr version 1.4.7 Note For more details, please refer to LDpred 2's homepage You can install LDpred and its dependencies in R with the following command: install.packages ( \"remotes\" ) library (remotes) remotes :: install_github ( \"https://github.com/privefl/bigsnpr.git\" ) Note For mac users, you might need to follow the guide here to be able to install LDpred2 Required Data \u00b6 We assume that you have the following files (or you can download it from here ): File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.cov This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples Warning While we do provide a rough guide on how to perform LDpred on bed files separated into individual chromosomes, this script is untested and extra caution is required 0. Prepare workspace \u00b6 On some server, you might need to first use the following code in order to run LDpred with multi-thread prepare workspace and load bigsnpr library (bigsnpr) options (bigstatsr.check.parallel.blas = FALSE ) options (default.nproc.blas = NULL ) 1. Read in the phenotype and covariate files \u00b6 read in phenotype and covariates library (data.table) library (magrittr) phenotype <- fread ( \"EUR.height\" ) covariate <- fread ( \"EUR.cov\" ) pcs <- fread ( \"EUR.eigenvec\" ) # rename columns colnames (pcs) <- c ( \"FID\" , \"IID\" , paste0 ( \"PC\" , 1:6 )) # generate required table pheno <- merge (phenotype, covariate) %>% merge (., pcs) 2. Obtain HapMap3 SNPs \u00b6 LDpred2 authors recommend restricting the analysis to only the HapMap3 SNPs load HapMap3 SNPs info <- readRDS ( url ( \"https://github.com/privefl/bigsnpr/raw/master/data-raw/hm3_variants.rds\" )) 3. Load and transform the summary statistic file \u00b6 Load summary statistic file # Read in the summary statistic file sumstats <- bigreadr :: fread2 ( \"Height.QC.gz\" ) # LDpred 2 require the header to follow the exact naming names (sumstats) <- c ( \"chr\" , \"pos\" , \"rsid\" , \"a1\" , \"a0\" , \"n_eff\" , \"beta_se\" , \"p\" , \"OR\" , \"INFO\" , \"MAF\" ) # Transform the OR into log(OR) sumstats $ beta <- log (sumstats $ OR) # Filter out hapmap SNPs sumstats <- sumstats[sumstats $ rsid %in% info $ rsid,] Warning Here, we know the exact ordering of the summary statistics file. However, in many cases, the ordering of the summary statistics differ, thus one must rename the columns according to their actual ordering 3. Calculate the LD matrix \u00b6 Genome Wide bed file # Get maximum amount of cores NCORES <- nb_cores () # Open a temporary file tmp <- tempfile (tmpdir = \"tmp-data\" ) on.exit ( file.remove ( paste0 (tmp, \".sbk\" )), add = TRUE ) # Initialize variables for storing the LD score and LD matrix corr <- NULL ld <- NULL # We want to know the ordering of samples in the bed file fam.order <- NULL # preprocess the bed file (only need to do once for each data set) snp_readBed ( \"EUR.QC.bed\" ) # now attach the genotype object obj.bigSNP <- snp_attach ( \"EUR.QC.rds\" ) # extract the SNP information from the genotype map <- obj.bigSNP $ map[ -3 ] names (map) <- c ( \"chr\" , \"rsid\" , \"pos\" , \"a1\" , \"a0\" ) # perform SNP matching info_snp <- snp_match (sumstats, map) # Assign the genotype to a variable for easier downstream analysis genotype <- obj.bigSNP $ genotypes # Rename the data structures CHR <- map $ chr POS <- map $ pos # get the CM information from 1000 Genome # will download the 1000G file to the current directory (\".\") POS2 <- snp_asGeneticPos (CHR, POS, dir = \".\" ) # calculate LD for (chr in 1:22 ) { # Extract SNPs that are included in the chromosome ind.chr <- which (info_snp $ chr == chr) ind.chr2 <- info_snp $ `_NUM_ID_`[ind.chr] # Calculate the LD corr0 <- snp_cor ( genotype, ind.col = ind.chr2, ncores = NCORES, infos.pos = POS2[ind.chr2], size = 3 / 1000 ) if (chr == 1 ) { ld <- Matrix :: colSums (corr0 ^2 ) corr <- as_SFBM (corr0, tmp) } else { ld <- c (ld, Matrix :: colSums (corr0 ^2 )) corr $ add_columns (corr0, nrow (corr)) } } # We assume the fam order is the same across different chromosomes fam.order <- as.data.table (obj.bigSNP $ fam) # Rename fam order setnames (fam.order, c ( \"family.ID\" , \"sample.ID\" ), c ( \"FID\" , \"IID\" )) Chromosome separated bed files # Get maximum amount of cores NCORES <- nb_cores () # Open a temporary file tmp <- tempfile (tmpdir = \"tmp-data\" ) on.exit ( file.remove ( paste0 (tmp, \".sbk\" )), add = TRUE ) # Initialize variables for storing the LD score and LD matrix corr <- NULL ld <- NULL # We want to know the ordering of samples in the bed file info_snp <- NULL fam.order <- NULL for (chr in 1:22 ) { # preprocess the bed file (only need to do once for each data set) # Assuming the file naming is EUR_chr#.bed snp_readBed ( paste0 ( \"EUR_chr\" ,chr, \".bed\" )) # now attach the genotype object obj.bigSNP <- snp_attach ( paste0 ( \"EUR_chr\" ,chr, \".rds\" )) # extract the SNP information from the genotype map <- obj.bigSNP $ map[ -3 ] names (map) <- c ( \"chr\" , \"rsid\" , \"pos\" , \"a1\" , \"a0\" ) # perform SNP matching tmp_snp <- snp_match (sumstats[sumstats $ chr == chr,], map) info_snp <- rbind (info_snp, tmp_snp) # Assign the genotype to a variable for easier downstream analysis genotype <- obj.bigSNP $ genotypes # Rename the data structures CHR <- map $ chr POS <- map $ pos # get the CM information from 1000 Genome # will download the 1000G file to the current directory (\".\") POS2 <- snp_asGeneticPos (CHR, POS, dir = \".\" ) # calculate LD # Extract SNPs that are included in the chromosome ind.chr <- which (tmp_snp $ chr == chr) ind.chr2 <- tmp_snp $ `_NUM_ID_`[ind.chr] # Calculate the LD corr0 <- snp_cor ( genotype, ind.col = ind.chr2, ncores = NCORES, infos.pos = POS2[ind.chr2], size = 3 / 1000 ) if (chr == 1 ) { ld <- Matrix :: colSums (corr0 ^2 ) corr <- as_SFBM (corr0, tmp) } else { ld <- c (ld, Matrix :: colSums (corr0 ^2 )) corr $ add_columns (corr0, nrow (corr)) } # We assume the fam order is the same across different chromosomes if ( is.null (fam.order)){ fam.order <- as.data.table (obj.bigSNP $ fam) } } # Rename fam order setnames (fam.order, c ( \"family.ID\" , \"sample.ID\" ), c ( \"FID\" , \"IID\" )) 4. Perform LD score regression \u00b6 Perform LD score regression df_beta <- info_snp[, c ( \"beta\" , \"beta_se\" , \"n_eff\" , \"_NUM_ID_\" )] ldsc <- snp_ldsc ( ld, length (ld), chi2 = (df_beta $ beta / df_beta $ beta_se) ^2 , sample_size = df_beta $ n_eff, blocks = NULL ) h2_est <- ldsc[[ \"h2\" ]] 5. Calculate the null R2 \u00b6 Calculate the null R2 (quantitative trait) # Reformat the phenotype file such that y is of the same order as the # sample ordering in the genotype file y <- pheno[fam.order, on = c ( \"FID\" , \"IID\" )] # Calculate the null R2 # use glm for binary trait # (will also need the fmsb package to calculate the pseudo R2) null.model <- paste ( \"PC\" , 1:6 , sep = \"\" , collapse = \"+\" ) %>% paste0 ( \"Height~Sex+\" , .) %>% as.formula %>% lm (., data = y) %>% summary null.r2 <- null.model $ r.squared Calculate the null R2 (binary trait) library (fmsb) # Reformat the phenotype file such that y is of the same order as the # sample ordering in the genotype file y <- pheno[fam.order, on = c ( \"FID\" , \"IID\" )] # Calculate the null R2 # use glm for binary trait # (will also need the fmsb package to calculate the pseudo R2) null.model <- paste ( \"PC\" , 1:6 , sep = \"\" , collapse = \"+\" ) %>% paste0 ( \"Height~Sex+\" , .) %>% as.formula %>% glm (., data = y, family = binomial) %>% summary null.r2 <- fmsb :: NagelkerkeR2 (null.model) Important Scripts for binary trait analysis only serve as a reference as we have not simulate any binary traits. In addition, Nagelkerke \\(R^2\\) is biased when there are ascertainment of samples. For more information, please refer to this paper infinitesimal model beta_inf <- snp_ldpred2_inf (corr, df_beta, h2 = h2_est) grid model # Prepare data for grid model p_seq <- signif ( seq_log ( 1e-4 , 1 , length.out = 17 ), 2 ) h2_seq <- round (h2_est * c ( 0.7 , 1 , 1.4 ), 4 ) grid.param <- expand.grid (p = p_seq, h2 = h2_seq, sparse = c ( FALSE , TRUE )) # Get adjusted beta from grid model beta_grid <- snp_ldpred2_grid (corr, df_beta, grid.param, ncores = NCORES) auto model # Get adjusted beta from the auto model multi_auto <- snp_ldpred2_auto ( corr, df_beta, h2_init = h2_est, vec_p_init = seq_log ( 1e-4 , 0.9 , length.out = NCORES), ncores = NCORES ) beta_auto <- sapply (multi_auto, function (auto) auto $ beta_est) 7. Obtain model PRS \u00b6 Using Genome wide bed file \u00b6 infinitesimal model if ( is.null (obj.bigSNP)){ obj.bigSNP <- snp_attach ( \"EUR.QC.rds\" ) } genotype <- obj.bigSNP $ genotypes # calculate PRS for all samples ind.test <- 1: nrow (genotype) pred_inf <- big_prodVec ( genotype, beta_inf, ind.row = ind.test, ind.col = info_snp $ `_NUM_ID_`) grid model if ( is.null (obj.bigSNP)){ obj.bigSNP <- snp_attach ( \"EUR.QC.rds\" ) } genotype <- obj.bigSNP $ genotypes # calculate PRS for all samples ind.test <- 1: nrow (genotype) pred_grid <- big_prodMat ( genotype, beta_grid, ind.col = info_snp $ `_NUM_ID_`) auto model if ( is.null (obj.bigSNP)){ obj.bigSNP <- snp_attach ( \"EUR.QC.rds\" ) } genotype <- obj.bigSNP $ genotypes # calculate PRS for all samples ind.test <- 1: nrow (genotype) pred_auto <- big_prodMat (genotype, beta_auto, ind.row = ind.test, ind.col = info_snp $ `_NUM_ID_`) # scale the PRS generated from AUTO pred_scaled <- apply (pred_auto, 2 , sd) final_beta_auto <- rowMeans (beta_auto[, abs (pred_scaled - median (pred_scaled)) < 3 * mad (pred_scaled)]) pred_auto <- big_prodVec (genotype, final_beta_auto, ind.row = ind.test, ind.col = info_snp $ `_NUM_ID_`) Using chromosome separated bed files \u00b6 infinitesimal model pred_inf <- NULL for (chr in 1:22 ){ obj.bigSNP <- snp_attach ( paste0 ( \"EUR_chr\" ,chr, \".rds\" )) genotype <- obj.bigSNP $ genotypes # calculate PRS for all samples ind.test <- 1: nrow (genotype) # Extract SNPs in this chromosome chr.idx <- which (info_snp $ chr == chr) ind.chr <- info_snp $ `_NUM_ID_`[chr.idx] tmp <- big_prodVec (genotype, beta_inf, ind.row = ind.test, ind.col = ind.chr) if ( is.null (pred_inf)){ pred_inf <- tmp }else{ pred_inf <- pred_inf + tmp } } grid model pred_grid <- NULL for (chr in 1:22 ){ obj.bigSNP <- snp_attach ( paste0 ( \"EUR_chr\" ,chr, \"_.rds\" )) genotype <- obj.bigSNP $ genotypes # calculate PRS for all samples ind.test <- 1: nrow (genotype) # Extract SNPs in this chromosome chr.idx <- which (info_snp $ chr == chr) ind.chr <- info_snp $ `_NUM_ID_`[chr.idx] tmp <- big_prodMat ( genotype, beta_grid, ind.col = ind.chr) if ( is.null (pred_grid)){ pred_grid <- tmp }else{ pred_grid <- pred_grid + tmp } } auto model pred_auto <- NULL for (chr in 1:22 ){ obj.bigSNP <- snp_attach ( paste0 ( \"EUR_chr\" ,chr, \"_.rds\" )) genotype <- obj.bigSNP $ genotypes # calculate PRS for all samples ind.test <- 1: nrow (genotype) # Extract SNPs in this chromosome chr.idx <- which (info_snp $ chr == chr) ind.chr <- info_snp $ `_NUM_ID_`[chr.idx] tmp <- big_prodMat (genotype, beta_auto, ind.row = ind.test, ind.col = ind.chr) # scale the PRS generated from AUTO pred_scaled <- apply (tmp, 2 , sd) final_beta_auto <- rowMeans (tmp[chr.idx, abs (pred_scaled - median (pred_scaled)) < 3 * mad (pred_scaled)]) tmp <- big_prodVec (genotype, final_beta_auto, ind.row = ind.test, ind.col = ind.chr) if ( is.null (pred_auto)){ pred_auto <- tmp }else{ pred_auto <- pred_auto + tmp } } 8. Get the final performance of the LDpred models \u00b6 infinitesimal model reg.formula <- paste ( \"PC\" , 1:6 , sep = \"\" , collapse = \"+\" ) %>% paste0 ( \"Height~PRS+Sex+\" , .) %>% as.formula reg.dat <- y reg.dat $ PRS <- pred_inf inf.model <- lm (reg.formula, dat = reg.dat) %>% summary (result <- data.table ( infinitesimal = inf.model $ r.squared - null.r2, null = null.r2 )) grid model reg.formula <- paste ( \"PC\" , 1:6 , sep = \"\" , collapse = \"+\" ) %>% paste0 ( \"Height~PRS+Sex+\" , .) %>% as.formula reg.dat <- y max.r2 <- 0 for (i in 1: ncol (pred_grid)){ reg.dat $ PRS <- pred_grid[,i] grid.model <- lm (reg.formula, dat = reg.dat) %>% summary if (max.r2 < grid.model $ r.squared){ max.r2 <- grid.model $ r.squared } } (result <- data.table ( grid = max.r2 - null.r2, null = null.r2 )) auto model reg.formula <- paste ( \"PC\" , 1:6 , sep = \"\" , collapse = \"+\" ) %>% paste0 ( \"Height~PRS+Sex+\" , .) %>% as.formula reg.dat <- y reg.dat $ PRS <- pred_auto auto.model <- lm (reg.formula, dat = reg.dat) %>% summary (result <- data.table ( auto = auto.model $ r.squared - null.r2, null = null.r2 )) How much phenotypic variation does the PRS from each model explain? Infinitesimal = 0.0248696 Grid Model = 0.001746926 Auto Model = 0.1751478","title":"LDpred-2"},{"location":"ldpred/#background","text":"LDpred-2 is one of the dedicated PRS programs which is an R package that uses a Bayesian approach to polygenic risk scoring.","title":"Background"},{"location":"ldpred/#installing-ldpred-2","text":"Note The script used here is based on LDpred 2 implemented under bigsnpr version 1.4.7 Note For more details, please refer to LDpred 2's homepage You can install LDpred and its dependencies in R with the following command: install.packages ( \"remotes\" ) library (remotes) remotes :: install_github ( \"https://github.com/privefl/bigsnpr.git\" ) Note For mac users, you might need to follow the guide here to be able to install LDpred2","title":"Installing LDpred-2"},{"location":"ldpred/#required-data","text":"We assume that you have the following files (or you can download it from here ): File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.cov This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples Warning While we do provide a rough guide on how to perform LDpred on bed files separated into individual chromosomes, this script is untested and extra caution is required","title":"Required Data"},{"location":"ldpred/#0-prepare-workspace","text":"On some server, you might need to first use the following code in order to run LDpred with multi-thread prepare workspace and load bigsnpr library (bigsnpr) options (bigstatsr.check.parallel.blas = FALSE ) options (default.nproc.blas = NULL )","title":"0. Prepare workspace"},{"location":"ldpred/#1-read-in-the-phenotype-and-covariate-files","text":"read in phenotype and covariates library (data.table) library (magrittr) phenotype <- fread ( \"EUR.height\" ) covariate <- fread ( \"EUR.cov\" ) pcs <- fread ( \"EUR.eigenvec\" ) # rename columns colnames (pcs) <- c ( \"FID\" , \"IID\" , paste0 ( \"PC\" , 1:6 )) # generate required table pheno <- merge (phenotype, covariate) %>% merge (., pcs)","title":"1. Read in the phenotype and covariate files"},{"location":"ldpred/#2-obtain-hapmap3-snps","text":"LDpred2 authors recommend restricting the analysis to only the HapMap3 SNPs load HapMap3 SNPs info <- readRDS ( url ( \"https://github.com/privefl/bigsnpr/raw/master/data-raw/hm3_variants.rds\" ))","title":"2. Obtain HapMap3 SNPs"},{"location":"ldpred/#3-load-and-transform-the-summary-statistic-file","text":"Load summary statistic file # Read in the summary statistic file sumstats <- bigreadr :: fread2 ( \"Height.QC.gz\" ) # LDpred 2 require the header to follow the exact naming names (sumstats) <- c ( \"chr\" , \"pos\" , \"rsid\" , \"a1\" , \"a0\" , \"n_eff\" , \"beta_se\" , \"p\" , \"OR\" , \"INFO\" , \"MAF\" ) # Transform the OR into log(OR) sumstats $ beta <- log (sumstats $ OR) # Filter out hapmap SNPs sumstats <- sumstats[sumstats $ rsid %in% info $ rsid,] Warning Here, we know the exact ordering of the summary statistics file. However, in many cases, the ordering of the summary statistics differ, thus one must rename the columns according to their actual ordering","title":"3. Load and transform the summary statistic file"},{"location":"ldpred/#3-calculate-the-ld-matrix","text":"Genome Wide bed file # Get maximum amount of cores NCORES <- nb_cores () # Open a temporary file tmp <- tempfile (tmpdir = \"tmp-data\" ) on.exit ( file.remove ( paste0 (tmp, \".sbk\" )), add = TRUE ) # Initialize variables for storing the LD score and LD matrix corr <- NULL ld <- NULL # We want to know the ordering of samples in the bed file fam.order <- NULL # preprocess the bed file (only need to do once for each data set) snp_readBed ( \"EUR.QC.bed\" ) # now attach the genotype object obj.bigSNP <- snp_attach ( \"EUR.QC.rds\" ) # extract the SNP information from the genotype map <- obj.bigSNP $ map[ -3 ] names (map) <- c ( \"chr\" , \"rsid\" , \"pos\" , \"a1\" , \"a0\" ) # perform SNP matching info_snp <- snp_match (sumstats, map) # Assign the genotype to a variable for easier downstream analysis genotype <- obj.bigSNP $ genotypes # Rename the data structures CHR <- map $ chr POS <- map $ pos # get the CM information from 1000 Genome # will download the 1000G file to the current directory (\".\") POS2 <- snp_asGeneticPos (CHR, POS, dir = \".\" ) # calculate LD for (chr in 1:22 ) { # Extract SNPs that are included in the chromosome ind.chr <- which (info_snp $ chr == chr) ind.chr2 <- info_snp $ `_NUM_ID_`[ind.chr] # Calculate the LD corr0 <- snp_cor ( genotype, ind.col = ind.chr2, ncores = NCORES, infos.pos = POS2[ind.chr2], size = 3 / 1000 ) if (chr == 1 ) { ld <- Matrix :: colSums (corr0 ^2 ) corr <- as_SFBM (corr0, tmp) } else { ld <- c (ld, Matrix :: colSums (corr0 ^2 )) corr $ add_columns (corr0, nrow (corr)) } } # We assume the fam order is the same across different chromosomes fam.order <- as.data.table (obj.bigSNP $ fam) # Rename fam order setnames (fam.order, c ( \"family.ID\" , \"sample.ID\" ), c ( \"FID\" , \"IID\" )) Chromosome separated bed files # Get maximum amount of cores NCORES <- nb_cores () # Open a temporary file tmp <- tempfile (tmpdir = \"tmp-data\" ) on.exit ( file.remove ( paste0 (tmp, \".sbk\" )), add = TRUE ) # Initialize variables for storing the LD score and LD matrix corr <- NULL ld <- NULL # We want to know the ordering of samples in the bed file info_snp <- NULL fam.order <- NULL for (chr in 1:22 ) { # preprocess the bed file (only need to do once for each data set) # Assuming the file naming is EUR_chr#.bed snp_readBed ( paste0 ( \"EUR_chr\" ,chr, \".bed\" )) # now attach the genotype object obj.bigSNP <- snp_attach ( paste0 ( \"EUR_chr\" ,chr, \".rds\" )) # extract the SNP information from the genotype map <- obj.bigSNP $ map[ -3 ] names (map) <- c ( \"chr\" , \"rsid\" , \"pos\" , \"a1\" , \"a0\" ) # perform SNP matching tmp_snp <- snp_match (sumstats[sumstats $ chr == chr,], map) info_snp <- rbind (info_snp, tmp_snp) # Assign the genotype to a variable for easier downstream analysis genotype <- obj.bigSNP $ genotypes # Rename the data structures CHR <- map $ chr POS <- map $ pos # get the CM information from 1000 Genome # will download the 1000G file to the current directory (\".\") POS2 <- snp_asGeneticPos (CHR, POS, dir = \".\" ) # calculate LD # Extract SNPs that are included in the chromosome ind.chr <- which (tmp_snp $ chr == chr) ind.chr2 <- tmp_snp $ `_NUM_ID_`[ind.chr] # Calculate the LD corr0 <- snp_cor ( genotype, ind.col = ind.chr2, ncores = NCORES, infos.pos = POS2[ind.chr2], size = 3 / 1000 ) if (chr == 1 ) { ld <- Matrix :: colSums (corr0 ^2 ) corr <- as_SFBM (corr0, tmp) } else { ld <- c (ld, Matrix :: colSums (corr0 ^2 )) corr $ add_columns (corr0, nrow (corr)) } # We assume the fam order is the same across different chromosomes if ( is.null (fam.order)){ fam.order <- as.data.table (obj.bigSNP $ fam) } } # Rename fam order setnames (fam.order, c ( \"family.ID\" , \"sample.ID\" ), c ( \"FID\" , \"IID\" ))","title":"3. Calculate the LD matrix"},{"location":"ldpred/#4-perform-ld-score-regression","text":"Perform LD score regression df_beta <- info_snp[, c ( \"beta\" , \"beta_se\" , \"n_eff\" , \"_NUM_ID_\" )] ldsc <- snp_ldsc ( ld, length (ld), chi2 = (df_beta $ beta / df_beta $ beta_se) ^2 , sample_size = df_beta $ n_eff, blocks = NULL ) h2_est <- ldsc[[ \"h2\" ]]","title":"4. Perform LD score regression"},{"location":"ldpred/#5-calculate-the-null-r2","text":"Calculate the null R2 (quantitative trait) # Reformat the phenotype file such that y is of the same order as the # sample ordering in the genotype file y <- pheno[fam.order, on = c ( \"FID\" , \"IID\" )] # Calculate the null R2 # use glm for binary trait # (will also need the fmsb package to calculate the pseudo R2) null.model <- paste ( \"PC\" , 1:6 , sep = \"\" , collapse = \"+\" ) %>% paste0 ( \"Height~Sex+\" , .) %>% as.formula %>% lm (., data = y) %>% summary null.r2 <- null.model $ r.squared Calculate the null R2 (binary trait) library (fmsb) # Reformat the phenotype file such that y is of the same order as the # sample ordering in the genotype file y <- pheno[fam.order, on = c ( \"FID\" , \"IID\" )] # Calculate the null R2 # use glm for binary trait # (will also need the fmsb package to calculate the pseudo R2) null.model <- paste ( \"PC\" , 1:6 , sep = \"\" , collapse = \"+\" ) %>% paste0 ( \"Height~Sex+\" , .) %>% as.formula %>% glm (., data = y, family = binomial) %>% summary null.r2 <- fmsb :: NagelkerkeR2 (null.model) Important Scripts for binary trait analysis only serve as a reference as we have not simulate any binary traits. In addition, Nagelkerke \\(R^2\\) is biased when there are ascertainment of samples. For more information, please refer to this paper infinitesimal model beta_inf <- snp_ldpred2_inf (corr, df_beta, h2 = h2_est) grid model # Prepare data for grid model p_seq <- signif ( seq_log ( 1e-4 , 1 , length.out = 17 ), 2 ) h2_seq <- round (h2_est * c ( 0.7 , 1 , 1.4 ), 4 ) grid.param <- expand.grid (p = p_seq, h2 = h2_seq, sparse = c ( FALSE , TRUE )) # Get adjusted beta from grid model beta_grid <- snp_ldpred2_grid (corr, df_beta, grid.param, ncores = NCORES) auto model # Get adjusted beta from the auto model multi_auto <- snp_ldpred2_auto ( corr, df_beta, h2_init = h2_est, vec_p_init = seq_log ( 1e-4 , 0.9 , length.out = NCORES), ncores = NCORES ) beta_auto <- sapply (multi_auto, function (auto) auto $ beta_est)","title":"5. Calculate the null R2"},{"location":"ldpred/#7-obtain-model-prs","text":"","title":"7. Obtain model PRS"},{"location":"ldpred/#using-genome-wide-bed-file","text":"infinitesimal model if ( is.null (obj.bigSNP)){ obj.bigSNP <- snp_attach ( \"EUR.QC.rds\" ) } genotype <- obj.bigSNP $ genotypes # calculate PRS for all samples ind.test <- 1: nrow (genotype) pred_inf <- big_prodVec ( genotype, beta_inf, ind.row = ind.test, ind.col = info_snp $ `_NUM_ID_`) grid model if ( is.null (obj.bigSNP)){ obj.bigSNP <- snp_attach ( \"EUR.QC.rds\" ) } genotype <- obj.bigSNP $ genotypes # calculate PRS for all samples ind.test <- 1: nrow (genotype) pred_grid <- big_prodMat ( genotype, beta_grid, ind.col = info_snp $ `_NUM_ID_`) auto model if ( is.null (obj.bigSNP)){ obj.bigSNP <- snp_attach ( \"EUR.QC.rds\" ) } genotype <- obj.bigSNP $ genotypes # calculate PRS for all samples ind.test <- 1: nrow (genotype) pred_auto <- big_prodMat (genotype, beta_auto, ind.row = ind.test, ind.col = info_snp $ `_NUM_ID_`) # scale the PRS generated from AUTO pred_scaled <- apply (pred_auto, 2 , sd) final_beta_auto <- rowMeans (beta_auto[, abs (pred_scaled - median (pred_scaled)) < 3 * mad (pred_scaled)]) pred_auto <- big_prodVec (genotype, final_beta_auto, ind.row = ind.test, ind.col = info_snp $ `_NUM_ID_`)","title":"Using Genome wide bed file"},{"location":"ldpred/#using-chromosome-separated-bed-files","text":"infinitesimal model pred_inf <- NULL for (chr in 1:22 ){ obj.bigSNP <- snp_attach ( paste0 ( \"EUR_chr\" ,chr, \".rds\" )) genotype <- obj.bigSNP $ genotypes # calculate PRS for all samples ind.test <- 1: nrow (genotype) # Extract SNPs in this chromosome chr.idx <- which (info_snp $ chr == chr) ind.chr <- info_snp $ `_NUM_ID_`[chr.idx] tmp <- big_prodVec (genotype, beta_inf, ind.row = ind.test, ind.col = ind.chr) if ( is.null (pred_inf)){ pred_inf <- tmp }else{ pred_inf <- pred_inf + tmp } } grid model pred_grid <- NULL for (chr in 1:22 ){ obj.bigSNP <- snp_attach ( paste0 ( \"EUR_chr\" ,chr, \"_.rds\" )) genotype <- obj.bigSNP $ genotypes # calculate PRS for all samples ind.test <- 1: nrow (genotype) # Extract SNPs in this chromosome chr.idx <- which (info_snp $ chr == chr) ind.chr <- info_snp $ `_NUM_ID_`[chr.idx] tmp <- big_prodMat ( genotype, beta_grid, ind.col = ind.chr) if ( is.null (pred_grid)){ pred_grid <- tmp }else{ pred_grid <- pred_grid + tmp } } auto model pred_auto <- NULL for (chr in 1:22 ){ obj.bigSNP <- snp_attach ( paste0 ( \"EUR_chr\" ,chr, \"_.rds\" )) genotype <- obj.bigSNP $ genotypes # calculate PRS for all samples ind.test <- 1: nrow (genotype) # Extract SNPs in this chromosome chr.idx <- which (info_snp $ chr == chr) ind.chr <- info_snp $ `_NUM_ID_`[chr.idx] tmp <- big_prodMat (genotype, beta_auto, ind.row = ind.test, ind.col = ind.chr) # scale the PRS generated from AUTO pred_scaled <- apply (tmp, 2 , sd) final_beta_auto <- rowMeans (tmp[chr.idx, abs (pred_scaled - median (pred_scaled)) < 3 * mad (pred_scaled)]) tmp <- big_prodVec (genotype, final_beta_auto, ind.row = ind.test, ind.col = ind.chr) if ( is.null (pred_auto)){ pred_auto <- tmp }else{ pred_auto <- pred_auto + tmp } }","title":"Using chromosome separated bed files"},{"location":"ldpred/#8-get-the-final-performance-of-the-ldpred-models","text":"infinitesimal model reg.formula <- paste ( \"PC\" , 1:6 , sep = \"\" , collapse = \"+\" ) %>% paste0 ( \"Height~PRS+Sex+\" , .) %>% as.formula reg.dat <- y reg.dat $ PRS <- pred_inf inf.model <- lm (reg.formula, dat = reg.dat) %>% summary (result <- data.table ( infinitesimal = inf.model $ r.squared - null.r2, null = null.r2 )) grid model reg.formula <- paste ( \"PC\" , 1:6 , sep = \"\" , collapse = \"+\" ) %>% paste0 ( \"Height~PRS+Sex+\" , .) %>% as.formula reg.dat <- y max.r2 <- 0 for (i in 1: ncol (pred_grid)){ reg.dat $ PRS <- pred_grid[,i] grid.model <- lm (reg.formula, dat = reg.dat) %>% summary if (max.r2 < grid.model $ r.squared){ max.r2 <- grid.model $ r.squared } } (result <- data.table ( grid = max.r2 - null.r2, null = null.r2 )) auto model reg.formula <- paste ( \"PC\" , 1:6 , sep = \"\" , collapse = \"+\" ) %>% paste0 ( \"Height~PRS+Sex+\" , .) %>% as.formula reg.dat <- y reg.dat $ PRS <- pred_auto auto.model <- lm (reg.formula, dat = reg.dat) %>% summary (result <- data.table ( auto = auto.model $ r.squared - null.r2, null = null.r2 )) How much phenotypic variation does the PRS from each model explain? Infinitesimal = 0.0248696 Grid Model = 0.001746926 Auto Model = 0.1751478","title":"8. Get the final performance of the LDpred models"},{"location":"plink/","text":"Background \u00b6 On this page, you will compute PRS using the popular genetic analyses tool plink - while plink is not a dedicated PRS software, you can perform every required steps of the C+T approach with plink . This multi-step process is a good way to learn the processes involved in computing PRS, which are typically performed automatically by PRS software. Required Data \u00b6 In the previous sections, we have generated the following files: File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.cov This file contains the covariates of the samples Update Effect Size \u00b6 When the effect size relates to disease risk and is thus given as an odds ratio (OR), rather than BETA (for continuous traits), then the PRS is computed as a product of ORs. To simplify this calculation, we take the natural logarithm of the OR so that the PRS can be computed using summation instead (which can be back-transformed afterwards). We can obtain the transformed summary statistics with R : Without data.table dat <- read.table ( gzfile ( \"Height.QC.gz\" ), header = T ) dat $ BETA <- log (dat $ OR) write.table (dat, \"Height.QC.Transformed\" , quote = F , row.names = F ) q () # exit R With data.table library (data.table) dat <- fread ( \"Height.QC.gz\" ) fwrite (dat[,BETA := log (OR)], \"Height.QC.Transformed\" , sep = \"\\t\" ) q () # exit R Warning Due to rounding of values, using awk to log transform OR can lead to less accurate results. Therefore, we recommend performing the transformation in R or allow the PRS software to perform the transformation directly. Clumping \u00b6 Linkage disequilibrium, which corresponds to the correlation between the genotypes of genetic variants across the genome, makes identifying the contribution from causal independent genetic variants extremely challenging. One way of approximately capturing the right level of causal signal is to perform clumping, which removes SNPs in ways that only weakly correlated SNPs are retained but preferentially retaining the SNPs most associated with the phenotype under study. Clumping can be performed using the following command in plink : plink \\ --bfile EUR.QC \\ --clump-p1 1 \\ --clump-r2 0 .1 \\ --clump-kb 250 \\ --clump Height.QC.Transformed \\ --clump-snp-field SNP \\ --clump-field P \\ --out EUR Each of the new parameters corresponds to the following Parameter Value Description clump-p1 1 P-value threshold for a SNP to be included as an index SNP. 1 is selected such that all SNPs are include for clumping clump-r2 0.1 SNPs having \\(r^2\\) higher than 0.1 with the index SNPs will be removed clump-kb 250 SNPs within 250k of the index SNP are considered for clumping clump Height.QC.Transformed Base data (summary statistic) file containing the P-value information clump-snp-field SNP Specifies that the column SNP contains the SNP IDs clump-field P Specifies that the column P contains the P-value information A more detailed description of the clumping process can be found here Note The \\(r^2\\) values computed by --clump are based on maximum likelihood haplotype frequency estimates This will generate EUR.clumped , containing the index SNPs after clumping is performed. We can extract the index SNP ID by performing the following command: awk 'NR!=1{print $3}' EUR.clumped > EUR.valid.snp $3 because the third column contains the SNP ID Note If your target data are small (e.g. N < 500) then you can use the 1000 Genomes Project samples for the LD calculation. Make sure to use the population that most closely reflects represents the base sample. Generate PRS \u00b6 plink provides a convenient function --score and --q-score-range for calculating polygenic scores. We will need three files: The base data file: Height.QC.Transformed A file containing SNP IDs and their corresponding P-values ( $3 because SNP ID is located in the third column; $8 because the P-value is located in the eighth column) awk '{print $3,$8}' Height.QC.Transformed > SNP.pvalue A file containing the different P-value thresholds for inclusion of SNPs in the PRS. Here calculate PRS corresponding to a few thresholds for illustration purposes: echo \"0.001 0 0.001\" > range_list echo \"0.05 0 0.05\" >> range_list echo \"0.1 0 0.1\" >> range_list echo \"0.2 0 0.2\" >> range_list echo \"0.3 0 0.3\" >> range_list echo \"0.4 0 0.4\" >> range_list echo \"0.5 0 0.5\" >> range_list The format of the range_list file should be as follows: Name of Threshold Lower bound Upper Bound Note The threshold boundaries are inclusive. For example, for the 0.05 threshold, we include all SNPs with P-value from 0 to 0.05 , including any SNPs with P-value equal to 0.05 . We can then calculate the PRS with the following plink command: plink \\ --bfile EUR.QC \\ --score Height.QC.Transformed 3 4 12 header \\ --q-score-range range_list SNP.pvalue \\ --extract EUR.valid.snp \\ --out EUR The meaning of the new parameters are as follows: Paramter Value Description score Height.QC.Transformed 3 4 12 header We read from the Height.QC.Transformed file, assuming that the 3 st column is the SNP ID; 4 th column is the effective allele information; the 12 th column is the effect size estimate; and that the file contains a header q-score-range range_list SNP.pvalue We want to calculate PRS based on the thresholds defined in range_list , where the threshold values (P-values) were stored in SNP.pvalue The above command and range_list will generate 7 files: EUR.0.5.profile EUR.0.4.profile EUR.0.3.profile EUR.0.2.profile EUR.0.1.profile EUR.0.05.profile EUR.0.001.profile Note The default formula for PRS calculation in PLINK is: \\[ PRS_j =\\frac{ \\sum_i^NS_i*G_{ij}}{P*M_j} \\] where the effect size of SNP \\(i\\) is \\(S_i\\) ; the number of effect alleles observed in sample \\(j\\) is \\(G_{ij}\\) ; the ploidy of the sample is \\(P\\) (is generally 2 for humans); the total number of SNPs included in the PRS is \\(N\\) ; and the number of non-missing SNPs observed in sample \\(j\\) is \\(M_j\\) . If the sample has a missing genotype for SNP \\(i\\) , then the population minor allele frequency multiplied by the ploidy ( \\(MAF_i*P\\) ) is used instead of \\(G_{ij}\\) . Accounting for Population Stratification \u00b6 Population structure is the principal source of confounding in GWAS and is usually accounted for by incorporating principal components (PCs) as covariates. We can incorporate PCs into our PRS analysis to account for population stratification. Again, we can calculate the PCs using plink : # First, we need to perform prunning plink \\ --bfile EUR.QC \\ --indep-pairwise 200 50 0 .25 \\ --out EUR # Then we calculate the first 6 PCs plink \\ --bfile EUR.QC \\ --extract EUR.prune.in \\ --pca 6 \\ --out EUR Note One way to select the appropriate number of PCs is to perform GWAS on the phenotype under study with different numbers of PCs. LDSC analysis can then be performed on the set of GWAS summary statistics and the GWAS that used the number of PCs that gave an LDSC intercept closest to 1 should correspond to that for which population structure was most accurately controlled for. Here the PCs have been stored in the EUR.eigenvec file and can be used as covariates in the regression model to account for population stratification. Important If the base and target samples are collected from different worldwide populations then the results from the PRS analysis may be biased (see Section 3.4 of our paper). Finding the \"best-fit\" PRS \u00b6 The P-value threshold that provides the \"best-fit\" PRS under the C+T method is usually unknown. To approximate the \"best-fit\" PRS, we can perform a regression between PRS calculated at a range of P-value thresholds and then select the PRS that explains the highest phenotypic variance (please see Section 4.6 of our paper on overfitting issues). This can be achieved using R as follows: detail p.threshold <- c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) # Read in the phenotype file phenotype <- read.table ( \"EUR.height\" , header = T ) # Read in the PCs pcs <- read.table ( \"EUR.eigenvec\" , header = F ) # The default output from plink does not include a header # To make things simple, we will add the appropriate headers # (1:6 because there are 6 PCs) colnames (pcs) <- c ( \"FID\" , \"IID\" , paste0 ( \"PC\" , 1:6 )) # Read in the covariates (here, it is sex) covariate <- read.table ( \"EUR.cov\" , header = T ) # Now merge the files pheno <- merge ( merge (phenotype, covariate, by = c ( \"FID\" , \"IID\" )), pcs, by = c ( \"FID\" , \"IID\" )) # We can then calculate the null model (model with PRS) using a linear regression # (as height is quantitative) null.model <- lm (Height ~ ., data = pheno[, ! colnames (pheno) %in% c ( \"FID\" , \"IID\" )]) # And the R2 of the null model is null.r2 <- summary (null.model) $ r.squared prs.result <- NULL for (i in p.threshold){ # Go through each p-value threshold prs <- read.table ( paste0 ( \"EUR.\" ,i, \".profile\" ), header = T ) # Merge the prs with the phenotype matrix # We only want the FID, IID and PRS from the PRS file, therefore we only select the # relevant columns pheno.prs <- merge (pheno, prs[, c ( \"FID\" , \"IID\" , \"SCORE\" )], by = c ( \"FID\" , \"IID\" )) # Now perform a linear regression on Height with PRS and the covariates # ignoring the FID and IID from our model model <- lm (Height ~ ., data = pheno.prs[, ! colnames (pheno.prs) %in% c ( \"FID\" , \"IID\" )]) # model R2 is obtained as model.r2 <- summary (model) $ r.squared # R2 of PRS is simply calculated as the model R2 minus the null R2 prs.r2 <- model.r2 - null.r2 # We can also obtain the coeffcient and p-value of association of PRS as follow prs.coef <- summary (model) $ coeff[ \"SCORE\" ,] prs.beta <- as.numeric (prs.coef[ 1 ]) prs.se <- as.numeric (prs.coef[ 2 ]) prs.p <- as.numeric (prs.coef[ 4 ]) # We can then store the results prs.result <- rbind (prs.result, data.frame (Threshold = i, R2 = prs.r2, P = prs.p, BETA = prs.beta,SE = prs.se)) } # Best result is: prs.result[ which.max (prs.result $ R2),] q () # exit R quick p.threshold <- c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) phenotype <- read.table ( \"EUR.height\" , header = T ) pcs <- read.table ( \"EUR.eigenvec\" , header = F ) colnames (pcs) <- c ( \"FID\" , \"IID\" , paste0 ( \"PC\" , 1:6 )) covariate <- read.table ( \"EUR.cov\" , header = T ) pheno <- merge ( merge (phenotype, covariate, by = c ( \"FID\" , \"IID\" )), pcs, by = c ( \"FID\" , \"IID\" )) null.r2 <- summary ( lm (Height ~ ., data = pheno[, ! colnames (pheno) %in% c ( \"FID\" , \"IID\" )])) $ r.squared prs.result <- NULL for (i in p.threshold){ pheno.prs <- merge (pheno, read.table ( paste0 ( \"EUR.\" ,i, \".profile\" ), header = T )[, c ( \"FID\" , \"IID\" , \"SCORE\" )], by = c ( \"FID\" , \"IID\" )) model <- summary ( lm (Height ~ ., data = pheno.prs[, ! colnames (pheno.prs) %in% c ( \"FID\" , \"IID\" )])) model.r2 <- model $ r.squared prs.r2 <- model.r2 - null.r2 prs.coef <- model $ coeff[ \"SCORE\" ,] prs.result <- rbind (prs.result, data.frame (Threshold = i, R2 = prs.r2, P = as.numeric (prs.coef[ 4 ]), BETA = as.numeric (prs.coef[ 1 ]), SE = as.numeric (prs.coef[ 2 ]))) } print (prs.result[ which.max (prs.result $ R2),]) q () # exit R with data.table and magrittr library (data.table) library (magrittr) p.threshold <- c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) phenotype <- fread ( \"EUR.height\" ) pcs <- fread ( \"EUR.eigenvec\" , header = F ) %>% setnames (., colnames (.), c ( \"FID\" , \"IID\" , paste0 ( \"PC\" , 1:6 )) ) covariate <- fread ( \"EUR.cov\" ) pheno <- merge (phenotype, covariate) %>% merge (., pcs) null.r2 <- summary ( lm (Height ~ ., data = pheno[, - c ( \"FID\" , \"IID\" )])) $ r.squared prs.result <- NULL for (i in p.threshold){ pheno.prs <- paste0 ( \"EUR.\" , i, \".profile\" ) %>% fread (.) %>% .[, c ( \"FID\" , \"IID\" , \"SCORE\" )] %>% merge (., pheno, by = c ( \"FID\" , \"IID\" )) model <- lm (Height ~ ., data = pheno.prs[, - c ( \"FID\" , \"IID\" )]) %>% summary model.r2 <- model $ r.squared prs.r2 <- model.r2 - null.r2 prs.coef <- model $ coeff[ \"SCORE\" ,] prs.result %<>% rbind (., data.frame (Threshold = i, R2 = prs.r2, P = as.numeric (prs.coef[ 4 ]), BETA = as.numeric (prs.coef[ 1 ]), SE = as.numeric (prs.coef[ 2 ]))) } print (prs.result[ which.max (prs.result $ R2),]) q () # exit R Which P-value threshold generates the \"best-fit\" PRS? 0.3 How much phenotypic variation does the \"best-fit\" PRS explain? 0.1612372","title":"PLINK"},{"location":"plink/#background","text":"On this page, you will compute PRS using the popular genetic analyses tool plink - while plink is not a dedicated PRS software, you can perform every required steps of the C+T approach with plink . This multi-step process is a good way to learn the processes involved in computing PRS, which are typically performed automatically by PRS software.","title":"Background"},{"location":"plink/#required-data","text":"In the previous sections, we have generated the following files: File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.cov This file contains the covariates of the samples","title":"Required Data"},{"location":"plink/#update-effect-size","text":"When the effect size relates to disease risk and is thus given as an odds ratio (OR), rather than BETA (for continuous traits), then the PRS is computed as a product of ORs. To simplify this calculation, we take the natural logarithm of the OR so that the PRS can be computed using summation instead (which can be back-transformed afterwards). We can obtain the transformed summary statistics with R : Without data.table dat <- read.table ( gzfile ( \"Height.QC.gz\" ), header = T ) dat $ BETA <- log (dat $ OR) write.table (dat, \"Height.QC.Transformed\" , quote = F , row.names = F ) q () # exit R With data.table library (data.table) dat <- fread ( \"Height.QC.gz\" ) fwrite (dat[,BETA := log (OR)], \"Height.QC.Transformed\" , sep = \"\\t\" ) q () # exit R Warning Due to rounding of values, using awk to log transform OR can lead to less accurate results. Therefore, we recommend performing the transformation in R or allow the PRS software to perform the transformation directly.","title":"Update Effect Size"},{"location":"plink/#clumping","text":"Linkage disequilibrium, which corresponds to the correlation between the genotypes of genetic variants across the genome, makes identifying the contribution from causal independent genetic variants extremely challenging. One way of approximately capturing the right level of causal signal is to perform clumping, which removes SNPs in ways that only weakly correlated SNPs are retained but preferentially retaining the SNPs most associated with the phenotype under study. Clumping can be performed using the following command in plink : plink \\ --bfile EUR.QC \\ --clump-p1 1 \\ --clump-r2 0 .1 \\ --clump-kb 250 \\ --clump Height.QC.Transformed \\ --clump-snp-field SNP \\ --clump-field P \\ --out EUR Each of the new parameters corresponds to the following Parameter Value Description clump-p1 1 P-value threshold for a SNP to be included as an index SNP. 1 is selected such that all SNPs are include for clumping clump-r2 0.1 SNPs having \\(r^2\\) higher than 0.1 with the index SNPs will be removed clump-kb 250 SNPs within 250k of the index SNP are considered for clumping clump Height.QC.Transformed Base data (summary statistic) file containing the P-value information clump-snp-field SNP Specifies that the column SNP contains the SNP IDs clump-field P Specifies that the column P contains the P-value information A more detailed description of the clumping process can be found here Note The \\(r^2\\) values computed by --clump are based on maximum likelihood haplotype frequency estimates This will generate EUR.clumped , containing the index SNPs after clumping is performed. We can extract the index SNP ID by performing the following command: awk 'NR!=1{print $3}' EUR.clumped > EUR.valid.snp $3 because the third column contains the SNP ID Note If your target data are small (e.g. N < 500) then you can use the 1000 Genomes Project samples for the LD calculation. Make sure to use the population that most closely reflects represents the base sample.","title":"Clumping"},{"location":"plink/#generate-prs","text":"plink provides a convenient function --score and --q-score-range for calculating polygenic scores. We will need three files: The base data file: Height.QC.Transformed A file containing SNP IDs and their corresponding P-values ( $3 because SNP ID is located in the third column; $8 because the P-value is located in the eighth column) awk '{print $3,$8}' Height.QC.Transformed > SNP.pvalue A file containing the different P-value thresholds for inclusion of SNPs in the PRS. Here calculate PRS corresponding to a few thresholds for illustration purposes: echo \"0.001 0 0.001\" > range_list echo \"0.05 0 0.05\" >> range_list echo \"0.1 0 0.1\" >> range_list echo \"0.2 0 0.2\" >> range_list echo \"0.3 0 0.3\" >> range_list echo \"0.4 0 0.4\" >> range_list echo \"0.5 0 0.5\" >> range_list The format of the range_list file should be as follows: Name of Threshold Lower bound Upper Bound Note The threshold boundaries are inclusive. For example, for the 0.05 threshold, we include all SNPs with P-value from 0 to 0.05 , including any SNPs with P-value equal to 0.05 . We can then calculate the PRS with the following plink command: plink \\ --bfile EUR.QC \\ --score Height.QC.Transformed 3 4 12 header \\ --q-score-range range_list SNP.pvalue \\ --extract EUR.valid.snp \\ --out EUR The meaning of the new parameters are as follows: Paramter Value Description score Height.QC.Transformed 3 4 12 header We read from the Height.QC.Transformed file, assuming that the 3 st column is the SNP ID; 4 th column is the effective allele information; the 12 th column is the effect size estimate; and that the file contains a header q-score-range range_list SNP.pvalue We want to calculate PRS based on the thresholds defined in range_list , where the threshold values (P-values) were stored in SNP.pvalue The above command and range_list will generate 7 files: EUR.0.5.profile EUR.0.4.profile EUR.0.3.profile EUR.0.2.profile EUR.0.1.profile EUR.0.05.profile EUR.0.001.profile Note The default formula for PRS calculation in PLINK is: \\[ PRS_j =\\frac{ \\sum_i^NS_i*G_{ij}}{P*M_j} \\] where the effect size of SNP \\(i\\) is \\(S_i\\) ; the number of effect alleles observed in sample \\(j\\) is \\(G_{ij}\\) ; the ploidy of the sample is \\(P\\) (is generally 2 for humans); the total number of SNPs included in the PRS is \\(N\\) ; and the number of non-missing SNPs observed in sample \\(j\\) is \\(M_j\\) . If the sample has a missing genotype for SNP \\(i\\) , then the population minor allele frequency multiplied by the ploidy ( \\(MAF_i*P\\) ) is used instead of \\(G_{ij}\\) .","title":"Generate PRS"},{"location":"plink/#accounting-for-population-stratification","text":"Population structure is the principal source of confounding in GWAS and is usually accounted for by incorporating principal components (PCs) as covariates. We can incorporate PCs into our PRS analysis to account for population stratification. Again, we can calculate the PCs using plink : # First, we need to perform prunning plink \\ --bfile EUR.QC \\ --indep-pairwise 200 50 0 .25 \\ --out EUR # Then we calculate the first 6 PCs plink \\ --bfile EUR.QC \\ --extract EUR.prune.in \\ --pca 6 \\ --out EUR Note One way to select the appropriate number of PCs is to perform GWAS on the phenotype under study with different numbers of PCs. LDSC analysis can then be performed on the set of GWAS summary statistics and the GWAS that used the number of PCs that gave an LDSC intercept closest to 1 should correspond to that for which population structure was most accurately controlled for. Here the PCs have been stored in the EUR.eigenvec file and can be used as covariates in the regression model to account for population stratification. Important If the base and target samples are collected from different worldwide populations then the results from the PRS analysis may be biased (see Section 3.4 of our paper).","title":"Accounting for Population Stratification"},{"location":"plink/#finding-the-best-fit-prs","text":"The P-value threshold that provides the \"best-fit\" PRS under the C+T method is usually unknown. To approximate the \"best-fit\" PRS, we can perform a regression between PRS calculated at a range of P-value thresholds and then select the PRS that explains the highest phenotypic variance (please see Section 4.6 of our paper on overfitting issues). This can be achieved using R as follows: detail p.threshold <- c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) # Read in the phenotype file phenotype <- read.table ( \"EUR.height\" , header = T ) # Read in the PCs pcs <- read.table ( \"EUR.eigenvec\" , header = F ) # The default output from plink does not include a header # To make things simple, we will add the appropriate headers # (1:6 because there are 6 PCs) colnames (pcs) <- c ( \"FID\" , \"IID\" , paste0 ( \"PC\" , 1:6 )) # Read in the covariates (here, it is sex) covariate <- read.table ( \"EUR.cov\" , header = T ) # Now merge the files pheno <- merge ( merge (phenotype, covariate, by = c ( \"FID\" , \"IID\" )), pcs, by = c ( \"FID\" , \"IID\" )) # We can then calculate the null model (model with PRS) using a linear regression # (as height is quantitative) null.model <- lm (Height ~ ., data = pheno[, ! colnames (pheno) %in% c ( \"FID\" , \"IID\" )]) # And the R2 of the null model is null.r2 <- summary (null.model) $ r.squared prs.result <- NULL for (i in p.threshold){ # Go through each p-value threshold prs <- read.table ( paste0 ( \"EUR.\" ,i, \".profile\" ), header = T ) # Merge the prs with the phenotype matrix # We only want the FID, IID and PRS from the PRS file, therefore we only select the # relevant columns pheno.prs <- merge (pheno, prs[, c ( \"FID\" , \"IID\" , \"SCORE\" )], by = c ( \"FID\" , \"IID\" )) # Now perform a linear regression on Height with PRS and the covariates # ignoring the FID and IID from our model model <- lm (Height ~ ., data = pheno.prs[, ! colnames (pheno.prs) %in% c ( \"FID\" , \"IID\" )]) # model R2 is obtained as model.r2 <- summary (model) $ r.squared # R2 of PRS is simply calculated as the model R2 minus the null R2 prs.r2 <- model.r2 - null.r2 # We can also obtain the coeffcient and p-value of association of PRS as follow prs.coef <- summary (model) $ coeff[ \"SCORE\" ,] prs.beta <- as.numeric (prs.coef[ 1 ]) prs.se <- as.numeric (prs.coef[ 2 ]) prs.p <- as.numeric (prs.coef[ 4 ]) # We can then store the results prs.result <- rbind (prs.result, data.frame (Threshold = i, R2 = prs.r2, P = prs.p, BETA = prs.beta,SE = prs.se)) } # Best result is: prs.result[ which.max (prs.result $ R2),] q () # exit R quick p.threshold <- c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) phenotype <- read.table ( \"EUR.height\" , header = T ) pcs <- read.table ( \"EUR.eigenvec\" , header = F ) colnames (pcs) <- c ( \"FID\" , \"IID\" , paste0 ( \"PC\" , 1:6 )) covariate <- read.table ( \"EUR.cov\" , header = T ) pheno <- merge ( merge (phenotype, covariate, by = c ( \"FID\" , \"IID\" )), pcs, by = c ( \"FID\" , \"IID\" )) null.r2 <- summary ( lm (Height ~ ., data = pheno[, ! colnames (pheno) %in% c ( \"FID\" , \"IID\" )])) $ r.squared prs.result <- NULL for (i in p.threshold){ pheno.prs <- merge (pheno, read.table ( paste0 ( \"EUR.\" ,i, \".profile\" ), header = T )[, c ( \"FID\" , \"IID\" , \"SCORE\" )], by = c ( \"FID\" , \"IID\" )) model <- summary ( lm (Height ~ ., data = pheno.prs[, ! colnames (pheno.prs) %in% c ( \"FID\" , \"IID\" )])) model.r2 <- model $ r.squared prs.r2 <- model.r2 - null.r2 prs.coef <- model $ coeff[ \"SCORE\" ,] prs.result <- rbind (prs.result, data.frame (Threshold = i, R2 = prs.r2, P = as.numeric (prs.coef[ 4 ]), BETA = as.numeric (prs.coef[ 1 ]), SE = as.numeric (prs.coef[ 2 ]))) } print (prs.result[ which.max (prs.result $ R2),]) q () # exit R with data.table and magrittr library (data.table) library (magrittr) p.threshold <- c ( 0.001 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ) phenotype <- fread ( \"EUR.height\" ) pcs <- fread ( \"EUR.eigenvec\" , header = F ) %>% setnames (., colnames (.), c ( \"FID\" , \"IID\" , paste0 ( \"PC\" , 1:6 )) ) covariate <- fread ( \"EUR.cov\" ) pheno <- merge (phenotype, covariate) %>% merge (., pcs) null.r2 <- summary ( lm (Height ~ ., data = pheno[, - c ( \"FID\" , \"IID\" )])) $ r.squared prs.result <- NULL for (i in p.threshold){ pheno.prs <- paste0 ( \"EUR.\" , i, \".profile\" ) %>% fread (.) %>% .[, c ( \"FID\" , \"IID\" , \"SCORE\" )] %>% merge (., pheno, by = c ( \"FID\" , \"IID\" )) model <- lm (Height ~ ., data = pheno.prs[, - c ( \"FID\" , \"IID\" )]) %>% summary model.r2 <- model $ r.squared prs.r2 <- model.r2 - null.r2 prs.coef <- model $ coeff[ \"SCORE\" ,] prs.result %<>% rbind (., data.frame (Threshold = i, R2 = prs.r2, P = as.numeric (prs.coef[ 4 ]), BETA = as.numeric (prs.coef[ 1 ]), SE = as.numeric (prs.coef[ 2 ]))) } print (prs.result[ which.max (prs.result $ R2),]) q () # exit R Which P-value threshold generates the \"best-fit\" PRS? 0.3 How much phenotypic variation does the \"best-fit\" PRS explain? 0.1612372","title":"Finding the \"best-fit\" PRS"},{"location":"plink_visual/","text":"Plotting the Results \u00b6 The PRS results corresponding to a range of P-value thresholds obtained by application of the C+T PRS method (eg. using PLINK or PRSice-2) can be visualised using R as follows: Note We will be using prs.result variable, which was generated in the previous section Without ggplot2 # We strongly recommend the use of ggplot2. Only follow this code if you # are desperate. # Specify that we want to generate plot in EUR.height.bar.png png ( \"EUR.height.bar.png\" , height =10 , width =10 , res =300 , unit = \"in\" ) # First, obtain the colorings based on the p-value col <- suppressWarnings ( colorRampPalette ( c ( \"dodgerblue\" , \"firebrick\" ))) # We want the color gradient to match the ranking of p-values prs.result <- prs.result[ order ( - log10 (prs.result $ P)),] prs.result $ color <- col ( nrow (prs.result)) prs.result <- prs.result[ order (prs.result $ Threshold),] # generate a pretty format for p-value output prs.result $ print.p <- round (prs.result $ P, digits = 3 ) prs.result $ print.p[ ! is.na (prs.result $ print.p) & prs.result $ print.p == 0 ] <- format (prs.result $ P[ ! is.na (prs.result $ print.p) & prs.result $ print.p == 0 ], digits = 2 ) prs.result $ print.p <- sub ( \"e\" , \"*x*10^\" , prs.result $ print.p) # Generate the axis labels xlab <- expression ( italic (P) - value ~ threshold ~ ( italic (P)[ T ])) ylab <- expression ( paste ( \"PRS model fit: \" , R ^ 2 )) # Setup the drawing area layout ( t ( 1:2 ), widths = c ( 8.8 , 1.2 )) par ( cex.lab =1.5 , cex.axis =1.25 , font.lab =2 , oma = c ( 0 , 0.5 , 0 , 0 ), mar = c ( 4 , 6 , 0.5 , 0.5 )) # Plotting the bars b <- barplot (height = prs.result $ R2, col = prs.result $ color, border = NA , ylim = c ( 0 , max (prs.result $ R2) *1.25 ), axes = F , ann = F ) # Plot the axis labels and axis ticks odd <- seq ( 0 , nrow (prs.result) +1 , 2 ) even <- seq ( 1 , nrow (prs.result), 2 ) axis (side =1 , at = b[odd], labels = prs.result $ Threshold[odd], lwd =2 ) axis (side =1 , at = b[even], labels = prs.result $ Threshold[even],lwd =2 ) axis (side =1 , at = c ( 0 ,b[ 1 ], 2* b[ length (b)] - b[ length (b) -1 ]), labels = c ( \"\" , \"\" , \"\" ), lwd =2 , lwd.tick =0 ) # Write the p-value on top of each bar text ( parse (text = paste ( prs.result $ print.p)), x = b +0.1 , y = prs.result $ R2 + ( max (prs.result $ R2) *1.05- max (prs.result $ R2)), srt = 45 ) # Now plot the axis lines box (bty = 'L' , lwd =2 ) axis ( 2 ,las =2 , lwd =2 ) # Plot the axis titles title (ylab = ylab, line =4 , cex.lab =1.5 , font =2 ) title (xlab = xlab, line =2.5 , cex.lab =1.5 , font =2 ) # Generate plot area for the legend par (cex.lab =1.5 , cex.axis =1.25 , font.lab =2 , mar = c ( 20 , 0 , 20 , 4 )) prs.result <- prs.result[ order ( - log10 (prs.result $ P)),] image ( 1 , - log10 (prs.result $ P), t ( seq_along ( - log10 (prs.result $ P))), col = prs.result $ color, axes = F ,ann = F ) axis ( 4 ,las =2 ,xaxs = 'r' ,yaxs = 'r' , tck =0.2 , col = \"white\" ) # plot legend title title ( bquote ( atop ( - log[ 10 ] ~ model, italic (P) - value), ), line =2 , cex =1.5 , font =2 , adj =0 ) # write the plot to file dev.off () q () # exit R ggplot2 # ggplot2 is a handy package for plotting library (ggplot2) # generate a pretty format for p-value output prs.result $ print.p <- round (prs.result $ P, digits = 3 ) prs.result $ print.p[ ! is.na (prs.result $ print.p) & prs.result $ print.p == 0 ] <- format (prs.result $ P[ ! is.na (prs.result $ print.p) & prs.result $ print.p == 0 ], digits = 2 ) prs.result $ print.p <- sub ( \"e\" , \"*x*10^\" , prs.result $ print.p) # Initialize ggplot, requiring the threshold as the x-axis (use factor so that it is uniformly distributed) ggplot (data = prs.result, aes (x = factor (Threshold), y = R2)) + # Specify that we want to print p-value on top of the bars geom_text ( aes (label = paste (print.p)), vjust = -1.5 , hjust = 0 , angle = 45 , cex = 4 , parse = T ) + # Specify the range of the plot, *1.25 to provide enough space for the p-values scale_y_continuous (limits = c ( 0 , max (prs.result $ R2) * 1.25 )) + # Specify the axis labels xlab ( expression ( italic (P) - value ~ threshold ~ ( italic (P)[ T ]))) + ylab ( expression ( paste ( \"PRS model fit: \" , R ^ 2 ))) + # Draw a bar plot geom_bar ( aes (fill = - log10 (P)), stat = \"identity\" ) + # Specify the colors scale_fill_gradient2 ( low = \"dodgerblue\" , high = \"firebrick\" , mid = \"dodgerblue\" , midpoint = 1e-4 , name = bquote ( atop ( - log[ 10 ] ~ model, italic (P) - value),) ) + # Some beautification of the plot theme_classic () + theme ( axis.title = element_text (face = \"bold\" , size = 18 ), axis.text = element_text (size = 14 ), legend.title = element_text (face = \"bold\" , size = 18 ), legend.text = element_text (size = 14 ), axis.text.x = element_text (angle = 45 , hjust = 1 ) ) # save the plot ggsave ( \"EUR.height.bar.png\" , height = 7 , width = 7 ) q () # exit R An example bar plot generated using ggplot2 In addition, we can visualise the relationship between the \"best-fit\" PRS (which may have been obtained from any of the PRS programs) and the phenotype of interest, coloured according to sex: Without ggplot2 # Read in the files prs <- read.table ( \"EUR.0.3.profile\" , header = T ) height <- read.table ( \"EUR.height\" , header = T ) sex <- read.table ( \"EUR.cov\" , header = T ) # Rename the sex sex $ Sex <- as.factor (sex $ Sex) levels (sex $ Sex) <- c ( \"Male\" , \"Female\" ) # Merge the files dat <- merge ( merge (prs, height), sex) # Start plotting plot (x = dat $ SCORE, y = dat $ Height, col = \"white\" , xlab = \"Polygenic Score\" , ylab = \"Height\" ) with ( subset (dat, Sex == \"Male\" ), points (x = SCORE, y = Height, col = \"red\" )) with ( subset (dat, Sex == \"Female\" ), points (x = SCORE, y = Height, col = \"blue\" )) q () # exit R ggplot2 library (ggplot2) # Read in the files prs <- read.table ( \"EUR.0.3.profile\" , header = T ) height <- read.table ( \"EUR.height\" , header = T ) sex <- read.table ( \"EUR.cov\" , header = T ) # Rename the sex sex $ Sex <- as.factor (sex $ Sex) levels (sex $ Sex) <- c ( \"Male\" , \"Female\" ) # Merge the files dat <- merge ( merge (prs, height), sex) # Start plotting ggplot (dat, aes (x = SCORE, y = Height, color = Sex)) + geom_point () + theme_classic () + labs (x = \"Polygenic Score\" , y = \"Height\" ) q () # exit R An example scatter plot generated using ggplot2 Programs such as PRSice-2 and bigsnpr include numerous options for plotting PRS results.","title":"4. Visualizing PRS Results"},{"location":"plink_visual/#plotting-the-results","text":"The PRS results corresponding to a range of P-value thresholds obtained by application of the C+T PRS method (eg. using PLINK or PRSice-2) can be visualised using R as follows: Note We will be using prs.result variable, which was generated in the previous section Without ggplot2 # We strongly recommend the use of ggplot2. Only follow this code if you # are desperate. # Specify that we want to generate plot in EUR.height.bar.png png ( \"EUR.height.bar.png\" , height =10 , width =10 , res =300 , unit = \"in\" ) # First, obtain the colorings based on the p-value col <- suppressWarnings ( colorRampPalette ( c ( \"dodgerblue\" , \"firebrick\" ))) # We want the color gradient to match the ranking of p-values prs.result <- prs.result[ order ( - log10 (prs.result $ P)),] prs.result $ color <- col ( nrow (prs.result)) prs.result <- prs.result[ order (prs.result $ Threshold),] # generate a pretty format for p-value output prs.result $ print.p <- round (prs.result $ P, digits = 3 ) prs.result $ print.p[ ! is.na (prs.result $ print.p) & prs.result $ print.p == 0 ] <- format (prs.result $ P[ ! is.na (prs.result $ print.p) & prs.result $ print.p == 0 ], digits = 2 ) prs.result $ print.p <- sub ( \"e\" , \"*x*10^\" , prs.result $ print.p) # Generate the axis labels xlab <- expression ( italic (P) - value ~ threshold ~ ( italic (P)[ T ])) ylab <- expression ( paste ( \"PRS model fit: \" , R ^ 2 )) # Setup the drawing area layout ( t ( 1:2 ), widths = c ( 8.8 , 1.2 )) par ( cex.lab =1.5 , cex.axis =1.25 , font.lab =2 , oma = c ( 0 , 0.5 , 0 , 0 ), mar = c ( 4 , 6 , 0.5 , 0.5 )) # Plotting the bars b <- barplot (height = prs.result $ R2, col = prs.result $ color, border = NA , ylim = c ( 0 , max (prs.result $ R2) *1.25 ), axes = F , ann = F ) # Plot the axis labels and axis ticks odd <- seq ( 0 , nrow (prs.result) +1 , 2 ) even <- seq ( 1 , nrow (prs.result), 2 ) axis (side =1 , at = b[odd], labels = prs.result $ Threshold[odd], lwd =2 ) axis (side =1 , at = b[even], labels = prs.result $ Threshold[even],lwd =2 ) axis (side =1 , at = c ( 0 ,b[ 1 ], 2* b[ length (b)] - b[ length (b) -1 ]), labels = c ( \"\" , \"\" , \"\" ), lwd =2 , lwd.tick =0 ) # Write the p-value on top of each bar text ( parse (text = paste ( prs.result $ print.p)), x = b +0.1 , y = prs.result $ R2 + ( max (prs.result $ R2) *1.05- max (prs.result $ R2)), srt = 45 ) # Now plot the axis lines box (bty = 'L' , lwd =2 ) axis ( 2 ,las =2 , lwd =2 ) # Plot the axis titles title (ylab = ylab, line =4 , cex.lab =1.5 , font =2 ) title (xlab = xlab, line =2.5 , cex.lab =1.5 , font =2 ) # Generate plot area for the legend par (cex.lab =1.5 , cex.axis =1.25 , font.lab =2 , mar = c ( 20 , 0 , 20 , 4 )) prs.result <- prs.result[ order ( - log10 (prs.result $ P)),] image ( 1 , - log10 (prs.result $ P), t ( seq_along ( - log10 (prs.result $ P))), col = prs.result $ color, axes = F ,ann = F ) axis ( 4 ,las =2 ,xaxs = 'r' ,yaxs = 'r' , tck =0.2 , col = \"white\" ) # plot legend title title ( bquote ( atop ( - log[ 10 ] ~ model, italic (P) - value), ), line =2 , cex =1.5 , font =2 , adj =0 ) # write the plot to file dev.off () q () # exit R ggplot2 # ggplot2 is a handy package for plotting library (ggplot2) # generate a pretty format for p-value output prs.result $ print.p <- round (prs.result $ P, digits = 3 ) prs.result $ print.p[ ! is.na (prs.result $ print.p) & prs.result $ print.p == 0 ] <- format (prs.result $ P[ ! is.na (prs.result $ print.p) & prs.result $ print.p == 0 ], digits = 2 ) prs.result $ print.p <- sub ( \"e\" , \"*x*10^\" , prs.result $ print.p) # Initialize ggplot, requiring the threshold as the x-axis (use factor so that it is uniformly distributed) ggplot (data = prs.result, aes (x = factor (Threshold), y = R2)) + # Specify that we want to print p-value on top of the bars geom_text ( aes (label = paste (print.p)), vjust = -1.5 , hjust = 0 , angle = 45 , cex = 4 , parse = T ) + # Specify the range of the plot, *1.25 to provide enough space for the p-values scale_y_continuous (limits = c ( 0 , max (prs.result $ R2) * 1.25 )) + # Specify the axis labels xlab ( expression ( italic (P) - value ~ threshold ~ ( italic (P)[ T ]))) + ylab ( expression ( paste ( \"PRS model fit: \" , R ^ 2 ))) + # Draw a bar plot geom_bar ( aes (fill = - log10 (P)), stat = \"identity\" ) + # Specify the colors scale_fill_gradient2 ( low = \"dodgerblue\" , high = \"firebrick\" , mid = \"dodgerblue\" , midpoint = 1e-4 , name = bquote ( atop ( - log[ 10 ] ~ model, italic (P) - value),) ) + # Some beautification of the plot theme_classic () + theme ( axis.title = element_text (face = \"bold\" , size = 18 ), axis.text = element_text (size = 14 ), legend.title = element_text (face = \"bold\" , size = 18 ), legend.text = element_text (size = 14 ), axis.text.x = element_text (angle = 45 , hjust = 1 ) ) # save the plot ggsave ( \"EUR.height.bar.png\" , height = 7 , width = 7 ) q () # exit R An example bar plot generated using ggplot2 In addition, we can visualise the relationship between the \"best-fit\" PRS (which may have been obtained from any of the PRS programs) and the phenotype of interest, coloured according to sex: Without ggplot2 # Read in the files prs <- read.table ( \"EUR.0.3.profile\" , header = T ) height <- read.table ( \"EUR.height\" , header = T ) sex <- read.table ( \"EUR.cov\" , header = T ) # Rename the sex sex $ Sex <- as.factor (sex $ Sex) levels (sex $ Sex) <- c ( \"Male\" , \"Female\" ) # Merge the files dat <- merge ( merge (prs, height), sex) # Start plotting plot (x = dat $ SCORE, y = dat $ Height, col = \"white\" , xlab = \"Polygenic Score\" , ylab = \"Height\" ) with ( subset (dat, Sex == \"Male\" ), points (x = SCORE, y = Height, col = \"red\" )) with ( subset (dat, Sex == \"Female\" ), points (x = SCORE, y = Height, col = \"blue\" )) q () # exit R ggplot2 library (ggplot2) # Read in the files prs <- read.table ( \"EUR.0.3.profile\" , header = T ) height <- read.table ( \"EUR.height\" , header = T ) sex <- read.table ( \"EUR.cov\" , header = T ) # Rename the sex sex $ Sex <- as.factor (sex $ Sex) levels (sex $ Sex) <- c ( \"Male\" , \"Female\" ) # Merge the files dat <- merge ( merge (prs, height), sex) # Start plotting ggplot (dat, aes (x = SCORE, y = Height, color = Sex)) + geom_point () + theme_classic () + labs (x = \"Polygenic Score\" , y = \"Height\" ) q () # exit R An example scatter plot generated using ggplot2 Programs such as PRSice-2 and bigsnpr include numerous options for plotting PRS results.","title":"Plotting the Results"},{"location":"prsice/","text":"Background \u00b6 PRSice-2 is one of the dedicated PRS programs which automates many of the steps from the previous page that used a sequence of PLINK functions (plus some QC steps). On this page you will run a PRS analysis using PRSice-2, which implements the standard C+T method. Obtaining PRSice-2 \u00b6 PRSice-2 can be downloaded from: Operating System Link Linux 64-bit v2.3.3 OS X 64-bit v2.3.3 and can be directly used after extracting the file. In this tutorial, you will only need PRSice.R and PRSice_XXX where XXX is the operation system Required Data \u00b6 This analysis assumes that you have the following files (or you can download it from here ): File Name Description Height.QC.gz The post QC base data file. While PRSice-2 can automatically apply most filtering on the base file, it cannot remove duplicated SNPs EUR.QC.bed This file contains the genotype data that passed the QC steps EUR.QC.bim This file contains the list of SNPs that passed the QC steps EUR.QC.fam This file contains the samples that passed the QC steps EUR.height This file contains the phenotype data of the samples EUR.cov This file contains the covariates of the samples EUR.eigenvec This file contains the principal components (PCs) of the samples Running PRS analysis \u00b6 To run PRSice-2 we need a single covariate file, and therefore our covariate file and PCs file should be combined. This can be done with R as follows: without data.table covariate <- read.table ( \"EUR.cov\" , header = T ) pcs <- read.table ( \"EUR.eigenvec\" , header = F ) colnames (pcs) <- c ( \"FID\" , \"IID\" , paste0 ( \"PC\" , 1:6 )) cov <- merge (covariate, pcs, by = c ( \"FID\" , \"IID\" )) write.table (cov, \"EUR.covariate\" , quote = F , row.names = F ) q () with data.table library (data.table) covariate <- fread ( \"EUR.cov\" ) pcs <- fread ( \"EUR.eigenvec\" , header = F ) colnames (pcs) <- c ( \"FID\" , \"IID\" , paste0 ( \"PC\" , 1:6 )) cov <- merge (covariate, pcs) fwrite (cov, \"EUR.covariate\" , sep = \"\\t\" ) q () which generates EUR.cov . PRSice-2 can then be run to obtain the PRS results as follows: Linux Rscript PRSice.R \\ --prsice PRSice_linux \\ --base Height.QC.gz \\ --target EUR.QC \\ --binary-target F \\ --pheno EUR.height \\ --cov EUR.covariate \\ --base-maf MAF:0.01 \\ --base-info INFO:0.8 \\ --stat OR \\ --or \\ --out EUR OS X Rscript PRSice.R \\ --prsice PRSice_mac \\ --base Height.QC.gz \\ --target EUR.QC \\ --binary-target F \\ --pheno EUR.height \\ --cov EUR.covariate \\ --base-maf MAF:0.01 \\ --base-info INFO:0.8 \\ --stat OR \\ --or \\ --out EUR Windows Rscript PRSice.R ^ --prsice PRSice_win64.exe ^ --base Height.QC.gz ^ --target EUR.QC ^ --binary-target F ^ --pheno EUR.height ^ --cov EUR.covariate ^ --base-maf MAF:0.01 ^ --base-info INFO:0.8 ^ --stat OR ^ --or ^ --out EUR The meaning of the parameters are as follow: Paramter Value Description prsice PRSice_xxx Informs PRSice.R that the location of the PRSice binary base Height.QC.gz Informs PRSice that the name of the GWAS summary statistic target EUR.QC Informs PRSice that the input genotype files should have a prefix of EUR.QC binary-target F Indicate if the phenotype of interest is a binary trait. F for no pheno EUR.height Provide PRSice with the phenotype file cov EUR.covariate Provide PRSice with the covariate file base-maf MAF:0.01 Filter out SNPs with MAF < 0.01 in the GWAS summary statistics, using information in the MAF column base-info INFO:0.8 Filter out SNPs with INFO < 0.8 in the GWAS summary statistics, using information in the INFO column stat OR Column name of the column containing the effect size or - Inform PRSice that the effect size is an Odd Ratio out EUR Informs PRSice that all output should have a prefix of EUR This will automatically perform \"high-resolution scoring\" and generate the \"best-fit\" PRS (in EUR.best ), with associated plots of the results. Users should read Section 4.6 of our paper to learn more about issues relating to overfitting in PRS analyses. Which P-value threshold generates the \"best-fit\" PRS? 0.13995 How much phenotypic variation does the \"best-fit\" PRS explain? 0.209902","title":"PRSice-2"},{"location":"prsice/#background","text":"PRSice-2 is one of the dedicated PRS programs which automates many of the steps from the previous page that used a sequence of PLINK functions (plus some QC steps). On this page you will run a PRS analysis using PRSice-2, which implements the standard C+T method.","title":"Background"},{"location":"prsice/#obtaining-prsice-2","text":"PRSice-2 can be downloaded from: Operating System Link Linux 64-bit v2.3.3 OS X 64-bit v2.3.3 and can be directly used after extracting the file. In this tutorial, you will only need PRSice.R and PRSice_XXX where XXX is the operation system","title":"Obtaining PRSice-2"},{"location":"prsice/#required-data","text":"This analysis assumes that you have the following files (or you can download it from here ): File Name Description Height.QC.gz The post QC base data file. While PRSice-2 can automatically apply most filtering on the base file, it cannot remove duplicated SNPs EUR.QC.bed This file contains the genotype data that passed the QC steps EUR.QC.bim This file contains the list of SNPs that passed the QC steps EUR.QC.fam This file contains the samples that passed the QC steps EUR.height This file contains the phenotype data of the samples EUR.cov This file contains the covariates of the samples EUR.eigenvec This file contains the principal components (PCs) of the samples","title":"Required Data"},{"location":"prsice/#running-prs-analysis","text":"To run PRSice-2 we need a single covariate file, and therefore our covariate file and PCs file should be combined. This can be done with R as follows: without data.table covariate <- read.table ( \"EUR.cov\" , header = T ) pcs <- read.table ( \"EUR.eigenvec\" , header = F ) colnames (pcs) <- c ( \"FID\" , \"IID\" , paste0 ( \"PC\" , 1:6 )) cov <- merge (covariate, pcs, by = c ( \"FID\" , \"IID\" )) write.table (cov, \"EUR.covariate\" , quote = F , row.names = F ) q () with data.table library (data.table) covariate <- fread ( \"EUR.cov\" ) pcs <- fread ( \"EUR.eigenvec\" , header = F ) colnames (pcs) <- c ( \"FID\" , \"IID\" , paste0 ( \"PC\" , 1:6 )) cov <- merge (covariate, pcs) fwrite (cov, \"EUR.covariate\" , sep = \"\\t\" ) q () which generates EUR.cov . PRSice-2 can then be run to obtain the PRS results as follows: Linux Rscript PRSice.R \\ --prsice PRSice_linux \\ --base Height.QC.gz \\ --target EUR.QC \\ --binary-target F \\ --pheno EUR.height \\ --cov EUR.covariate \\ --base-maf MAF:0.01 \\ --base-info INFO:0.8 \\ --stat OR \\ --or \\ --out EUR OS X Rscript PRSice.R \\ --prsice PRSice_mac \\ --base Height.QC.gz \\ --target EUR.QC \\ --binary-target F \\ --pheno EUR.height \\ --cov EUR.covariate \\ --base-maf MAF:0.01 \\ --base-info INFO:0.8 \\ --stat OR \\ --or \\ --out EUR Windows Rscript PRSice.R ^ --prsice PRSice_win64.exe ^ --base Height.QC.gz ^ --target EUR.QC ^ --binary-target F ^ --pheno EUR.height ^ --cov EUR.covariate ^ --base-maf MAF:0.01 ^ --base-info INFO:0.8 ^ --stat OR ^ --or ^ --out EUR The meaning of the parameters are as follow: Paramter Value Description prsice PRSice_xxx Informs PRSice.R that the location of the PRSice binary base Height.QC.gz Informs PRSice that the name of the GWAS summary statistic target EUR.QC Informs PRSice that the input genotype files should have a prefix of EUR.QC binary-target F Indicate if the phenotype of interest is a binary trait. F for no pheno EUR.height Provide PRSice with the phenotype file cov EUR.covariate Provide PRSice with the covariate file base-maf MAF:0.01 Filter out SNPs with MAF < 0.01 in the GWAS summary statistics, using information in the MAF column base-info INFO:0.8 Filter out SNPs with INFO < 0.8 in the GWAS summary statistics, using information in the INFO column stat OR Column name of the column containing the effect size or - Inform PRSice that the effect size is an Odd Ratio out EUR Informs PRSice that all output should have a prefix of EUR This will automatically perform \"high-resolution scoring\" and generate the \"best-fit\" PRS (in EUR.best ), with associated plots of the results. Users should read Section 4.6 of our paper to learn more about issues relating to overfitting in PRS analyses. Which P-value threshold generates the \"best-fit\" PRS? 0.13995 How much phenotypic variation does the \"best-fit\" PRS explain? 0.209902","title":"Running PRS analysis"},{"location":"target/","text":"QC of Target Data \u00b6 Obtaining the target data \u00b6 Target data consist of individual-level genotype-phenotype data, usually generated within your lab/department/collaboration. For this tutorial, we have simulated some genotype-phenotype data using the 1000 Genomes Project European samples. You can download the data here Unzip the data as follow: unzip EUR.zip Note You will need plink in this section, which can be download from here . Install the program plink and include its location in your PATH directory, which allows us to use plink instead of ./plink in the commands below. If PLINK is not in your PATH directory and is instead in your working directory, replace all instances of plink in the tutorial with ./plink . QC checklist: Target data \u00b6 Below are the QC steps that comprise the QC checklist for the target data. # Sample size \u00b6 We recommend that users only perform PRS analyses on target data of at least 100 individuals. The sample size of our target data here is 503 individuals. # File transfer \u00b6 Usually we do not need to download and transfer the target data file because it is typically generated locally. However, the file should contain an md5sum code in case we send the data file to collaborators who may want to confirm that the file has not changed during the transfer. What is the md5sum code for each of the target files? File md5sum EUR.bed 98bcef133f683b1272d3ea5f97742e0e EUR.bim 6b286002904880055a9c94e01522f059 EUR.cov 85ed18288c708e095385418517e9c3bd EUR.fam e7b856f0c7bcaffc8405926d08386e97 EUR.height dd445ce969a81cded20da5c88b82d4df # Genome build \u00b6 As stated in the base data section, the genome build for our base and target data is the same, as it should be. # Standard GWAS QC \u00b6 The target data must be quality controlled to at least the standards implemented in GWAS studies, e.g. removing SNPs with low genotyping rate, low minor allele frequency, out of Hardy-Weinberg Equilibrium, removing individuals with low genotyping rate (see Marees et al ). The following plink command applies some of these QC metrics to the target data: plink \\ --bfile EUR \\ --maf 0 .01 \\ --hwe 1e-6 \\ --geno 0 .01 \\ --mind 0 .01 \\ --write-snplist \\ --make-just-fam \\ --out EUR.QC Each of the parameters corresponds to the following Parameter Value Description bfile EUR Informs plink that the input genotype files should have a prefix of EUR maf 0.01 Removes all SNPs with minor allele frequency less than 0.01. Genotyping errors typically have a larger influence on SNPs with low MAF. Studies with large sample sizes could apply a lower MAF threshold hwe 1e-6 Removes SNPs with low P-value from the Hardy-Weinberg Equilibrium Fisher's exact or chi-squared test. SNPs with significant P-values from the HWE test are more likely affected by genotyping error or the effects of natural selection. Filtering should be performed on the control samples to avoid filtering SNPs that are causal (under selection in cases). When phenotype information is included, plink will automatically perform the filtering in the controls. geno 0.01 Excludes SNPs that are missing in a high fraction of subjects. A two-stage filtering process is usually performed (see Marees et al ). mind 0.01 Excludes individuals who have a high rate of genotype missingness, since this may indicate problems in the DNA sample or processing. (see Marees et al for more details). make-just-fam - Informs plink to only generate the QC'ed sample name to avoid generating the .bed file. write-snplist - Informs plink to only generate the QC'ed SNP list to avoid generating the .bed file. out EUR.QC Informs plink that all output should have a prefix of EUR.QC How many SNPs and samples were filtered? 14 samples were removed due to a high rate of genotype missingness 5,353 SNP were removed due to missing genotype data 944 SNPs were removed due to being out of Hardy-Weinberg Equilibrium 5,061 SNPs were removed due to low minor allele frequency Note Normally, we can generate a new genotype file using the new sample list. However, this will use up a lot of storage space. Using plink 's --extract , --exclude , --keep , --remove , --make-just-fam and --write-snplist functions, we can work solely on the list of samples and SNPs without duplicating the genotype file, reducing the storage space usage. Very high or low heterozygosity rates in individuals could be due to DNA contamination or to high levels of inbreeding. Therefore, samples with extreme heterozygosity are typically removed prior to downstream analyses. First, we perform pruning to remove highly correlated SNPs: plink \\ --bfile EUR \\ --keep EUR.QC.fam \\ --extract EUR.QC.snplist \\ --indep-pairwise 200 50 0 .25 \\ --out EUR.QC Each of the parameters corresponds to the following Parameter Value Description bfile EUR Informs plink that the input genotype files should have a prefix of EUR keep EUR.QC.fam Informs plink that we only want to use samples in EUR.QC.fam in the analysis extract EUR.QC.snplist Informs plink that we only want to use SNPs in EUR.QC.snplist in the analysis indep-pairwise 200 50 0.25 Informs plink that we wish to perform pruning with a window size of 200 variants, sliding across the genome with step size of 50 variants at a time, and filter out any SNPs with LD \\(r^2\\) higher than 0.25 out EUR.QC Informs plink that all output should have a prefix of EUR.QC This will generate two files 1) EUR.QC.prune.in and 2) EUR.QC.prune.out . All SNPs within EUR.QC.prune.in have a pairwise \\(r^2 < 0.25\\) . Heterozygosity rates can then be computed using plink : plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.QC.fam \\ --het \\ --out EUR.QC This will generate the EUR.QC.het file, which contains F coefficient estimates for assessing heterozygosity. We will remove individuals with F coefficients that are more than 3 standard deviation (SD) units from the mean, which can be performed using the following R command (assuming that you have R downloaded, then you can open an R session by typing R in your terminal): Without library dat <- read.table ( \"EUR.QC.het\" , header = T ) # Read in the EUR.het file, specify it has header m <- mean (dat $ F ) # Calculate the mean s <- sd (dat $ F ) # Calculate the SD valid <- subset (dat, F <= m +3* s & F >= m -3* s) # Get any samples with F coefficient within 3 SD of the population mean write.table (valid[, c ( 1 , 2 )], \"EUR.valid.sample\" , quote = F , row.names = F ) # print FID and IID for valid samples q () # exit R With data.table library (data.table) # Read in file dat <- fread ( \"EUR.QC.het\" ) # Get samples with F coefficient within 3 SD of the population mean valid <- dat[ F <= mean ( F ) +3* sd ( F ) & F >= mean ( F ) -3* sd ( F )] # print FID and IID for valid samples fwrite (valid[, c ( \"FID\" , \"IID\" )], \"EUR.valid.sample\" , sep = \"\\t\" ) q () # exit R How many samples were excluded due to high heterozygosity rate? 2 samples were excluded # Ambiguous SNPs \u00b6 These were removed during the base data QC. # Mismatching SNPs \u00b6 SNPs that have mismatching alleles reported in the base and target data may be resolvable by strand-flipping the alleles to their complementary alleles in e.g. the target data, such as for a SNP with A/C in the base data and G/T in the target. This can be achieved with the following steps: Note Most PRS software will perform strand-flipping automatically, thus this step is usually not required. 1. Load the bim file, the summary statistic and the QC SNP list into R Without data.table # Read in bim file bim <- read.table ( \"EUR.bim\" ) colnames (bim) <- c ( \"CHR\" , \"SNP\" , \"CM\" , \"BP\" , \"B.A1\" , \"B.A2\" ) # Read in QCed SNPs qc <- read.table ( \"EUR.QC.snplist\" , header = F , stringsAsFactors = F ) # Read in the GWAS data height <- read.table ( gzfile ( \"Height.QC.gz\" ), header = T , stringsAsFactors = F , sep = \"\\t\" ) # Change all alleles to upper case for easy comparison height $ A1 <- toupper (height $ A1) height $ A2 <- toupper (height $ A2) bim $ B.A1 <- toupper (bim $ B.A1) bim $ B.A2 <- toupper (bim $ B.A2) With data.table and magrittr # magrittr allow us to do piping, which help to reduce the # amount of intermediate data types library (data.table) library (magrittr) # Read in bim file bim <- fread ( \"EUR.bim\" ) %>% # Note: . represents the output from previous step # The syntax here means, setnames of the data read from # the bim file, and replace the original column names by # the new names setnames (., colnames (.), c ( \"CHR\" , \"SNP\" , \"CM\" , \"BP\" , \"B.A1\" , \"B.A2\" )) %>% # And immediately change the alleles to upper cases .[, c ( \"B.A1\" , \"B.A2\" ) := list ( toupper (B.A1), toupper (B.A2))] # Read in summary statistic data (require data.table v1.12.0+) height <- fread ( \"Height.QC.gz\" ) %>% # And immediately change the alleles to upper cases .[, c ( \"A1\" , \"A2\" ) := list ( toupper (A1), toupper (A2))] # Read in QCed SNPs qc <- fread ( \"EUR.QC.snplist\" , header = F ) 2. Identify SNPs that require strand flipping Without data.table # Merge summary statistic with target info <- merge (bim, height, by = c ( \"SNP\" , \"CHR\" , \"BP\" )) # Filter QCed SNPs info <- info[info $ SNP %in% qc $ V1,] # Function for finding the complementary allele complement <- function (x) { switch ( x, \"A\" = \"T\" , \"C\" = \"G\" , \"T\" = \"A\" , \"G\" = \"C\" , return ( NA ) ) } # Get SNPs that have the same alleles across base and target info.match <- subset (info, A1 == B.A1 & A2 == B.A2) # Identify SNPs that are complementary between base and target info $ C.A1 <- sapply (info $ B.A1, complement) info $ C.A2 <- sapply (info $ B.A2, complement) info.complement <- subset (info, A1 == C.A1 & A2 == C.A2) # Update the complementary alleles in the bim file # This allow us to match the allele in subsequent analysis complement.snps <- bim $ SNP %in% info.complement $ SNP bim[complement.snps,] $ B.A1 <- sapply (bim[complement.snps,] $ B.A1, complement) bim[complement.snps,] $ B.A2 <- sapply (bim[complement.snps,] $ B.A2, complement) With data.table and magrittr # Merge summary statistic with target info <- merge (bim, height, by = c ( \"SNP\" , \"CHR\" , \"BP\" )) %>% # And filter out QCed SNPs .[SNP %in% qc[,V1]] # Function for calculating the complementary allele complement <- function (x){ switch (x, \"A\" = \"T\" , \"C\" = \"G\" , \"T\" = \"A\" , \"G\" = \"C\" , return ( NA ) ) } # Get SNPs that have the same alleles across base and target info.match <- info[A1 == B.A1 & A2 == B.A2, SNP] # Identify SNPs that are complementary between base and target com.snps <- info[ sapply (B.A1, complement) == A1 & sapply (B.A2, complement) == A2, SNP] # Now update the bim file bim[SNP %in% com.snps, c ( \"B.A1\" , \"B.A2\" ) := list ( sapply (B.A1, complement), sapply (B.A2, complement))] 3. Identify SNPs that require recoding in the target (to ensure the coding allele in the target data is the effective allele in the base summary statistic) Without data.table # identify SNPs that need recoding info.recode <- subset (info, A1 == B.A2 & A2 == B.A1) # Update the recode SNPs recode.snps <- bim $ SNP %in% info.recode $ SNP tmp <- bim[recode.snps,] $ B.A1 bim[recode.snps,] $ B.A1 <- bim[recode.snps,] $ B.A2 bim[recode.snps,] $ B.A2 <- tmp # identify SNPs that need recoding & complement info.crecode <- subset (info, A1 == C.A2 & A2 == C.A1) # Update the recode + strand flip SNPs com.snps <- bim $ SNP %in% info.crecode $ SNP tmp <- bim[com.snps,] $ B.A1 bim[com.snps,] $ B.A1 <- as.character ( sapply (bim[com.snps,] $ B.A2, complement)) bim[com.snps,] $ B.A2 <- as.character ( sapply (tmp, complement)) # Output updated bim file write.table ( bim[, c ( \"SNP\" , \"B.A1\" )], \"EUR.a1\" , quote = F , row.names = F , col.names = F , sep = \"\\t\" ) With data.table and magrittr # identify SNPs that need recoding recode.snps <- info[B.A1 == A2 & B.A2 == A1, SNP] # Update the bim file bim[SNP %in% recode.snps, c ( \"B.A1\" , \"B.A2\" ) := list (B.A2, B.A1)] # identify SNPs that need recoding & complement com.recode <- info[ sapply (B.A1, complement) == A2 & sapply (B.A2, complement) == A1, SNP] # Now update the bim file bim[SNP %in% com.recode, c ( \"B.A1\" , \"B.A2\" ) := list ( sapply (B.A2, complement), sapply (B.A1, complement))] # Write the updated bim file fwrite (bim[, c ( \"SNP\" , \"B.A1\" )], \"EUR.a1\" , col.names = F , sep = \"\\t\" ) 4. Identify SNPs that have different allele in base and target (usually due to difference in genome build or Indel) Without data.table mismatch <- bim $ SNP[ ! (bim $ SNP %in% info.match $ SNP | bim $ SNP %in% info.complement $ SNP | bim $ SNP %in% info.recode $ SNP | bim $ SNP %in% info.crecode $ SNP)] write.table ( mismatch, \"EUR.mismatch\" , quote = F , row.names = F , col.names = F ) q () # exit R With data.table mismatch <- bim[ ! (SNP %in% info.match | SNP %in% com.snps | SNP %in% recode.snps | SNP %in% com.recode), SNP] write.table (mismatch, \"EUR.mismatch\" , quote = F , row.names = F , col.names = F ) q () # exit R We can then use the EUR.a1 file to update the A1 alleles # Duplicate SNPs \u00b6 Make sure to remove any duplicate SNPs in your target data (these target data were simulated and so include no duplicated SNPs). # Sex chromosomes \u00b6 Sometimes sample mislabelling can occur, which may lead to invalid results. One indication of a mislabelled sample is a difference between reported sex and that indicated by the sex chromosomes. While this may be due to a difference in sex and gender identity, it could also reflect mislabeling of samples or misreporting and, thus, individuals in which there is a mismatch between biological and reported sex are typically removed. A sex check can be performed in PLINK, in which individuals are called as females if their X chromosome homozygosity estimate (F statistic) is < 0.2 and as males if the estimate is > 0.8. Before performing a sex check, pruning should be performed (see here ). A sex check can then easily be conducted using plink plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.valid.sample \\ --check-sex \\ --out EUR.QC This will generate a file called EUR.QC.sexcheck containing the F-statistics for each individual. Individuals are typically called as being biologically male if the F-statistic is > 0.8 and biologically female if F < 0.2. Without library # Read in file valid <- read.table ( \"EUR.valid.sample\" , header = T ) dat <- read.table ( \"EUR.QC.sexcheck\" , header = T ) valid <- subset (dat, STATUS == \"OK\" & FID %in% valid $ FID) write.table (valid[, c ( \"FID\" , \"IID\" )], \"EUR.QC.valid\" , row.names = F , col.names = F , sep = \"\\t\" , quote = F ) q () # exit R With data.table library (data.table) # Read in file valid <- fread ( \"EUR.valid.sample\" ) dat <- fread ( \"EUR.QC.sexcheck\" )[FID %in% valid $ FID] fwrite (dat[STATUS == \"OK\" , c ( \"FID\" , \"IID\" )], \"EUR.QC.valid\" , sep = \"\\t\" ) q () # exit R How many samples were excluded due mismatched Sex information? 4 samples were excluded # Sample overlap \u00b6 Since the target data were simulated there are no overlapping samples between the base and target data here (see the relevant section of the paper for discussion of the importance of avoiding sample overlap). # Relatedness \u00b6 Closely related individuals in the target data may lead to overfitted results, limiting the generalisability of the results. Before calculating the relatedness, pruning should be performed (see here ). Individuals that have a first or second degree relative in the sample ( \\(\\hat{\\pi} > 0.125\\) ) can be removed with the following command: plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.QC.valid \\ --rel-cutoff 0 .125 \\ --out EUR.QC How many related samples were excluded? 0 samples were excluded Note A greedy algorithm is used to remove closely related individuals in a way that optimizes the size of the sample retained. However, the algorithm is dependent on the random seed used, which can generate different results. Therefore, to reproduce the same result, you will need to specify the same random seed. PLINK's algorithm for removing related individuals does not account for the phenotype under study. To minimize the removal of cases of a disease, the following algorithm can be used instead: GreedyRelated . Generate final QC'ed target data file \u00b6 After performing the full analysis, you can generate a QC'ed data set with the following command: plink \\ --bfile EUR \\ --make-bed \\ --keep EUR.QC.rel.id \\ --out EUR.QC \\ --extract EUR.QC.snplist \\ --exclude EUR.mismatch \\ --a1-allele EUR.a1 Each of the parameters corresponds to the following Parameter Value Description bfile EUR Informs plink that the input genotype files should have a prefix of EUR keep EUR.QC.rel.id Informs plink that we only want to keep samples in EUR.QC.rel.id extract EUR.QC.snplist Informs plink that we only want to use SNPs in EUR.QC.snplist in the analysis exclude EUR.mismatch Informs plink that we wish to remove any SNPs in EUR.mismatch a1-allele EUR.a1 Fix all A1 alleles to those specified in EUR.a1 out EUR.QC Informs plink that all output should have a prefix of EUR.QC","title":"2. QC of Target Data"},{"location":"target/#qc-of-target-data","text":"","title":"QC of Target Data"},{"location":"target/#obtaining-the-target-data","text":"Target data consist of individual-level genotype-phenotype data, usually generated within your lab/department/collaboration. For this tutorial, we have simulated some genotype-phenotype data using the 1000 Genomes Project European samples. You can download the data here Unzip the data as follow: unzip EUR.zip Note You will need plink in this section, which can be download from here . Install the program plink and include its location in your PATH directory, which allows us to use plink instead of ./plink in the commands below. If PLINK is not in your PATH directory and is instead in your working directory, replace all instances of plink in the tutorial with ./plink .","title":"Obtaining the target data"},{"location":"target/#qc-checklist-target-data","text":"Below are the QC steps that comprise the QC checklist for the target data.","title":"QC checklist: Target data"},{"location":"target/#sample-size","text":"We recommend that users only perform PRS analyses on target data of at least 100 individuals. The sample size of our target data here is 503 individuals.","title":"# Sample size"},{"location":"target/#file-transfer","text":"Usually we do not need to download and transfer the target data file because it is typically generated locally. However, the file should contain an md5sum code in case we send the data file to collaborators who may want to confirm that the file has not changed during the transfer. What is the md5sum code for each of the target files? File md5sum EUR.bed 98bcef133f683b1272d3ea5f97742e0e EUR.bim 6b286002904880055a9c94e01522f059 EUR.cov 85ed18288c708e095385418517e9c3bd EUR.fam e7b856f0c7bcaffc8405926d08386e97 EUR.height dd445ce969a81cded20da5c88b82d4df","title":"# File transfer"},{"location":"target/#genome-build","text":"As stated in the base data section, the genome build for our base and target data is the same, as it should be.","title":"# Genome build"},{"location":"target/#standard-gwas-qc","text":"The target data must be quality controlled to at least the standards implemented in GWAS studies, e.g. removing SNPs with low genotyping rate, low minor allele frequency, out of Hardy-Weinberg Equilibrium, removing individuals with low genotyping rate (see Marees et al ). The following plink command applies some of these QC metrics to the target data: plink \\ --bfile EUR \\ --maf 0 .01 \\ --hwe 1e-6 \\ --geno 0 .01 \\ --mind 0 .01 \\ --write-snplist \\ --make-just-fam \\ --out EUR.QC Each of the parameters corresponds to the following Parameter Value Description bfile EUR Informs plink that the input genotype files should have a prefix of EUR maf 0.01 Removes all SNPs with minor allele frequency less than 0.01. Genotyping errors typically have a larger influence on SNPs with low MAF. Studies with large sample sizes could apply a lower MAF threshold hwe 1e-6 Removes SNPs with low P-value from the Hardy-Weinberg Equilibrium Fisher's exact or chi-squared test. SNPs with significant P-values from the HWE test are more likely affected by genotyping error or the effects of natural selection. Filtering should be performed on the control samples to avoid filtering SNPs that are causal (under selection in cases). When phenotype information is included, plink will automatically perform the filtering in the controls. geno 0.01 Excludes SNPs that are missing in a high fraction of subjects. A two-stage filtering process is usually performed (see Marees et al ). mind 0.01 Excludes individuals who have a high rate of genotype missingness, since this may indicate problems in the DNA sample or processing. (see Marees et al for more details). make-just-fam - Informs plink to only generate the QC'ed sample name to avoid generating the .bed file. write-snplist - Informs plink to only generate the QC'ed SNP list to avoid generating the .bed file. out EUR.QC Informs plink that all output should have a prefix of EUR.QC How many SNPs and samples were filtered? 14 samples were removed due to a high rate of genotype missingness 5,353 SNP were removed due to missing genotype data 944 SNPs were removed due to being out of Hardy-Weinberg Equilibrium 5,061 SNPs were removed due to low minor allele frequency Note Normally, we can generate a new genotype file using the new sample list. However, this will use up a lot of storage space. Using plink 's --extract , --exclude , --keep , --remove , --make-just-fam and --write-snplist functions, we can work solely on the list of samples and SNPs without duplicating the genotype file, reducing the storage space usage. Very high or low heterozygosity rates in individuals could be due to DNA contamination or to high levels of inbreeding. Therefore, samples with extreme heterozygosity are typically removed prior to downstream analyses. First, we perform pruning to remove highly correlated SNPs: plink \\ --bfile EUR \\ --keep EUR.QC.fam \\ --extract EUR.QC.snplist \\ --indep-pairwise 200 50 0 .25 \\ --out EUR.QC Each of the parameters corresponds to the following Parameter Value Description bfile EUR Informs plink that the input genotype files should have a prefix of EUR keep EUR.QC.fam Informs plink that we only want to use samples in EUR.QC.fam in the analysis extract EUR.QC.snplist Informs plink that we only want to use SNPs in EUR.QC.snplist in the analysis indep-pairwise 200 50 0.25 Informs plink that we wish to perform pruning with a window size of 200 variants, sliding across the genome with step size of 50 variants at a time, and filter out any SNPs with LD \\(r^2\\) higher than 0.25 out EUR.QC Informs plink that all output should have a prefix of EUR.QC This will generate two files 1) EUR.QC.prune.in and 2) EUR.QC.prune.out . All SNPs within EUR.QC.prune.in have a pairwise \\(r^2 < 0.25\\) . Heterozygosity rates can then be computed using plink : plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.QC.fam \\ --het \\ --out EUR.QC This will generate the EUR.QC.het file, which contains F coefficient estimates for assessing heterozygosity. We will remove individuals with F coefficients that are more than 3 standard deviation (SD) units from the mean, which can be performed using the following R command (assuming that you have R downloaded, then you can open an R session by typing R in your terminal): Without library dat <- read.table ( \"EUR.QC.het\" , header = T ) # Read in the EUR.het file, specify it has header m <- mean (dat $ F ) # Calculate the mean s <- sd (dat $ F ) # Calculate the SD valid <- subset (dat, F <= m +3* s & F >= m -3* s) # Get any samples with F coefficient within 3 SD of the population mean write.table (valid[, c ( 1 , 2 )], \"EUR.valid.sample\" , quote = F , row.names = F ) # print FID and IID for valid samples q () # exit R With data.table library (data.table) # Read in file dat <- fread ( \"EUR.QC.het\" ) # Get samples with F coefficient within 3 SD of the population mean valid <- dat[ F <= mean ( F ) +3* sd ( F ) & F >= mean ( F ) -3* sd ( F )] # print FID and IID for valid samples fwrite (valid[, c ( \"FID\" , \"IID\" )], \"EUR.valid.sample\" , sep = \"\\t\" ) q () # exit R How many samples were excluded due to high heterozygosity rate? 2 samples were excluded","title":"# Standard GWAS QC"},{"location":"target/#ambiguous-snps","text":"These were removed during the base data QC.","title":"# Ambiguous SNPs"},{"location":"target/#mismatching-snps","text":"SNPs that have mismatching alleles reported in the base and target data may be resolvable by strand-flipping the alleles to their complementary alleles in e.g. the target data, such as for a SNP with A/C in the base data and G/T in the target. This can be achieved with the following steps: Note Most PRS software will perform strand-flipping automatically, thus this step is usually not required. 1. Load the bim file, the summary statistic and the QC SNP list into R Without data.table # Read in bim file bim <- read.table ( \"EUR.bim\" ) colnames (bim) <- c ( \"CHR\" , \"SNP\" , \"CM\" , \"BP\" , \"B.A1\" , \"B.A2\" ) # Read in QCed SNPs qc <- read.table ( \"EUR.QC.snplist\" , header = F , stringsAsFactors = F ) # Read in the GWAS data height <- read.table ( gzfile ( \"Height.QC.gz\" ), header = T , stringsAsFactors = F , sep = \"\\t\" ) # Change all alleles to upper case for easy comparison height $ A1 <- toupper (height $ A1) height $ A2 <- toupper (height $ A2) bim $ B.A1 <- toupper (bim $ B.A1) bim $ B.A2 <- toupper (bim $ B.A2) With data.table and magrittr # magrittr allow us to do piping, which help to reduce the # amount of intermediate data types library (data.table) library (magrittr) # Read in bim file bim <- fread ( \"EUR.bim\" ) %>% # Note: . represents the output from previous step # The syntax here means, setnames of the data read from # the bim file, and replace the original column names by # the new names setnames (., colnames (.), c ( \"CHR\" , \"SNP\" , \"CM\" , \"BP\" , \"B.A1\" , \"B.A2\" )) %>% # And immediately change the alleles to upper cases .[, c ( \"B.A1\" , \"B.A2\" ) := list ( toupper (B.A1), toupper (B.A2))] # Read in summary statistic data (require data.table v1.12.0+) height <- fread ( \"Height.QC.gz\" ) %>% # And immediately change the alleles to upper cases .[, c ( \"A1\" , \"A2\" ) := list ( toupper (A1), toupper (A2))] # Read in QCed SNPs qc <- fread ( \"EUR.QC.snplist\" , header = F ) 2. Identify SNPs that require strand flipping Without data.table # Merge summary statistic with target info <- merge (bim, height, by = c ( \"SNP\" , \"CHR\" , \"BP\" )) # Filter QCed SNPs info <- info[info $ SNP %in% qc $ V1,] # Function for finding the complementary allele complement <- function (x) { switch ( x, \"A\" = \"T\" , \"C\" = \"G\" , \"T\" = \"A\" , \"G\" = \"C\" , return ( NA ) ) } # Get SNPs that have the same alleles across base and target info.match <- subset (info, A1 == B.A1 & A2 == B.A2) # Identify SNPs that are complementary between base and target info $ C.A1 <- sapply (info $ B.A1, complement) info $ C.A2 <- sapply (info $ B.A2, complement) info.complement <- subset (info, A1 == C.A1 & A2 == C.A2) # Update the complementary alleles in the bim file # This allow us to match the allele in subsequent analysis complement.snps <- bim $ SNP %in% info.complement $ SNP bim[complement.snps,] $ B.A1 <- sapply (bim[complement.snps,] $ B.A1, complement) bim[complement.snps,] $ B.A2 <- sapply (bim[complement.snps,] $ B.A2, complement) With data.table and magrittr # Merge summary statistic with target info <- merge (bim, height, by = c ( \"SNP\" , \"CHR\" , \"BP\" )) %>% # And filter out QCed SNPs .[SNP %in% qc[,V1]] # Function for calculating the complementary allele complement <- function (x){ switch (x, \"A\" = \"T\" , \"C\" = \"G\" , \"T\" = \"A\" , \"G\" = \"C\" , return ( NA ) ) } # Get SNPs that have the same alleles across base and target info.match <- info[A1 == B.A1 & A2 == B.A2, SNP] # Identify SNPs that are complementary between base and target com.snps <- info[ sapply (B.A1, complement) == A1 & sapply (B.A2, complement) == A2, SNP] # Now update the bim file bim[SNP %in% com.snps, c ( \"B.A1\" , \"B.A2\" ) := list ( sapply (B.A1, complement), sapply (B.A2, complement))] 3. Identify SNPs that require recoding in the target (to ensure the coding allele in the target data is the effective allele in the base summary statistic) Without data.table # identify SNPs that need recoding info.recode <- subset (info, A1 == B.A2 & A2 == B.A1) # Update the recode SNPs recode.snps <- bim $ SNP %in% info.recode $ SNP tmp <- bim[recode.snps,] $ B.A1 bim[recode.snps,] $ B.A1 <- bim[recode.snps,] $ B.A2 bim[recode.snps,] $ B.A2 <- tmp # identify SNPs that need recoding & complement info.crecode <- subset (info, A1 == C.A2 & A2 == C.A1) # Update the recode + strand flip SNPs com.snps <- bim $ SNP %in% info.crecode $ SNP tmp <- bim[com.snps,] $ B.A1 bim[com.snps,] $ B.A1 <- as.character ( sapply (bim[com.snps,] $ B.A2, complement)) bim[com.snps,] $ B.A2 <- as.character ( sapply (tmp, complement)) # Output updated bim file write.table ( bim[, c ( \"SNP\" , \"B.A1\" )], \"EUR.a1\" , quote = F , row.names = F , col.names = F , sep = \"\\t\" ) With data.table and magrittr # identify SNPs that need recoding recode.snps <- info[B.A1 == A2 & B.A2 == A1, SNP] # Update the bim file bim[SNP %in% recode.snps, c ( \"B.A1\" , \"B.A2\" ) := list (B.A2, B.A1)] # identify SNPs that need recoding & complement com.recode <- info[ sapply (B.A1, complement) == A2 & sapply (B.A2, complement) == A1, SNP] # Now update the bim file bim[SNP %in% com.recode, c ( \"B.A1\" , \"B.A2\" ) := list ( sapply (B.A2, complement), sapply (B.A1, complement))] # Write the updated bim file fwrite (bim[, c ( \"SNP\" , \"B.A1\" )], \"EUR.a1\" , col.names = F , sep = \"\\t\" ) 4. Identify SNPs that have different allele in base and target (usually due to difference in genome build or Indel) Without data.table mismatch <- bim $ SNP[ ! (bim $ SNP %in% info.match $ SNP | bim $ SNP %in% info.complement $ SNP | bim $ SNP %in% info.recode $ SNP | bim $ SNP %in% info.crecode $ SNP)] write.table ( mismatch, \"EUR.mismatch\" , quote = F , row.names = F , col.names = F ) q () # exit R With data.table mismatch <- bim[ ! (SNP %in% info.match | SNP %in% com.snps | SNP %in% recode.snps | SNP %in% com.recode), SNP] write.table (mismatch, \"EUR.mismatch\" , quote = F , row.names = F , col.names = F ) q () # exit R We can then use the EUR.a1 file to update the A1 alleles","title":"# Mismatching SNPs"},{"location":"target/#duplicate-snps","text":"Make sure to remove any duplicate SNPs in your target data (these target data were simulated and so include no duplicated SNPs).","title":"# Duplicate SNPs"},{"location":"target/#sex-chromosomes","text":"Sometimes sample mislabelling can occur, which may lead to invalid results. One indication of a mislabelled sample is a difference between reported sex and that indicated by the sex chromosomes. While this may be due to a difference in sex and gender identity, it could also reflect mislabeling of samples or misreporting and, thus, individuals in which there is a mismatch between biological and reported sex are typically removed. A sex check can be performed in PLINK, in which individuals are called as females if their X chromosome homozygosity estimate (F statistic) is < 0.2 and as males if the estimate is > 0.8. Before performing a sex check, pruning should be performed (see here ). A sex check can then easily be conducted using plink plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.valid.sample \\ --check-sex \\ --out EUR.QC This will generate a file called EUR.QC.sexcheck containing the F-statistics for each individual. Individuals are typically called as being biologically male if the F-statistic is > 0.8 and biologically female if F < 0.2. Without library # Read in file valid <- read.table ( \"EUR.valid.sample\" , header = T ) dat <- read.table ( \"EUR.QC.sexcheck\" , header = T ) valid <- subset (dat, STATUS == \"OK\" & FID %in% valid $ FID) write.table (valid[, c ( \"FID\" , \"IID\" )], \"EUR.QC.valid\" , row.names = F , col.names = F , sep = \"\\t\" , quote = F ) q () # exit R With data.table library (data.table) # Read in file valid <- fread ( \"EUR.valid.sample\" ) dat <- fread ( \"EUR.QC.sexcheck\" )[FID %in% valid $ FID] fwrite (dat[STATUS == \"OK\" , c ( \"FID\" , \"IID\" )], \"EUR.QC.valid\" , sep = \"\\t\" ) q () # exit R How many samples were excluded due mismatched Sex information? 4 samples were excluded","title":"# Sex chromosomes"},{"location":"target/#sample-overlap","text":"Since the target data were simulated there are no overlapping samples between the base and target data here (see the relevant section of the paper for discussion of the importance of avoiding sample overlap).","title":"# Sample overlap"},{"location":"target/#relatedness","text":"Closely related individuals in the target data may lead to overfitted results, limiting the generalisability of the results. Before calculating the relatedness, pruning should be performed (see here ). Individuals that have a first or second degree relative in the sample ( \\(\\hat{\\pi} > 0.125\\) ) can be removed with the following command: plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.QC.valid \\ --rel-cutoff 0 .125 \\ --out EUR.QC How many related samples were excluded? 0 samples were excluded Note A greedy algorithm is used to remove closely related individuals in a way that optimizes the size of the sample retained. However, the algorithm is dependent on the random seed used, which can generate different results. Therefore, to reproduce the same result, you will need to specify the same random seed. PLINK's algorithm for removing related individuals does not account for the phenotype under study. To minimize the removal of cases of a disease, the following algorithm can be used instead: GreedyRelated .","title":"# Relatedness"},{"location":"target/#generate-final-qced-target-data-file","text":"After performing the full analysis, you can generate a QC'ed data set with the following command: plink \\ --bfile EUR \\ --make-bed \\ --keep EUR.QC.rel.id \\ --out EUR.QC \\ --extract EUR.QC.snplist \\ --exclude EUR.mismatch \\ --a1-allele EUR.a1 Each of the parameters corresponds to the following Parameter Value Description bfile EUR Informs plink that the input genotype files should have a prefix of EUR keep EUR.QC.rel.id Informs plink that we only want to keep samples in EUR.QC.rel.id extract EUR.QC.snplist Informs plink that we only want to use SNPs in EUR.QC.snplist in the analysis exclude EUR.mismatch Informs plink that we wish to remove any SNPs in EUR.mismatch a1-allele EUR.a1 Fix all A1 alleles to those specified in EUR.a1 out EUR.QC Informs plink that all output should have a prefix of EUR.QC","title":"Generate final QC'ed target data file"}]}